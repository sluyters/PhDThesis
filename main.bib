%===========================================================
% References starting by A
%===========================================================
@inproceedings{Ackad:2015,
    author = {Ackad, Christopher and Clayphan, Andrew and Tomitsch, Martin and Kay, Judy},
    title = {{An In-the-Wild Study of Learning Mid-Air Gestures to Browse Hierarchical Information at a Large Interactive Public Display}},
    year = {2015},
    isbn = {9781450335744},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2750858.2807532},
    doi = {10.1145/2750858.2807532},
    abstract = {This paper describes the design and evaluation of our Media Ribbon, a large public interactive display for browsing hierarchical information, with mid-air gestures. Browsing a hierarchical information space is a fundamental form of interaction. Designing learnable mid-air gestures is a current challenge for large display interaction. Our in-the-wild evaluation draws on 41 days of quantitative log data, with 4484 gestures detected, and qualitative data from 15 interviews, and associated video. We explored: whether our design enabled people to learn the gestures; how our tutorial and feedback mechanisms supported learning; and the effectiveness of support for browsing hierarchical information. Our contributions are: (1) design of large public display for browsing of hierarchical information; (2) with its gesture set; (3) insights into the ways people learn and use this interface in our context; and (4) guidelines for designing learnable mid-air gestures.},
    booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
    pages = {1227–1238},
    numpages = {12},
    keywords = {interactive public information displays, gestural interaction, user centred design and pervasive computing},
    venue = {Osaka, Japan},
    series = {UbiComp '15},
}


@INPROCEEDINGS {Aich:2023,
    author = {Aich, Shubhra and Ruiz-Santaquiteria, Jesus and Lu, Zhenyu and Garg, Prachi and Joseph, K. J. and Garcia, Alvaro Fernandez and Balasubramanian, Vineeth N. and Kin, Kenrick and Wan, Chengde and Camgoz, Necati Cihan and Ma, Shugao and De La Torre, Fernando},
    booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
    title = {{Data-Free Class-Incremental Hand Gesture Recognition}},
    year = {2023},
    volume = {},
    issn = {},
    pages = {20901-20910},
    abstract = {This paper investigates data-free class-incremental learning (DFCIL) for hand gesture recognition from 3D skeleton sequences. In this class-incremental learning (CIL) setting, while incrementally registering the new classes, we do not have access to the training samples (i.e. data-free) of the already known classes due to privacy. Existing DFCIL methods primarily focus on various forms of knowledge distillation for model inversion to mitigate catastrophic forgetting. Unlike SOTA methods, we delve deeper into the choice of the best samples for inversion. Inspired by the well-grounded theory of max-margin classification, we find that the best samples tend to lie close to the approximate decision boundary within a reasonable margin. To this end, we propose BOAT-MI – a simple and effective boundary-aware prototypical sampling mechanism for model inversion for DFCIL. Our sampling scheme outperforms SOTA methods significantly on two 3D skeleton gesture datasets, the publicly available SHREC 2017, and EgoGesture3D – which we extract from a publicly available RGBD dataset. Both our codebase and the EgoGesture3D skeleton dataset are publicly available: https://github.com/humansensinglab/dfcil-hgr.},
    keywords = {training;data privacy;computer vision;three-dimensional displays;gesture recognition;skeleton},
    doi = {10.1109/ICCV51070.2023.01916},
    url = {https://doi.ieeecomputersociety.org/10.1109/ICCV51070.2023.01916},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {oct},
}



@techreport{Aigner:2012,
    author = {Aigner, Roland and Wigdor, Daniel and Benko, Hrvoje and Haller, Michael and Lindbauer, David and Ion, Alexandra and Zhao, Shengdong and Koh, Jeffrey Tzu Kwan Valino},
    title = {{Understanding Mid-Air Hand Gestures: A Study of Human Preferences in Usage of Gesture Types for HCI}},
    year = {2012},
    month = nov,
    abstract = {In this paper we present the results of a study of human preferences in using mid-air gestures for directing other humans. Rather than contributing a specific set of gestures, we contribute a set of gesture types, which together make a set of the core actions needed to complete any of our six chosen tasks in the domain of human-to-human gestural communication without the speech channel. We observed 12 participants, cooperating to accomplish different tasks only using hand gestures to communicate. We analyzed 5,500 gestures in terms of hand usage and gesture type, using a novel classification scheme which combines three existing taxonomies in order to better capture this interaction space. Our findings indicate that, depending on the meaning of the gesture, there is preference in the usage of gesture types, such as pointing, pantomimic acting, direct manipulation, semaphoric, or iconic gestures. These results can be used as guidelines to design purely gesture driven interfaces for interactive environments and surfaces.},
    publisher = {Microsoft Research},
    url = {https://www.microsoft.com/en-us/research/publication/understanding-mid-air-hand-gestures-a-study-of-human-preferences-in-usage-of-gesture-types-for-hci/},
    number = {MSR-TR-2012-111},
}


@inproceedings{Agresti:2019,
    author={Agresti, Gianluca and Milani, Simone},
    booktitle={Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing},
    series    = {ICASSP 2019}, 
    venue = {Brighton, UK},
    dates = {12-17 May 2019},
    title={{Material Identification Using RF Sensors and Convolutional Neural Networks}}, 
    year={2019},
    volume={},
    number={},
    pages={3662-3666},
    doi={10.1109/ICASSP.2019.8682296},
    publisher={IEEE},
    address={Piscataway, NJ, USA},
}


@incollection{Ahmad:2021,
	address = {Cham},
	title = {{GigaHertz}: {Gesture} {Sensing} {Using} {Microwave} {Radar} and {IR} {Sensor} with {Machine} {Learning} {Algorithms}},
	volume = {1200},
	isbn = {978-3-030-51858-5 978-3-030-51859-2},
	shorttitle = {{GigaHertz}},
	url = {http://link.springer.com/10.1007/978-3-030-51859-2_39},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Image {Processing} and {Capsule} {Networks}},
	publisher = {Springer International Publishing},
	author = {Ahmad, Misbah and Ghawale, Milind and Dubey, Sakshi and Gupta, Ayushi and Sonar, Poonam},
	editor = {Chen, Joy Iong-Zong and Tavares, João Manuel R. S. and Shakya, Subarna and Iliyasu, Abdullah M.},
	year = {2021},
	doi = {10.1007/978-3-030-51859-2_39},
	note = {Series Title: Advances in Intelligent Systems and Computing},
	pages = {422--434},
}


@article{Ahmed:2019,
	title = {Finger-{Counting}-{Based} {Gesture} {Recognition} within {Cars} {Using} {Impulse} {Radar} with {Convolutional} {Neural} {Network}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/6/1429},
	doi = {10.3390/s19061429},
	abstract = {The diversion of a driver's attention from driving can be catastrophic. Given that conventional button- and touch-based interfaces may distract the driver, developing novel distraction-free interfaces for the various devices present in cars has becomes necessary. Hand gesture recognition may provide an alternative interface inside cars. Given that cars are the targeted application area, we determined the optimal location for the radar sensor, so that the signal reflected from the driver's hand during gesturing is unaffected by interference from the motion of the driver's body or other motions within the car. We implemented a Convolutional Neural Network-based technique to recognize the finger-counting-based hand gestures using an Impulse Radio (IR) radar sensor. The accuracy of the proposed method was sufficiently high for real-world applications.},
	number = {6},
	journal = {Sensors},
	author = {Ahmed, Shahzad and Khan, Faheem and Ghaffar, Asim and Hussain, Farhan and Cho, Sung Ho},
    pages = {1--14},
	year = {2019},
}


@article{Ahmed:2020,
	title = {Hand {Gesture} {Recognition} {Using} an {IR}-{UWB} {Radar} with an {Inception} {Module}-{Based} {Classifier}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/2/564},
	doi = {10.3390/s20020564},
	abstract = {The emerging integration of technology in daily lives has increased the need for more convenient methods for human-computer interaction (HCI). Given that the existing HCI approaches exhibit various limitations, hand gesture recognition-based HCI may serve as a more natural mode of man-machine interaction in many situations. Inspired by an inception module-based deep-learning network (GoogLeNet), this paper presents a novel hand gesture recognition technique for impulse-radio ultra-wideband (IR-UWB) radars which demonstrates a higher gesture recognition accuracy. First, methodology to demonstrate radar signals as three-dimensional image patterns is presented and then, the inception module-based variant of GoogLeNet is used to analyze the pattern within the images for the recognition of different hand gestures. The proposed framework is exploited for eight different hand gestures with a promising classification accuracy of 95\%. To verify the robustness of the proposed algorithm, multiple human subjects were involved in data acquisition.},
    pages = {1--18},
	number = {2},
	journal = {Sensors},
	author = {Ahmed, Shahzad and Cho, Sung Ho},
	year = {2020},
}


@article{Ahmed:2021,
    author = {Ahmed, Shahzad and  Wang, Dingyang and  Park, Junyoung and  Cho, Sung Ho},
    title = {UWB-gestures, a public dataset of dynamic hand gestures acquired using impulse radar sensors},
    year = {2021},
    issue_date = {April 2021},
    volume = {8},
    number = {102},
    url = {https://www.nature.com/articles/s41597-021-00876-0},
    doi = {10.1038/s41597-021-00876-0},
    journal = {Scientific Data},
    month = apr,
}


@article{Ahmed:2021:Survey,
    AUTHOR = {Ahmed, Shahzad and Kallu, Karam Dad and Ahmed, Sarfaraz and Cho, Sung Ho},
    TITLE = {Hand Gestures Recognition Using Radar Sensors for Human-Computer-Interaction: A Review},
    JOURNAL = {Remote Sensing},
    VOLUME = {13},
    YEAR = {2021},
    NUMBER = {3},
    article-NUMBER = {527},
    URL = {https://www.mdpi.com/2072-4292/13/3/527},
    ISSN = {2072-4292},
    ABSTRACT = {Human–Computer Interfaces (HCI) deals with the study of interface between humans and computers. The use of radar and other RF sensors to develop HCI based on Hand Gesture Recognition (HGR) has gained increasing attention over the past decade. Today, devices have built-in radars for recognizing and categorizing hand movements. In this article, we present the first ever review related to HGR using radar sensors. We review the available techniques for multi-domain hand gestures data representation for different signal processing and deep-learning-based HGR algorithms. We classify the radars used for HGR as pulsed and continuous-wave radars, and both the hardware and the algorithmic details of each category is presented in detail. Quantitative and qualitative analysis of ongoing trends related to radar-based HCI, and available radar hardware and algorithms is also presented. At the end, developed devices and applications based on gesture-recognition through radar are discussed. Limitations, future aspects and research directions related to this field are also discussed.},
    DOI = {10.3390/rs13030527},
}


@inproceedings{Akl:2012,
    author = {Akl, Ahmad and Valaee, Shahrokh},
    year = {2010},
    month = {04},
    pages = {2270 - 2273},
    booktitle={Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing},
    title = {Accelerometer-based gesture recognition via dynamic-time warping, affinity propagation, \& compressive sensing},
    series = {ICASSP 2010},
    venue = {Dallas, TX, USA},
    dates = {14-19 March 2010},
    doi = {10.1109/ICASSP.2010.5495895},
}


@article{Aksallakh:2014,  
    author={Alsallakh, Bilal and Hanbury, Allan and Hauser, Helwig and Miksch, Silvia and Rauber, Andreas},  
    journal={IEEE Transactions on Visualization and Computer Graphics},   
    title={Visual Methods for Analyzing Probabilistic Classification Data},   
    year={2014},  
    volume={20},  
    number={12},  
    pages={1703-1712},
    url={https://ieeexplore.ieee.org/document/6875957},
    doi={10.1109/TVCG.2014.2346660},
}


@incollection{AlHourani:2018,
    title = {Chapter 7 - Millimeter-wave integrated radar systems and techniques},
    editor = {Rama Chellappa and Sergios Theodoridis},
    booktitle = {Academic Press Library in Signal Processing, Volume 7},
    publisher = {Academic Press},
    pages = {317-363},
    year = {2018},
    isbn = {978-0-12-811887-0},
    doi = {https://doi.org/10.1016/B978-0-12-811887-0.00007-9},
    url = {https://www.sciencedirect.com/science/article/pii/B9780128118870000079},
    author = {Akram Al-Hourani and Robin J. Evans and Peter M. Farrell and Bill Moran and Marco Martorella and Sithamparanathan Kandeepan and Stan Skafidas and Udaya Parampalli},
    keywords = {Integrated radar, Radar on a chip, Pseudo-random stepped frequency radar, Radar interference, Millimeter-wave radar, Radar information theory},
    abstract = {Recent advances in semiconductor fabrication are providing the opportunity to build very small and cost-effective single chip integrated radar systems operating at millimeter-wave frequencies and beyond. These tiny radar systems will enable a wide range of new applications such as automotive radar, safety helmet radar, robot guidance radar, UAV collision avoidance and mapping radar, bicycle safety radar, and many other possibilities. Considerable progress has been made in designing and building such systems; however, many challenges remain related to limitations imposed by radio frequency IC technology such as Complementary Metal Oxide Semiconductors (CMOS) including modest dynamic range and modest noise figure for low noise amplifies (LNAs), oscillator phase noise, limited transmit power, etc. There is also likely to be a significant interference challenge arising from large numbers of small radars operating in close spatial and spectral proximity in many of the envisioned applications. In this chapter we present an overview of some of the design challenges facing millimeter-wave radar, and shed light on recent advances in system design and signal processing techniques, including adaptive waveform scheduling and interference mitigation. Advances in signal processing will allow multiple radars to operate in an uncoordinated manner with limited processing power using adaptive waveform scheduling, that leads to better interference mitigation and higher ranging performance. We present some properties of the millimeter-wave spectrum and investigate ray-tracing methods as an efficient, accurate, and rapid testing platform for consumer radars applications, where we develop a ray-tracing propagation model for automotive radar operating in an urban environment. Also, we introduce novel tools from stochastic geometry to characterize the statistics of the interference resulting from many radars sharing the same spectrum in a particular location. Specifically, we study automotive radar applications and obtain interference statistics and the estimated detection performance. Finally, we briefly discuss certain fundamental limitations imposed on radar system capabilities, by CMOS technology and by the information carrying capacity of electromagnetic waves.},
}


@inproceedings{Ali:2019,
    author    = {Ali, Abdullah X. and Morris, Meredith Ringel and Wobbrock, Jacob O.},
    editor    = {Brewster, Stephen A. and Fitzpatrick, Geraldine and Cox, Anna L. and Kostakos, Vassilis},
    title     = {Crowdlicit: {A} System for Conducting Distributed End-User Elicitation and Identification Studies},
    booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems},
    series    = {CHI 2019},
    venue  = {Glasgow, Scotland, UK},
    dates     = {May 04-09, 2019},
    pages     = {255},
    publisher = {{ACM}},
    year      = {2019},
    url       = {https://doi.org/10.1145/3290605.3300485},
    doi       = {10.1145/3290605.3300485},
    timestamp = {Sun, 02 Jun 2019 21:13:35 +0200},
    biburl    = {https://dblp.org/rec/conf/chi/AliMW19.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Ali:2021,
    author = {Ali, Abdullah and Ringel Morris, Meredith and O. Wobbrock, Jacob},
    title = {{“I Am Iron Man”: Priming Improves the Learnability and Memorability of User-Elicited Gestures}},
    year = {2021},
    isbn = {9781450380966},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411764.3445758},
    doi = {10.1145/3411764.3445758},
    abstract = {Priming is used as a way of increasing the diversity of proposals in end-user elicitation studies, but priming has not been investigated thoroughly in this context. We conduct a distributed end-user elicitation study with 167 participants, which had three priming groups: a no-priming control group, sci-fi priming, and a creative mindset group. We evaluated the gestures proposed by these groups in a distributed learnability and memorability study with 18 participants. We found that the user-elicited gestures from the sci-fi group were significantly faster to learn, requiring an average of 1.22 viewings to learn compared to 1.60 viewings required to learn the control gestures, and 1.56 viewings to learn the gestures elicited from the creative mindset group. In addition, both primed gesture groups had higher memorability with 80\% of the sci-fi-primed gestures and 73\% of the creative mindset group gestures were recalled correctly after one week without practice compared to 43\% of the control group gestures.},
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {359},
    numpages = {14},
    keywords = {end-user identification, learnability, memorability, crowdsourcing, Mechanical Turk, Distributed Interaction Design, end-user elicitation study},
    venue = {Yokohama, Japan},
    series = {CHI '21},
}


@inproceedings{Alloulah:2018,
    author = {Alloulah, Mohammed and Isopoussu, Anton and Kawsar, Fahim},
    title = {On Indoor Human Sensing Using Commodity Radar},
    year = {2018},
    isbn = {9781450359665},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3267305.3274180},
    doi = {10.1145/3267305.3274180},
    abstract = {Radio frequency radar indoors is gaining traction owing to its promise for extended coverage and device-free operation. However, while the well-behaved radar sensing model affords clear advantages, the cluttered indoor environment presents numerous challenges for reliable human sensing. Classic radar techniques are hard to call upon since the kinematic and clutter behaviours in aerospace are vastly different from their indoor counterparts. We demonstrate the peculiarities of indoor radar using a commercial 2D array commodity device in the 6 to 8.5 GHz band. We then present a set of processing tools suited for indoor radar human sensing. We show that excessive indoor clutter and erratic human kinematics can be largely mitigated building on such processing tools without resorting to much low-level techniques unsupported by commercial commodity radars.},
    booktitle = {Proceedings of the ACM International Joint Conference and International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers},
    pages = {1331–1336},
    numpages = {6},
    keywords = {Human Sensing, Indoor Radar},
    venue = {Singapore, Singapore},
    dates = {October 8-12, 2018},
    series = {UbiComp '18},
}


@inproceedings{Alt:2018,
    author = {Alt, Florian and Geiger, Sabrina and H\"{o}hl, Wolfgang},
    title = {{ShapelineGuide: Teaching Mid-Air Gestures for Large Interactive Displays}},
    year = {2018},
    isbn = {9781450357654},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3205873.3205887},
    doi = {10.1145/3205873.3205887},
    abstract = {We present ShapelineGuide, a dynamic visual guide that supports users of large interactive displays while performing mid-air gestures. Today, we find many examples of large displays supporting interaction through gestures performed in Mid-air. Yet, approaches that support users in learning and executing these gestures are still scarce. Prior approaches require complex setups, are targeted towards the use of 2D gestures, or focus on the initial gestures only. Our work extends state-of-the-art by presenting a feedforward system that provides users constant updates on their gestures. We report on the design and implementation of the approach and present findings from an evaluation of the system in a lab study (N=44), focusing on learning performance, accuracy, and errors. We found that ShapelineGuide helps users with regard to learning the gestures as well as decreases execution times and cognitive load.},
    booktitle = {Proceedings of the 7th ACM International Symposium on Pervasive Displays},
    articleno = {3},
    numpages = {8},
    keywords = {Displays, Dynamic Guides, Feedback, Feedforward, Gestures},
    location = {Munich, Germany},
    series = {PerDis '18},
}


@inproceedings{Amin:2019a,
	title = {Automatic {Arm} {Motion} {Recognition} {Using} {Radar} for {Smart} {Home} {Technologies}},
	doi = {10.1109/RADAR41533.2019.171318},
	abstract = {In considering man-machine interface for smart home technology, we introduce a simple but effective technique in automatic arm motion recognition using radar. The proposed technique classifies arm motions based on the envelopes of their micro-Doppler (MD) signatures. These envelopes capture the distinctions among different arm movements and their corresponding positive and negative Doppler frequencies that are generated during each arm motion. We detect the positive and negative frequency envelopes of MD separately, and form a feature vector of their augmentation. We use the k-nearest neighbor (k NN) classifier and Manhattan distance (L1) measure, in lieu of Euclidean distance (L2), so as not to diminish small but critical envelope values. It is shown that this method can achieve higher than 99\% classification rates when choosing specific arm motion articulations from a sitting down position.},
	booktitle = {Proceedings of the IEEE {International} {Radar} {Conference}},
	series={RADAR '19},
    venue={Boston, MA, USA},
	dates={22-26 April 2019},
	author = {Amin, Moeness G. and Zeng, Zhengxin and Shan, Tao and Guendel, R.G.},
	month = sep,
	year = {2019},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	note = {ISSN: 2640-7736},
	keywords = {arm motion articulations, arm motion recognition, arm movements, automatic arm motion recognition, critical envelope values, Doppler effect, Doppler radar, Euclidean distance, feature extraction, Feature extraction, gesture recognition, home automation, image classification, image motion analysis, k-nearest neighbor classifier, kNN classifier, L1 measure, man-machine interface, Manhattan distance, micro-Doppler, microDoppler signatures, negative Doppler frequencies, negative frequency envelopes, positive Doppler frequencies, positive frequency envelopes, Radar applications, radar imaging, smart home technology, smart homes, Spectrogram, Time-frequency analysis, time-frequency representations},
	pages = {1--4},
}


@inproceedings{Amin:2019b,
    author = {Amin, Moeness G. and Zeng, Zhengxin and Shan, Tao},
	title = {Hand {Gesture} {Recognition} based on {Radar} {Micro}-{Doppler} {Signature} {Envelopes}},
	doi = {10.1109/RADAR.2019.8835661},
	abstract = {We introduce a simple but effective technique in automatic hand gesture recognition using radar. The proposed technique classifies hand gestures based on the envelopes of their micro-Doppler (MD) signatures. These envelopes capture the distinctions among different hand movements and their corresponding positive and negative Doppler frequencies that are generated during each gesture act. We detect the positive and negative frequency envelopes of MD separately, and form a feature vector of their augmentation. We use the k-nearest neighbor (kNN) classifier and Manhattan distance (L1) measure, in lieu of Euclidean distance (L2), so as not to diminish small but critical envelope values. It is shown that this method outperforms both low-dimension representation techniques based on principal component analysis (PCA) and sparse reconstruction using Gaussian-windowed Fourier dictionary, and can achieve very high classification rates.},
	booktitle = {Proceedings of the {IEEE} Int. {Radar} {Conference}},
	series={RADAR '19},
	venue={Boston, MA, USA},
	dates={22-26 April 2019},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	month = apr,
	year = {2019},
	note = {ISSN: 2375-5318},
	keywords = {automatic hand gesture recognition, Correlation, Doppler effect, Doppler radar, Euclidean distance, Feature extraction, feature vector, Fourier transforms, Gaussian processes, Gaussian-windowed Fourier dictionary, gesture recognition, Gesture recognition, Hand gesture recognition, hand movements, image classification, image reconstruction, image representation, k-nearest neighbor classifier, kNN classifier, low-dimension representation techniques, Manhattan distance, micro-Doppler, microDoppler signatures, nearest neighbour methods, negative Doppler frequencies, negative frequency envelopes, PCA, positive Doppler frequencies, positive frequency envelopes, principal component analysis, Radar, radar microDoppler signature envelopes, sparse reconstruction, Spectrogram, Time-frequency analysis, time-frequency representations},
	pages = {1--6},
}


@inproceedings{Amin:2020,
	title = {Arm {Motion} {Classification} {Using} {Curve} {Matching} of {Maximum} {Instantaneous} {Doppler} {Frequency} {Signatures}},
	doi = {10.1109/RADAR42522.2020.9114779},
	abstract = {Hand and arm gesture recognition using the radio frequency (RF) sensing modality proves valuable in man-machine interface and smart environment. In this paper, we use curve matching techniques for measuring the similarities and differences of the maximum instantaneous Doppler frequencies corresponding to different arm gestures. In particular, we apply both Fréchet and dynamic time warping (DTW) distances that, unlike the Euclidean (L2) and Manhattan (L1) distances, take into account both the location and the order of the points for rendering two curves similar or dissimilar. It is shown that improved arm gesture classification can be achieved by using the DTW method, in lieu of L2 and L1 distances, under the nearest neighbor (NN) classifier.},
	booktitle = {Proceedings of the {IEEE} {International} {Radar} {Conference}},
	series={RADAR '20},
	author = {Amin, Moeness G. and Zeng, Zhengxin and Shan, Tao},
	month = apr,
	year = {2020},
	note = {ISSN: 2640-7736},
	keywords = {arm gesture classification, arm motion classification, Arm motion recognition, curve matching, curve matching techniques, digital signatures, DTW distance, dynamic time warping, Euclidean distances, Frechet time warping, gesture recognition, hand gesture recognition, image classification, image matching, image motion analysis, instantaneous Doppler frequency, instantaneous Doppler frequency signatures, man-machine interface, Manhattan distances, micro-Doppler signature, nearest neighbor classifier, radio frequency sensing modality, smart environment},
	pages = {303--308},
}


@article{Anastasiou:2020,
    author    = {Anastasiou, Dimitra and Ras, Eric},
    title     = {Gestures in Tangible User Interfaces},
    journal   = {{ERCIM} News},
    volume    = {2020},
    number    = {120},
    year      = {2020},
    url       = {https://ercim-news.ercim.eu/en120/special/gestures-in-tangible-user-interfaces},
    timestamp = {Thu, 30 Jan 2020 12:21:45 +0100},
    biburl    = {https://dblp.org/rec/journals/ercim/AnastasiouR20.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    numpages = {52},
}


@article{Andrews:2011,
    author = {Andrews, Christopher and Endert, Alex and Yost, Beth and North, Chris},
    title ={{Information visualization on large, high-resolution displays: Issues, challenges, and opportunities}},
    journal = {Information Visualization},
    volume = {10},
    number = {4},
    pages = {341-355},
    year = {2011},
    doi = {10.1177/1473871611415997},
    URL = {https://doi.org/10.1177/1473871611415997},
    eprint = {https://doi.org/10.1177/1473871611415997},
    abstract = { Larger, higher-resolution displays are becoming accessible to a greater number of users as display technologies decrease in cost and software for the displays improves. The additional pixels are especially useful for information visualization where scalability has typically been limited by the number of pixels available on a display. But how will visualizations for larger displays need to fundamentally differ from visualizations on desktop displays? Are the basic visualization design principles different? With this potentially new design paradigm comes questions such as whether the relative effectiveness of various graphical encodings are different on large displays, which visualizations and datasets benefit the most, and how interaction with visualizations on large, high-resolution displays will need to change. As we explore these possibilities, we shift away from the technical limitations of scalability imposed by traditional displays (e.g. number of pixels) to studying the human abilities that emerge when these limitations are removed. There is much potential for information visualizations to benefit from large, high-resolution displays, but this potential will only be realized through understanding the interaction between visualization design, perception, interaction techniques, and the display technology. In this paper we present critical design issues and outline some of the challenges and future opportunities for designing visualizations for large, high-resolution displays. We hope that these issues, challenges, and opportunities will provide guidance for future research in this area.},
}


@article{Angelini:2015,
    AUTHOR = {Angelini, Leonardo and Lalanne, Denis and Hoven, Elise Van den and Khaled, Omar Abou and Mugellini, Elena},
    TITLE = {Move, Hold and Touch: A Framework for Tangible Gesture Interactive Systems},
    JOURNAL = {Machines},
    VOLUME = {3},
    YEAR = {2015},
    NUMBER = {3},
    PAGES = {173--207},
    URL = {https://www.mdpi.com/2075-1702/3/3/173},
    ISSN = {2075-1702},
    ABSTRACT = {Technology is spreading in our everyday world, and digital interaction beyond the screen, with real objects, allows taking advantage of our natural manipulative and communicative skills. Tangible gesture interaction takes advantage of these skills by bridging two popular domains in Human-Computer Interaction, tangible interaction and gestural interaction. In this paper, we present the Tangible Gesture Interaction Framework (TGIF) for classifying and guiding works in this field. We propose a classification of gestures according to three relationships with objects: move, hold and touch. Following this classification, we analyzed previous work in the literature to obtain guidelines and common practices for designing and building new tangible gesture interactive systems. We describe four interactive systems as application examples of the TGIF guidelines and we discuss the descriptive, evaluative and generative power of TGIF.},
    DOI = {10.3390/machines3030173},
}


@inproceedings{Annett:2014,
    author = {Annett, Michelle and Ng, Albert and Dietz, Paul and Bischof, Walter F. and Gupta, Anoop},
    title = {{How Low Should We Go? Understanding the Perception of Latency While Inking}},
    year = {2014},
    isbn = {9781482260038},
    publisher = {Canadian Information Processing Society},
    address = {CAN},
    abstract = {Recent advances in hardware have enabled researchers to study the perception of latency. Thus far, latency research has utilized simple touch and stylus-based tasks that do not represent inking activities found in the real world. In this work, we report on two studies that utilized writing and sketching tasks to understand the limits of human perception. Our studies revealed that latency perception while inking is worse (~50 milliseconds) than perception while performing non-inking tasks reported previously (~2-7 milliseconds). We also determined that latency perception is not based on the distance from the stylus' nib to the ink, but rather on the presence of a visual referent such as the hand or stylus. The prior and current work has informed the Latency Perception Model, a framework upon which latency knowledge and the underlying mechanisms of perception can be understood and further explored.},
    booktitle = {Proceedings of Graphics Interface 2014},
    pages = {167–174},
    numpages = {8},
    keywords = {latency perception model, psychophysics, pen, stylus, latency, just-noticeable difference, direct interaction, responsiveness, perception, indirect interaction, delay},
    venue = {Montreal, Quebec, Canada},
    series = {GI '14},
}


@inproceedings{Anthony:2010,
    author = {Anthony, Lisa and Wobbrock, Jacob O.},
    title = {{A Lightweight Multistroke Recognizer for User Interface Prototypes}},
    booktitle = {Proceedings of Graphics Interface 2010},
    series = {GI '10},
    year = {2010},
    isbn = {978-1-56881-712-5},
    venue = {Ottawa, Ontario, Canada},
    pages = {245--252},
    numpages = {8},
    url = {http://dl.acm.org/citation.cfm?id=1839214.1839258},
    acmid = {1839258},
    publisher = {Canadian Information Processing Society},
    address = {Toronto, Ont., Canada},
    keywords = {gesture recognition, marks, multistrokes, rapid prototyping, stroke recognition, symbols, unistrokes, user interfaces},
} 


@inproceedings{Anthony:2012,
    author = {Anthony, Lisa and Wobbrock, Jacob O.},
    title = {{\$N-ProTractor: A Fast and Accurate Multistroke Recognizer}},
    booktitle = {Proceedings of Graphics Interface 2012},
    series = {GI '12},
    year = {2012},
    isbn = {978-1-4503-1420-6},
    venue = {Toronto, Ontario, Canada},
    pages = {117--120},
    numpages = {4},
    url = {http://dl.acm.org/citation.cfm?id=2305276.2305296},
    acmid = {2305296},
    publisher = {Canadian Information Processing Society},
    address = {Toronto, Ont., Canada},
    keywords = {\$N, evaluation, multistroke gesture recognition, protractor, stroke recognition, template matching},
}


@inproceedings{Anthony:2013,
    author = {Anthony, Lisa and Vatavu, Radu-Daniel and Wobbrock, Jacob O.},
    title = {{Understanding the Consistency of Users' Pen and Finger Stroke Gesture Articulation}},
    year = {2013},
    isbn = {9781482216806},
    publisher = {Canadian Information Processing Society},
    address = {CAN},
    abstract = {Little work has been done on understanding the articulation patterns of users' touch and surface gestures, despite the importance of such knowledge to inform the design of gesture recognizers and gesture sets for different applications. We report a methodology to analyze user consistency in gesture production, both between-users and within-user, by employing articulation features such as stroke type, stroke direction, and stroke ordering, and by measuring variations in execution with geometric and kinematic gesture descriptors. We report results on four gesture datasets (40,305 samples of 63 gesture types by 113 users). We find a high degree of consistency within-users (.91), lower consistency between-users (.55), higher consistency for certain gestures (e.g., less geometrically complex shapes are more consistent than complex ones), and a loglinear relationship between number of strokes and consistency. We highlight implications of our results to help designers create better surface gesture interfaces informed by user behavior.},
    booktitle = {Proceedings of Graphics Interface 2013},
    pages = {87–94},
    numpages = {8},
    venue = {Regina, Sascatchewan, Canada},
    series = {GI '13},
}


@inproceedings{Appert:2009,
    author = {Appert, Caroline and Zhai, Shumin},
    title = {{Using Strokes as Command Shortcuts: Cognitive Benefits and Toolkit Support}},
    year = {2009},
    isbn = {9781605582467},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1518701.1519052},
    doi = {10.1145/1518701.1519052},
    abstract = {This paper investigates using stroke gestures as shortcuts to menu selection. We first experimentally measured the performance and ease of learning of stroke shortcuts in comparison to keyboard shortcuts when there is no mnemonic link between the shortcut and the command. While both types of shortcuts had the same level of performance with enough practice, stroke shortcuts had substantial cognitive advantages in learning and recall. With the same amount of practice, users could successfully recall more shortcuts and make fewer errors with stroke shortcuts than with keyboard shortcuts. The second half of the paper focuses on UI development support and articulates guidelines for toolkits to implement stroke shortcuts in a wide range of software applications. We illustrate how to apply these guidelines by introducing the Stroke Shortcuts Toolkit (SST) which is a library for adding stroke shortcuts to Java Swing applications with just a few lines of code.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2289–2298},
    numpages = {10},
    keywords = {shortcuts, gesture, stroke, toolkit},
    venue = {Boston, MA, USA},
    series = {CHI '09},
}


@article{Archer:1997,
    title={{Unspoken Diversity: Cultural Differences in Gestures}},
    author={Archer, Dane},
    journal={Qualitative Sociology},
    year={1997},
    volume={20},
    pages={79-105},
    url={https://api.semanticscholar.org/CorpusID:142578394},
    doi={10.1023/A:1024716331692},
}


@article{Ardito:2015,
    author = {Ardito, Carmelo and Buono, Paolo and Costabile, Maria Francesca and Desolda, Giuseppe},
    title = {{Interaction with Large Displays: A Survey}},
    year = {2015},
    issue_date = {April 2015},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {47},
    number = {3},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/2682623},
    doi = {10.1145/2682623},
    abstract = {Large interactive displays are increasingly placed in public (or semipublic) locations, including museums, shops, various city settings, and offices. This article discusses the evolution of such displays by looking at their use and analyzing how they are changing the concept of human-computer interaction through new modalities. By surveying the literature on systems using these displays, relevant features were identified and used as classification dimensions. The analysis provided may inform the design and development of future installations. A discussion on research challenges concludes the article.},
    journal = {ACM Comput. Surv.},
    month = feb,
    articleno = {46},
    numpages = {38},
    keywords = {touchless interaction, multitouch displays, Public installations},
}


@inproceedings{Argelaguet:2017,
    author={Argelaguet, Ferran and Ducoffe, Mélanie and Lécuyer, Anatole and Gribonval, Remi},
    booktitle={Proc. of IEEE Symposium on 3D User Interfaces},
    series={(3DUI '17)},
    title={{Spatial and rotation invariant 3D gesture recognition based on sparse representation}}, 
    venue={Los Angeles, CA, USA},
    dates={18-19 March 2017},
    publisher={IEEE},
    address={Piscataway, NJ, USA},
    doi={10.1109/3DUI.2017.7893333},
    url={https://ieeexplore.ieee.org/document/7893333},
    year={2017},  
    volume={}, 
    number={},  
    pages={158-167},
}


@article{Arsalan:2019,
	title = {Character {Recognition} in {Air}-{Writing} {Based} on {Network} of {Radars} for {Human}-{Machine} {Interface}},
	volume = {19},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2019.2922395},
	abstract = {Radar technology plays a vital role in contact-less detection of hand gestures or motions, which forms an alternate and intuitive form of human-computer interface. Air-writing refers to the writing of linguistic characters or words in free space by hand gesture movements. In this paper, we propose an air-writing system based on a network of millimeter wave radars. We propose a two-stage approach for extraction and recognition of handwriting gestures. The extraction processing stage uses a fine range estimate combined with the trilateration technique to detect and localize the hand marker, followed by a smoothening filter to create a trajectory of the character through the hand movement. For the recognition stage, we explore two approaches: one extracts the time-series trajectory data and recognizes the drawn character using long short term memory (LSTM), bi-directional LSTM (BLSTM), and convolutional LSTM (ConvLSTM) with connectionist temporal classification (CTC) loss function, and the other approach reconstructs a 2D image from the trajectory of drawn character and uses deep convolutional neural network (DCNN) to classify the alphabets drawn by the user. ConvLSTM-CTC performs best among LSTM variants on time-series trajectory data achieving 98.33\% classification accuracy similar to DCNN over the chosen character set. This paper employs real data using a network of three 60-GHz millimeter wave radar sensor to demonstrate the success of the proposed setup and associated algorithm with design consideration.},
	number = {19},
	journal = {IEEE Sensors Journal},
	author = {Arsalan, Muhammad and Santra, Avik},
	month = oct,
	year = {2019},
	keywords = {2D image, 60 GHz mm-wave radar, air-writing system, bidirectional LSTM, BLSTM, character recognition, Chirp, connectionist temporal classification, connectionist temporal classification loss function, contact-less detection, ConvLSTM-CTC, convolutional LSTM, convolutional neural nets, DCNN, deep convolutional neural network, drawn character, extraction processing stage, feature extraction, frequency 60 GHz, gesture recognition, hand gesture movements, hand marker, handwriting gestures, handwritten character recognition, human computer interaction, human-computer interface, human-machine interface, image classification, image filtering, learning (artificial intelligence), linguistic characters, long short term memory, millimeter wave radar sensor, millimeter wave radars, millimetre wave radar, network of radars, object detection, Radar antennas, Radar cross-sections, radar detection, radar receivers, radar technology, sensing, Sensors, smoothening filter, smoothing methods, time series, time-series trajectory data, Trajectory, trilateration technique, Writing},
	pages = {8855--8864},
}


@inproceedings{Arthamanolap:2019,
	title = {Doppler {Radar} for {Dynamic} {Hand} {Gesture} {Recognition} based on {Signal} {Image} {Processing}},
	doi = {10.1109/ECTI-CON47248.2019.8955217},
	abstract = {From previous researches, Doppler radar was used to detect signal for implement with many applications. Nevertheless, it is difficult to analyze for recognize object. At present, technique of deep learning in terms of signal processing and image processing are using in many researches to classify categories of data. In this paper, signal image was used by deep learning to classify hand gesture by receiving signals from 24GHz transceiver: BGT24MTR11. We transformed the signals to images for 3 categories including Spectrogram, Time domain from original signal and feature MFCC graph. After that those of converted image will be trained by Deep learning for classify the hand gesture types. From the result of this experiment has been shown that signal image can be used to recognize hand gesture and Spectrogram graph makes the highest accuracy as 94\%.},
	booktitle = {Proceedings of 16th {International} {Conference} on {Electrical} {Engineering}/{Electronics}, {Computer}, {Telecommunications} and {Information} {Technology}},
	series={{ECTI}-{CON}'19},
	venue={Pattaya, Thailand},
	dates={10-13 July 2019},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	author = {Arthamanolap, Kongphum and Gabbualoy, Somprasong and Phasukkit, Pattarapong},
	month = jul,
	year = {2019},
	keywords = {BGT24MTR11, converted image, Deep Learning, deep learning technique, Doppler radar, feature MFCC graph, frequency 24.0 GHz, gesture recognition, graph theory, hand gesture, hand gesture recognition, image classification, image processing, learning (artificial intelligence), MFCC, neural nets, object recognition, radar computing, radar detection, radar imaging, signal image, signal image processing, Spectrogram, Spectrogram graph},
	pages = {931--934},
}


@inproceedings{Ashbrook:2010,
    author = {Ashbrook, Daniel and Starner, Thad},
    title = {{MAGIC}: A Motion Gesture Design Tool},
    year = {2010},
    isbn = {9781605589299},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1753326.1753653},
    doi = {10.1145/1753326.1753653},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2159–2168},
    numpages = {10},
    keywords = {gesture},
    venue = {Atlanta, Georgia, USA},
    series = {CHI ’10},
}


@article{Attygalle:2021,
    author    = {Attygalle, Nuwan T. and Leiva, Luis A. and Kljun, Matjaz and Sandor, Christian and Plopski, Alexander and Kato, Hirokazu and Copič Pucihar, Klen},
    title     = {No Interface, No Problem: Gesture Recognition on Physical Objects
               Using Radar Sensing},
    journal   = {Sensors},
    volume    = {21},
    number    = {17},
    pages     = {5771},
    year      = {2021},
    url       = {https://doi.org/10.3390/s21175771},
    doi       = {10.3390/s21175771},
    timestamp = {Thu, 16 Sep 2021 18:04:48 +0200},
    biburl    = {https://dblp.org/rec/journals/sensors/AttygalleLKSPKP21.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Avrahami:2018,
    author = {Avrahami, Daniel and Patel, Mitesh and Yamaura, Yusuke and Kratz, Sven},
    title = {Below the Surface: Unobtrusive Activity Recognition for Work Surfaces Using RF-Radar Sensing},
    year = {2018},
    isbn = {9781450349451},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3172944.3172962},
    doi = {10.1145/3172944.3172962},
    abstract = {Activity recognition is a core component of many intelligent and context-aware systems. In this paper, we present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in two work domains; recognizing work activities at a convenience-store counter (useful for post-hoc analytics) and recognizing common office deskwork activities (useful for real-time applications). We classify seven clerk activities with 94.9\% accuracy using data collected in a lab environment, and recognize six common deskwork activities collected in real offices with 95.3\% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users privacy concerns associated with cameras and is useful for a wide range of intelligent systems.},
    booktitle = {Proc. of the 23rd ACM International Conference on Intelligent User Interfaces},
    pages = {439–451},
    numpages = {13},
    keywords = {retail, activity recognition, IMU, radio frequency radar sensor, deskwork, sensing},
    venue = {Tokyo, Japan},
    dates =  {March 7-11, 2018},
    series = {IUI '18},
}


@article{Avrahami:2020,
    author = {Avrahami, Daniel and Patel, Mitesh and Yamaura, Yusuke and Kratz, Sven and Cooper, Matthew},
    title = {Unobtrusive Activity Recognition and Position Estimation for Work Surfaces Using RF-Radar Sensing},
    year = {2019},
    issue_date = {January 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {10},
    number = {1},
    issn = {2160-6455},
    url = {https://doi.org/10.1145/3241383},
    doi = {10.1145/3241383},
    abstract = {Activity recognition is a core component of many intelligent and context-aware systems. We present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in three domains: recognizing work activities at a convenience-store counter, recognizing common office deskwork activities, and estimating the position of customers in a showroom environment. Our examples illustrate potential benefits for both post-hoc business analytics and for real-time applications. Our solution was able to classify seven clerk activities with 94.9\% accuracy using data collected in a lab environment and able to recognize six common deskwork activities collected in real offices with 95.3\% accuracy. Using two sensors simultaneously, we demonstrate coarse position estimation around a large surface with 95.4\% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users’ privacy concerns associated with cameras and is useful for a wide range of intelligent systems.},
    journal = {ACM Trans. Interact. Intell. Syst.},
    month = aug,
    articleno = {11},
    numpages = {28},
    keywords = {retail, radio frequency radar sensor, deskwork, IMU, sensing, Activity recognition},
}


%===========================================================
% References starting by B
%===========================================================
@article{Bachmann:2018, 
    title={{Review of Three-Dimensional Human-Computer Interaction with Focus on the Leap Motion Controller}}, 
    volume={18}, 
    ISSN={1424-8220}, 
    url={http://dx.doi.org/10.3390/s18072194}, 
    DOI={10.3390/s18072194}, 
    number={7}, 
    journal={Sensors},
    publisher={MDPI AG}, 
    author={Bachmann, Daniel and Weichert, Frank and Rinkenauer, Gerhard}, 
    year={2018},
    month=jul, 
    pages={2194}, 
}


@article{Bangor:2008,
    author = {Bangor, Aaron and Kortum, Philip and Miller, James},
    title = {An Empirical Evaluation of the System Usability Scale},
    journal = {International Journal of Human–Computer Interaction},
    volume = {24},
    number = {6},
    pages = {574-594},
    year  = {2008},
    publisher = {Taylor \& Francis},
    doi = {10.1080/10447310802205776},
    URL = {https://doi.org/10.1080/10447310802205776},
}


@inproceedings{Bannon:2020,
	title = {Exploring gesture recognition with low-cost {CW} radar modules in comparison to {FMCW} architectures},
	doi = {10.1109/RADAR42522.2020.9114650},
	abstract = {Radar-based hand gesture recognition is an area receiving a significant amount of interest in recent years due to the rapid increase in the availability of low-cost low-footprint RF sensors. The most common configuration is Frequency Modulated Continuous Wave (FMCW), whereas Continuous Wave (CW) radar is not receiving as much attention. In this paper we explore the use of extremely low cost CW radar modules for gesture recognition. In doing so a set of signal processing electronics is developed, implemented, and used to supply the resulting signal to PC audio input for recording. A dataset of gestures was recorded and gesture recognition accuracy was compared to FMCW recordings to show that CW systems can provide a high accuracy for gesture recognition at a very low cost.},
	booktitle = {Proceedings of the {IEEE} {International} {Radar} {Conference}},
	author = {Bannon, Alan and Capraru, Richard and Ritchie, Matthew},
	series={{RADAR}'20},
	venue={Washington, DC, USA},
	dates={28-30 April 2020},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	month = apr,
	year = {2020},
	note = {ISSN: 2640-7736},
	keywords = {CW radar, FM radar, FMCW radar module systems, FMCW recording architectures, frequency modulated continuous wave radar, gesture recognition, low-footprint RF sensors, radar-based hand gesture recognition},
	pages = {744--748},
}


@article{Bangor:2009,
    author = {Bangor, Aaron and Kortum, Philip and Miller, James},
    title = {Determining What Individual SUS Scores Mean: Adding an Adjective Rating Scale},
    year = {2009},
    issue_date = {May 2009},
    publisher = {Usability Professionals' Association},
    address = {Bloomingdale, IL},
    volume = {4},
    number = {3},
    issn = {1931-3357},
    abstract = {The System Usability Scale (SUS) is an inexpensive, yet effective tool for assessing the usability of a product, including Web sites, cell phones, interactive voice response systems, TV applications, and more. It provides an easy-to-understand score from 0 (negative) to 100 (positive). While a 100-point scale is intuitive in many respects and allows for relative judgments, information describing how the numeric score translates into an absolute judgment of usability is not known. To help answer that question, a seven-point adjective-anchored Likert scale was added as an eleventh question to nearly 1,000 SUS surveys. Results show that the Likert scale scores correlate extremely well with the SUS scores (r=0.822). The addition of the adjective rating scale to the SUS may help practitioners interpret individual SUS scores and aid in explaining the results to non-human factors professionals.},
    journal = {J. Usability Studies},
    month = may,
    pages = {114–123},
    numpages = {10},
    keywords = {user satisfaction, SUS, system usability scale, surveys, usability}
}


@inproceedings{Bau:2008,
    author = {Bau, Olivier and Mackay, Wendy E.},
    title = {{OctoPocus: a dynamic guide for learning gesture-based command sets}},
    year = {2008},
    isbn = {9781595939753},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1449715.1449724},
    doi = {10.1145/1449715.1449724},
    abstract = {We describe OctoPocus, an example of a dynamic guide that combines on-screen feedforward and feedback to help users learn, execute and remember gesture sets. OctoPocus can be applied to a wide range of single-stroke gestures and recognition algorithms and helps users progress smoothly from novice to expert performance. We provide an analysis of the design space and describe the results of two experi-ments that show that OctoPocus is significantly faster and improves learning of arbitrary gestures, compared to con-ventional Help menus. It can also be adapted to a mark-based gesture set, significantly improving input time compared to a two-level, four-item Hierarchical Marking menu.},
    booktitle = {Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology},
    pages = {37–46},
    numpages = {10},
    keywords = {pen input, octopocus, mouse input, gesture recognition, feedforward, feedback, dynamic guides},
    location = {Monterey, CA, USA},
    series = {UIST '08},
}


@article{Berenguer:2019, 
    author={Berenguer, Abel Díaz and Oveneke, Meshia Cédric and Khalid, Habib-Ur-Rehman and Alioscha-Perez, Mitchel and Bourdoux, André and Sahli, Hichem}, 
    journal={IEEE Access},   
    title={GestureVLAD: Combining Unsupervised Features Representation and Spatio-Temporal Aggregation for Doppler-Radar Gesture Recognition},   
    year={2019},  
    volume={7},  
    number={},  
    pages={137122-137135},  
    doi={10.1109/ACCESS.2019.2942305},
}


@book{Boduch:2019,
    title     = {{React Material-UI Cookbook}},
    author    = {Boduch, Adam},
    year      = {2019},
    publisher = {Packt Publishing},
    address   = {Birmingham, United Kingdom},
    url       = {https://www.packtpub.com/application-development/react-material-ui-cookbook},
}


@inproceedings{Bojja:2019,
    author={Bojja, Abhishake Kumar and Mueller, Franziska and Malireddi, Sri Raghu and Oberweger, Markus and Lepetit, Vincent and Theobalt, Christian and Yi, Kwang Moo and Tagliasacchi, Andrea},
    booktitle={Proceedings of the 16th IEEE Conference on Computer and Robot Vision}, 
    series = {CRV '19}, 
    title={HandSeg: An Automatically Labeled Dataset for Hand Segmentation from Depth Images}, 
    year={2019},
    volume={},
    number={},
    venue={Kingston, QC, Canada},
    dates={29-31 May 2019},
    publisher={IEEE Press},
    address={Piscataway, NJ, USA},
    pages={151-158},
    doi={10.1109/CRV.2019.00028},
    url={https://ieeexplore.ieee.org/abstract/document/8781633},
}


@incollection{Bole:2014,
    title = {Chapter 2 - The Radar System – Technical Principles},
    editor = {Bole, Alan and Wall, Alan and Norris, Andy},
    booktitle = {Radar and ARPA Manual (Third Edition)},
    publisher = {Butterworth-Heinemann},
    edition = {Third Edition},
    address = {Oxford},
    pages = {29-137},
    year = {2014},
    isbn = {978-0-08-097752-2},
    doi = {https://doi.org/10.1016/B978-0-08-097752-2.00002-7},
    url = {https://www.sciencedirect.com/science/article/pii/B9780080977522000027},
    author = {Bole, Alan and Wall, Alan and Norris, Andy},
    keywords = {Magnetron, Coherent, Barker codes, Clutter, Transmitter, Receiver, Amplifier, Modulator, Pulse Repetition Frequency (PRF), Radio Frequency (RF), Gain, Pulse length Beamwidth, Antenna Siting, Interference, Sidelobes, Pulse compression, Interswitching},
    abstract = {It is here that the equipment for actually obtaining the range and bearing of a target is considered in a way that does not assume any previous knowledge of electronic technology. Conventional magnetron-based radar is described as well as the more recently available coherent radars, together with the benefits that the latter can bring to the navigator. The importance and role of digital processing in modern marine radar systems is considered, as well as how the processed information is finally displayed. All this is directed to allow the user to have a detailed understanding of the optimisation of a radar and its display, not least in filtering out responses from sea and rain clutter.}
}


@inproceedings{Boulahia:2017, 
    author={Boulahia, Said Yacine and Anquetil, Eric and Kulpa, Richard and Multon, Franck},  
    booktitle={Proceedings of the 12th IEEE International Conference on Automatic Face Gesture Recognition},
    series={FG '17},
    doi={10.1109/FG.2017.63},
    url={https://ieeexplore.ieee.org/document/7961777},
    title={{3D Multistroke Mapping (3DMM): Transfer of Hand-Drawn Pattern Representation for Skeleton-Based Gesture Recognition}},   year={2017},  volume={},  number={},  pages={462-467},
    dates={30 May-3 June 2017},
    venue={Washington, DC, USA},
    publisher={IEEE},
}


@inproceedings{Boulabiar:2014,
    author="Boulabiar, Mohamed-Ikbel
    and Coppin, Gilles
    and Poirier, Franck",
    editor="Kurosu, Masaaki",
    title="The Study of the Full Cycle of Gesture Interaction, The Continuum between 2D and 3D",
    booktitle="Human-Computer Interaction. Advanced Interaction Modalities and Techniques",
    year="2014",
    publisher="Springer International Publishing",
    address="Cham",
    pages="24--35",
    abstract="The goal of HCI researchers is to make interaction with computer interfaces simpler, efficient and more natural. In a context of object manipulation, we think that reaching this goal requires the ability to predict and recognize how humans grasp then manipulate objects. This is based on studies explaining human vision, reach, grasp taxonomies and manipulations. In this paper, we study the full cycle of gesture interaction using different points of view, then attempt to organize them using Norman's theory of Human Action, we link the psychology of object sensing to HCI goals and propose a simplification of gestures classes into four principal families. Our simplification of gestures classes still allow the expression of more detailed subclasses differentiated by the gesture properties.",
    isbn="978-3-319-07230-2"
}


@book{Bottou:1998,
    author = "Bottou, Léon",
    title = "Online Algorithms and Stochastic Approximations",
    year = "1998",
    publisher = "Cambridge University Press",
    address = "Cambridge, UK",
    isbn="978-0-521-65263-6",
}


@inproceedings{Bragdon:2011,
    author = {Bragdon, Andrew and DeLine, Rob and Hinckley, Ken and Morris, Meredith Ringel},
    title = {{Code Space: Touch + Air Gesture Hybrid Interactions for Supporting Developer Meetings}},
    year = {2011},
    isbn = {9781450308717},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2076354.2076393},
    doi = {10.1145/2076354.2076393},
    booktitle = {Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces},
    pages = {212–221},
    numpages = {10},
    keywords = {cross-device interaction, touch, mobile devices, development teams, depth camera},
    venue = {Kobe, Japan},
    series = {ITS ’11},
}
  
@book{Brandon:2014,
    author = "Brandon, Sanders",
    title = "Mastering Leap Motion",
    year = "2014",
    publisher = "Packt Publishing",
    address = "Birmingham",
    isbn="978-1-783-55139-2",
}

@article{Brooke:1996,
    author={Brooke, John},
    title={{SUS-A quick and dirty usability scale}},
    journal={Usability evaluation in industry},
    pages={189--194},
    year={1996},
    publisher={CRC press},
}

@book{Brunelli:2009,
    author = "Brunelli, Roberto",
    title = "Template Matching Techniques in Computer Vision: Theory and Practice",
    year = "2009",
    publisher = "John Wiley \& Sons",
    address = "New York",
    isbn="978-0-470-51706-2",
}


@incollection{Bush:2011,
    author = {Bush, Ashley and Purao, Sandeep},
    title = {{Mapping UML Techniques to Design Activities}},
    booktitle = {Information Modeling in the New Millennium},
    publisher = {IDEA Publishing Group},
    year = {2011},
    editor = {Matti Rossi and Keng Siau},
    chapter = {12},
    pages = {199--217},
}

%===========================================================
% References starting by C
%===========================================================
@article{Cai:2014,
    author = "Cai, Wenjie and Uchida, Seiichi and Sakoe, Hiroaki",
    title = "Comparative performance analysis of stroke correspondence search methods for stroke-order free online multi-stroke character recognition",
    year = "2014",
    journal = "Frontiers of Computer Science volume",
    volume = "8",
    pages = "pages773–-784",
    doi= "https://doi.org/10.1007/s11704-014-3207-6",
    url="https://link.springer.com/article/10.1007Fs11704-014-3207-6",
}

@article{Cai:2019,
    author     = {Cai, Minghao and Tanaka, Jiro},
    journal    = {Hum.-Centric Comput. Inf. Sci.},
    title      = {{Go Together: Providing Nonverbal Awareness Cues to Enhance Co-Located Sensation in Remote Communication}},
    year       = {2019},
    issn       = {2192-1962},
    month      = dec,
    number     = {1},
    volume     = {9},
    address    = {Berlin, Heidelberg},
    articleno  = {180},
    doi        = {10.1186/s13673-019-0180-y},
    issue_date = {December 2019},
    keywords   = {Collaboration, Awareness, Togetherness, Remote communication, Mixed reality, Gesture, 360° view sharing},
    numpages   = {25},
    publisher  = {Springer-Verlag},
    url        = {https://doi.org/10.1186/s13673-019-0180-y},
}


@article{Calado:2022,
  author    = {Calado, Alexandre and Roselli, Paolo and Errico, Vito and Magrofuoco, Nathan and Vanderdonckt, Jean and Saggio, Giovanni},
  title     = {{A Geometric Model-Based Approach to Hand Gesture Recognition}},
  journal   = {{IEEE} Trans. Syst. Man Cybern. Syst.},
  volume    = {52},
  number    = {10},
  pages     = {6151--6161},
  year      = {2022},
  url       = {https://doi.org/10.1109/TSMC.2021.3138589},
  doi       = {10.1109/TSMC.2021.3138589},
  timestamp = {Sun, 02 Oct 2022 15:52:09 +0200},
  biburl    = {https://dblp.org/rec/journals/tsmc/CaladoREMVS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Callender:1979,
    ISSN = {00220655, 17453984},
    URL = {http://www.jstor.org/stable/1434452},
    abstract = {It has long been recognized that some procedures for estimating internal consistency reliability may be superior mathematically to the more commonly used methods such as Coefficient Alpha or odd-even split-half coefficients, but two problems have limited their practical application. One is computational difficulty, coupled with very limited empirical evidence of noticeable improvements in accuracy. The other problem--that of using sample coefficients to infer population reliability--is especially relevant for the better reliability estimators (those that maximize statistical functions) because of the possibility of overestimation due to capitalization on chance sampling error. An empirical comparison of MSPLIT maximized split-half coefficients, Guttman's λ 2, and Coefficient Alpha (Guttman's λ 3) showed that a cross-validation procedure applied to the MSPLIT coefficients produced a noticeable improvement in accuracy and thus seems to offer a solution for both practical limitations.},
    author = {Callender, John C. and Osburn, H. G.},
    journal = {Journal of Educational Measurement},
    number = {2},
    pages = {89--99},
    publisher = {[National Council on Measurement in Education, Wiley]},
    title = {{An Empirical Comparison of Coefficient Alpha, Guttman's Lambda - 2, and MSPLIT Maximized Split-Half Reliability Estimates}},
    volume = {16},
    year = {1979},
}


@article{Calvary:2003,
    author    = {Calvary, Ga{\"{e}}lle and Coutaz, Jo{\"{e}}lle and Thevenin, David and Limbourg, Quentin and Bouillon, Laurent and Vanderdonckt, Jean},
    title     = {{A Unifying Reference Framework for multi-target user interfaces}},
    journal   = {Interact. Comput.},
    volume    = {15},
    number    = {3},
    pages     = {289--308},
    year      = {2003},
    url       = {https://doi.org/10.1016/S0953-5438(03)00010-9},
    doi       = {10.1016/S0953-5438(03)00010-9},
    timestamp = {Fri, 13 Mar 2020 10:53:26 +0100},
    biburl    = {https://dblp.org/rec/journals/iwc/CalvaryCTLBV03.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Camgoz:2015,
    author="Camg{\"o}z, Necati Cihan and Kindiroglu, Ahmet Alp and Akarun, Lale",
    editor="Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten",
    title="Gesture Recognition Using Template Based Random Forest Classifiers",
    booktitle="Computer Vision - ECCV 2014 Workshops",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="579--594",
    url={https://link.springer.com/chapter/10.1007/978-3-319-16178-5_41},
    doi={10.1007/978-3-319-16178-5_41},
    abstract="This paper presents a framework for spotting and recognizing continuous human gestures. Skeleton based features are extracted from normalized human body coordinates to represent gestures. These features are then used to construct spatio-temporal template based Random Decision Forest models. Finally, predictions from different models are fused at decision-level to improve overall recognition performance. Our method has shown competitive results on the ChaLearn 2014 Looking at People: Gesture Recognition dataset. Trained on a dataset of 20 gesture vocabulary and 7754 gesture samples, our method achieved a Jaccard Index of {\$}{\$}0.74663{\$}{\$}on the test set, reaching 7th place among contenders. Among methods that exclusively used skeleton based features, our method obtained the highest recognition performance.",
    isbn="978-3-319-16178-5",
}


@inproceedings{Caputo:2015,
    author    = {Caputo, Fabio Marco and Giachetti, Andrea},
    booktitle = {Proceedings of the 11th Biannual Conference on Italian SIGCHI Chapter},
    title     = {Evaluation of Basic Object Manipulation Modes for Low-Cost Immersive Virtual Reality},
    year      = {2015},
    address   = {New York, NY, USA},
    pages     = {74–77},
    publisher = {Association for Computing Machinery},
    series    = {CHItaly 2015},
    doi       = {10.1145/2808435.2808439},
    isbn      = {9781450336840},
    venue  = {Rome, Italy},
    numpages  = {4},
    url       = {https://doi.org/10.1145/2808435.2808439},
}


@inproceedings{Caputo:2017,
    author = {Caputo, Fabio Marco and Prebianca, Pietro and Carcangiu, Alessandro and Spano, Lucio Davide and Giachetti, Andrea},
    title = {{A 3 Cent Recognizer: Simple and Effective Retrieval and Classification of Mid-Air Gestures from Single 3D Traces}},
    year = {2017},
    publisher = {Eurographics Association},
    address = {Goslar, DEU},
    url = {https://doi.org/10.2312/stag.20171221},
    doi = {10.2312/stag.20171221},
    booktitle = {Proceedings of the Conference on Smart Tools and Applications in Computer Graphics},
    pages = {9–15},
    numpages = {7},
    venue = {Catania, Italy},
    series = {STAG ’17},
}


@article{Caputo:2018,
    title = {{Comparing 3D trajectories for simple mid-air gesture recognition}},
    journal = "Computers \& Graphics",
    volume = "73",
    pages = "17 - 25",
    year = "2018",
    issn = "0097-8493",
    doi = "https://doi.org/10.1016/j.cag.2018.02.009",
    url = "http://www.sciencedirect.com/science/article/pii/S0097849318300335",
    author = "Caputo, Fabio Marco and Prebianca, Pietro and Carcangiu, Alessandro and Spano, Lucio Davide and Giachetti, Andrea",
    keywords = "Gestures recognition, Trajectory matching, Classification, Mid-air gestures, Gestures dataset",
}


@inproceedings{Caputo:2019,
    booktitle = {Eurographics Workshop on 3D Object Retrieval},
    editor = {Biasotti, Silvia and Lavoué, Guillaume and Veltkamp, Remco},
    title = {{Online Gesture Recognition}},
    author = {Caputo, Fabio Marco and Burato, S. and Pavan, Gianni and Voillemin, Théo and Wannous, Hazem and Vandeborre, Jean-Philippe and Maghoumi, Mehran and Taranta II, Eugene M. and Razmjoo, Alaleh and LaViola Jr., Joseph J. and Manganaro, Fabio and Pini, Stefano and Borghi, Guido and Vezzani, Roberto and Cucchiara, Rita and Nguyen, Hai{-}Dang and Tran, Minh{-}Triet and Giachetti, Andrea},
    year = {2019},
    publisher = {The Eurographics Association},
    ISSN = {1997-0471},
    ISBN = {978-3-03868-077-2},
    DOI = {10.2312/3dor.20191067},
    pages = {93-102},
    numpages = {10},
}


@article{Caputo:2021,
    author    = {Caputo, Ariel and Giachetti, Andrea and Soso, Simone and Pintani, Deborah and D'Eusanio, Andrea and Pini, Stefano and Borghi, Guido and Simoni, Alessandro and Vezzani, Roberto and Cucchiara, Rita and Ranieri, Andrea and Giannini, Franca and Lupinetti, Katia and Monti, Marina and Maghoumi, Mehran and LaViola Jr., Joseph J. and Le, Minh{-}Quan and Nguyen, Hai{-}Dang and Tran, Minh{-}Triet},
    title     = {{SHREC} 2021: Skeleton-based hand gesture recognition in the wild},
    journal   = {Computer Graphics},
    volume    = {99},
    pages     = {201--211},
    year      = {2021},
    url       = {https://doi.org/10.1016/j.cag.2021.07.007},
    doi       = {10.1016/j.cag.2021.07.007},
    timestamp = {Mon, 03 Jan 2022 22:08:02 +0100},
    biburl    = {https://dblp.org/rec/journals/cg/CaputoGSPDPBSVC21.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Carcangiu:2017,
    author="Carcangiu, Alessandro
    and Spano, Lucio Davide
    and Fumera, Giorgio
    and Roli, Fabio",
    editor="Battiato, Sebastiano
    and Gallo, Giovanni
    and Schettini, Raimondo
    and Stanco, Filippo",
    title="Gesture Modelling and Recognition by Integrating Declarative Models and Pattern Recognition Algorithms",
    booktitle="Image Analysis and Processing - ICIAP 2017          ",
    year="2017",
    publisher="Springer International Publishing",
    address="Cham",
    pages="84--95",
    abstract="Gesture recognition approaches based on computer vision and machine learning mainly focus on recognition accuracy and robustness. Research on user interface development focuses instead on the orthogonal problem of providing guidance for performing and discovering interactive gestures, through compositional approaches that provide information on gesture sub-parts. We make a first step toward combining the advantages of both approaches. We introduce DEICTIC, a compositional and declarative gesture description model which uses basic Hidden Markov Models (HMMs) to recognize meaningful pre-defined primitives (gesture sub-parts), and uses a composition of basic HMMs to recognize complex gestures. Preliminary empirical results show that DEICTIC exhibits a similar recognition performance as ``monolithic'' HMMs used in state-of-the-art vision-based approaches, retaining at the same time the advantages of declarative approaches.",
    isbn="978-3-319-68560-1",
}


@article{Carcangiu:2018,
    author = {Carcangiu, Alessandro and Spano, Lucio Davide},
    title = {G-Gene: A Gene Alignment Method for Online Partial Stroke Gestures Recognition},
    year = {2018},
    issue_date = {June 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {2},
    number = {EICS},
    url = {https://doi.org/10.1145/3229095},
    doi = {10.1145/3229095},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = jun,
    articleno = {article 13},
    numpages = {17},
    keywords = {online recognition, classification, feedback, compositional gesture modelling, gestures, feedforward, hidden markov models},
}


@article{Carcangiu:2019,
    title = "DEICTIC: A compositional and declarative gesture description based on hidden markov models",
    journal = "International Journal of Human-Computer Studies",
    volume = "122",
    pages = "113 - 132",
    year = "2019",
    issn = "1071-5819",
    doi = "https://doi.org/10.1016/j.ijhcs.2018.09.001",
    url = "http://www.sciencedirect.com/science/article/pii/S1071581918305081",
    author = "Carcangiu, Alessandro and Spano, Lucio Davide and Fumera, Giorgio and Roli, Fabio",
    keywords = "Gestures, Classification, Hidden markov models, Compositional gesture modelling, Declarative gesture modelling,",
    abstract = "The consumer-level devices that track the user’s gestures eased the design and the implementation of interactive applications relying on body movements as input. Gesture recognition based on computer vision and machine-learning focus mainly on accuracy and robustness. The resulting classifiers label precisely gestures after their performance, but they do not provide intermediate information during the execution. Human-Computer Interaction research focused instead on providing an easy and effective guidance for performing and discovering interactive gestures. The compositional approaches developed for solving such problem provide information on both the whole gesture and on its sub-parts, but they exploit heuristic techniques that have a low recognition accuracy. In this paper, we introduce DEICTIC, a compositional and declarative description for stroke gestures, which uses basic Hidden Markov Models (HMMs) to recognise meaningful predefined primitives (gesture sub-parts) and it composes them to recognise complex gestures. It provides information for supporting gesture guidance and it reaches an accuracy comparable with state-of-the-art approaches, evaluated on two datasets from the literature. Through a developer evaluation, we show that the implementation of a guidance system with DEICTIC requires an effort comparable to compositional approaches, while the definition procedure and the perceived recognition accuracy is comparable to machine learning.",
}


@inproceedings{Carcangiu:2019:IUI,
    author = {Carcangiu, Alessandro and Spano, Lucio Davide},
    title = {{Integrating Declarative Models and HMMs for Online Gesture Recognition}},
    year = {2019},
    isbn = {9781450366731},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3308557.3308709},
    doi = {10.1145/3308557.3308709},
    booktitle = {Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion},
    pages = {87–88},
    numpages = {2},
    keywords = {feedback, compositional gesture modelling, online recognition, hidden Markov models, gestures, feedforward},
    venue = {Marina del Ray, California},
    series = {IUI ’19},
}


@inproceedings{Casiez:2012,
	address = {New York, NY, USA},
	series = {{CHI} '12},
	title = {{1€ filter: a simple speed-based low-pass filter for noisy input in interactive systems}},
	isbn = {978-1-4503-1015-4},
	shorttitle = {1€ filter},
	url = {https://doi.org/10.1145/2207676.2208639},
	doi = {10.1145/2207676.2208639},
	abstract = {The 1 € filter ("one Euro filter") is a simple algorithm to filter noisy signals for high precision and responsiveness. It uses a first order low-pass filter with an adaptive cutoff frequency: at low speeds, a low cutoff stabilizes the signal by reducing jitter, but as speed increases, the cutoff is increased to reduce lag. The algorithm is easy to implement, uses very few resources, and with two easily understood parameters, it is easy to tune. In a comparison with other filters, the 1 € filter has less lag using a reference amount of jitter reduction.},
	urldate = {2020-12-29},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Casiez, Géry and Roussel, Nicolas and Vogel, Daniel},
	month = may,
	year = {2012},
	keywords = {precision, signal, filtering, jitter, lag, noise, responsiveness},
	pages = {2527--2530},
}


@inproceedings{Chatterjee:2015,
    author    = {Chatterjee, Ishan and Xiao, Robert and Harrison, Chris},
    booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
    title     = {{Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions}},
    year      = {2015},
    address   = {New York, NY, USA},
    pages     = {131–138},
    publisher = {Association for Computing Machinery},
    series    = {ICMI '15},
    doi       = {10.1145/2818346.2820752},
    isbn      = {9781450339124},
    keywords  = {eye tracking, sensors, input technologies, touch-free interaction, interaction techniques, cursor, pointing, free-space gestures},
    venue  = {Seattle, Washington, USA},
    numpages  = {8},
    url       = {https://doi.org/10.1145/2818346.2820752},
}


@book{Chen:2004,
    publisher = {Wiley \& Sons, New York, NY, USA},
    author = {Chen, Linfeng andVaradan,  V. V. and Ong, C. K. and Neo, Chye Poh},
    title = {{Microwave Electronics: Measurement and Materials Characterization}},
    year = {2004},
    series = {Technology \& Engineering},
}


@inproceedings{Chen:2014,
    author = {Chen, Xiang “Anthony” and Schwarz, Julia and Harrison, Chris and Mankoff, Jennifer and Hudson, Scott E.},
    title = {{Air+touch: Interweaving Touch \& in-Air Gestures}},
    year = {2014},
    isbn = {9781450330695},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2642918.2647392},
    doi = {10.1145/2642918.2647392},
    booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
    pages = {519–525},
    numpages = {7},
    keywords = {interaction techniques, free space gestures, input sensing, around device interaction, touch input},
    venue = {Honolulu, Hawaii, USA},
    series = {UIST ’14}
}
  

@inproceedings{Chen:2016,
    author = {Chen, Yineng and Su, Xiaojun and Tian, Feng and Huang, Jin and Zhang, Xiaolong (Luke) and Dai, Guozhong and Wang, Hongan},
    title = {{Pactolus: A Method for Mid-Air Gesture Segmentation within EMG}},
    year = {2016},
    isbn = {9781450340823},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2851581.2892492},
    doi = {10.1145/2851581.2892492},
    booktitle = {Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
    pages = {1760–1765},
    numpages = {6},
    keywords = {midas problem, EMG, mid-air gesture segmentation},
    venue = {San Jose, California, USA},
    series = {CHI EA ’16}
}


@phdthesis{Chen:2017,
    author       = {Chen, Zhaoxin},
    title        = {{Recognition and interpretation of multi-touch gesture interaction.}},
    school       = {{INSA} de Rennes, France},
    year         = {2017},
    url          = {https://tel.archives-ouvertes.fr/tel-01578068},
    timestamp    = {Tue, 21 Jul 2020 00:40:47 +0200},
    biburl       = {https://dblp.org/rec/phd/hal/Chen17a.bib},
    bibsource    = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Chen:2019,
	title = {Dynamic {Hand} {Gesture} {Classification} {Based} on {Multistatic} {Radar} {Micro}-{Doppler} {Signatures} {Using} {Convolutional} {Neural} {Network}},
	doi = {10.1109/RADAR.2019.8835796},
	abstract = {We propose a novel convolutional neural network (CNN) for dynamic hand gesture classification based on multistatic radar micro-Doppler signatures. The time-frequency spectrograms of micro-Doppler signatures at all the receiver antennas are adopted as the input to CNN, where data fusion of different receivers is carried out at an adjustable position. The optimal fusion position that achieves the highest classification accuracy is determined by a series of experiments. Experimental results on measured data show that 1) the accuracy of classification using multistatic radar is significantly higher than monostatic radar, and that 2) fusion at the middle of CNN achieves the best classification accuracy.},
	booktitle = {Proceedings of the {IEEE} International {Radar} {Conference}},
	series={RADAR '19},
	venue={Boston, MA, USA},
	dates={22-26 April 2019},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	author = {Chen, Zhaoxi and Li, Gang and Fioranelli, Francesco and Griffiths, Hugh},
	month = apr,
	year = {2019},
	note = {ISSN: 2375-5318},
	keywords = {classification accuracy, CNN, Convolution, convolutional neural nets, convolutional neural network, data fusion, dynamic hand gesture classification, Feature extraction, gesture recognition, image classification, image fusion, micro-Doppler, micro-Doppler signatures, multistatic radar, Multistatic radar, multistatic radar microdoppler signatures, optimal fusion position, Radar antennas, receiver antennas, Receivers, sensor fusion, Spectrogram, time-frequency analysis, time-frequency spectrograms},
	pages = {1--5},
}


@article{Cheng:2016,  
    author={Cheng, Hong and Yang, Lu and Liu, Zicheng},  
    journal={IEEE Transactions on Circuits and Systems for Video Technology},   
    title={{Survey on 3D Hand Gesture Recognition}},   
    year={2016},  
    volume={26},
    url={https://ieeexplore.ieee.org/document/7208833},
    doi={10.1109/TCSVT.2015.2469551},
    number={9},  
    pages={1659-1673},
}


@book{Chew:1990,
   Author = {Chew, Weng Cho},
   Title = {{Waves and Fields in Inhomogeneous Media}},
   Publisher = {Van Nostrand Reinhold},
   Address = {New York},
   Year = {1990},
}


@article{Chiang:2017,
    author     = {Chiang, Cheng-Chin and Wang, Ren-Hong and Chen, Bo-Ruei},
    journal    = {Pattern Recogn.},
    title      = {{Recognizing Arbitrarily Connected and Superimposed Handwritten Numerals in Intangible Writing Interfaces}},
    year       = {2017},
    issn       = {0031-3203},
    month      = jan,
    number     = {C},
    pages      = {15–28},
    volume     = {61},
    address    = {USA},
    doi        = {10.1016/j.patcog.2016.07.018},
    issue_date = {January 2017},
    keywords   = {Intangible writing interface, Dynamic time warping, Key numeral spotting, Finger gesture recognition, Graph path finding},
    numpages   = {14},
    publisher  = {Elsevier Science Inc.},
    url        = {https://doi.org/10.1016/j.patcog.2016.07.018},
}


@article{Choi:2014,
	title = {{A taxonomy and notation method for three-dimensional hand gestures}},
	volume = {44},
	issn = {0169-8141},
	url = {http://www.sciencedirect.com/science/article/pii/S0169814113001285},
	doi = {10.1016/j.ergon.2013.10.011},
	abstract = {Recently, studies on gesture-based interfaces have made an effort to improve the intuitiveness of gesture commands by asking users to define a gesture for a command. However, there are few methods to organize and notate user-defined gestures in a systematic approach. To resolve this, we propose a three-dimensional (3D) Hand Gesture Taxonomy and Notation Method. We first derived elements of a hand gesture by analyzing related studies and subsequently developed the 3D Hand Gesture Taxonomy based on the elements. Moreover, we devised a Notation Method based on a combination of the elements and also matched a code to each element for easy notation. Finally, we have verified the usefulness of the Notation Method by training participants to notate hand gestures and by asking another set of participants to recreate the notated gestures. In short, this research proposes a novel and systematic approach to notate hand gesture commands.
    Relevance to industry
    This study develops a 3D Hand Gesture Taxonomy and Notation Method. The results of this study can be used as a guideline to organize hand gestures for enhancing the usability of gesture-based interfaces.},
	language = {en},
	number = {1},
	urldate = {2021-01-06},
	journal = {International Journal of Industrial Ergonomics},
	author = {Choi, Eunjung and Kim, Heejin and Chung, Min K.},
	month = jan,
	year = {2014},
	keywords = {Hand gesture commands, Hand gesture elements, Hand gesture notation method, Hand gesture taxonomy},
	pages = {171--188},
}


@article{Choi:2019,
	title = {Short-{Range} {Radar} {Based} {Real}-{Time} {Hand} {Gesture} {Recognition} {Using} {LSTM} {Encoder}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2903586},
	abstract = {Due to the development of short-range radar with high-resolution, the radar sensor has a high potential to be used in real human-computer interaction (HCI) applications. The radar sensor has advantages over optical cameras in that it is unaffected by illumination and it is able to detect the objects in an occluded environment. This paper proposes a hand gesture recognition system for a real-time application of HCI using 60 GHz frequency-modulated continuous wave (FMCW) radar, Soli, developed by Google. The overall system includes signal processing part that generates range-Doppler map (RDM) sequences without clutter and machine learning part including a long short-term memory (LSTM) encoder to learn the temporal characteristics of the RDM sequences. A set of data is collected from 10 participants for the experiment. The proposed hand gesture recognition system successfully distinguishes 10 gestures with a high classification accuracy of 99.10\%. It also recognizes the gestures of a new participant with an accuracy of 98.48\%.},
	journal = {IEEE Access},
	author = {Choi, J. and Ryu, S. and Kim, J.},
	year = {2019},
	keywords = {Clutter, CW radar, Doppler effect, Doppler radar, FM radar, FMCW radar, frequency 60.0 GHz, frequency-modulated continuous wave radar, gesture recognitio, gesture recognition, Gesture recognition, hand gesture recognition system, HCI applications, human computer interaction, human-computer interaction applications, image classification, Laser radar, long short-term memory encoder, LSTM encoder, machine learning, occluded environment, optical cameras, radar clutter, range-Doppler map sequence generation, RDM sequence generation, real-time interaction, short-range radar resolution, short-range radar sensor, signal processing, Signal processing},
	pages = {33610--33618},
}


@inproceedings{Clark:2016,
    author    = {Clark, Andrew and Moodley, Deshendran},
    booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
    title     = {{A System for a Hand Gesture-Manipulated Virtual Reality Environment}},
    year      = {2016},
    address   = {New York, NY, USA},
    publisher = {Association for Computing Machinery},
    series    = {SAICSIT '16},
    articleno = {10},
    doi       = {10.1145/2987491.2987511},
    isbn      = {9781450348058},
    keywords  = {Machine Learning, Virtual Reality, Leap Motion Controller, Hand Gesture Recognition},
    venue  = {Johannesburg, South Africa},
    numpages  = {10},
    url       = {https://doi.org/10.1145/2987491.2987511},
}


@book{Cohen:1988,
    publisher = {Routledge},
    author = {Cohen, Jacob},
    title = {{Statistical Power Analysis for the Behavioral Sciences}},
    edition = {2},
    isbn={9780805802832},
    url={https://www.routledge.com/Statistical-Power-Analysis-for-the-Behavioral-Sciences/Cohen/p/book/9780805802832},
    year = {1988},
}


@article{Cohen:1992,
    author={Cohen, Jacob},
    journal={Psychological Bulletin},
    title={{A power primer}},   
    year={1992},  
    volume={112},
    url={https://doi.apa.org/doiLanding?doi=10.1037/0033-2909.112.1.155},
    doi={https://doi.org/10.1037/0033-2909.112.1.155},
    number={1},  
    pages={155--159},
}


@book{Cole:2011,
   title =     {{Heat Conduction Using Green's Functions}},
   author =    {Cole, Kevin D. and Beck, James V. and Haji-Sheikh, A. and Litkouhi, Bahman},
   publisher = {CRC Press},
   address = {Boca Raton, Florida, United States},
   isbn =      {9781439813546},
   year =      {2011},
   series =    {Series in Computational and Physical Processes in Mechanics and Thermal Sciences},
   edition =   {2},
   url =       {https://www.routledge.com/Heat-Conduction-Using-Greens-Functions/Cole-Beck-Haji-Sheikh-Litkouhi/p/book/9781439813546\#},
}


@misc{Colgan:2017, 
    title={{How Does the Leap Motion Controller Work?}}, 
    url={https://blog.leapmotion.com/hardware-to-software-how-does-the-leap-motion-controller-work/}, 
    journal={Leap Motion Blog}, 
    author={Colgan, Alex}, 
    year={2017}, 
    month=jan,
} 


@inproceedings{Cook:2016,
    title        = {{Enabling Gesture Interaction with 3D Point Cloud}},
    author       = {Cook, Harisson and Nguyen, Quang Vinh and Simoff, Simeone and Huang, Mao Lin},
    year         = 2016,
    month        = jun,
    booktitle    = {Proceedings of the 24th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision},
    publisher    = {Computer Science Research Notes CSRN},
    volume       = 2602,
    pages        = {59–68},
    isbn         = {978-80-86943-58-9},
    url          = {https://opus.lib.uts.edu.au/handle/10453/45407},
}


@inproceedings{Copic:2019,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '19},
	title = {The {Missing} {Interface}: {Micro}-{Gestures} on {Augmented} {Objects}},
	isbn = {978-1-4503-5971-9},
	shorttitle = {The {Missing} {Interface}},
	url = {https://doi.org/10.1145/3290607.3312986},
	doi = {10.1145/3290607.3312986},
	abstract = {Augmenting arbitrary physical objects with digital content leads to the missing interface problem, because those objects were never designed to incorporate such digital content and so they lack a user interface. A review of related work reveals that current approaches fail due to limited detection fidelity and spatial resolution. Our proposal, based on Google Soli's radar sensing technology, is designed to detect micro-gestures on objects with sub-millimeter precision. Preliminary results with a custom gesture set show that Soli's core features and traditional machine learning models (Random Forest and Support Vector Machine) do not lead to robust recognition accuracy, and so more advanced techniques should be used instead, possibly incorporating additional sensor features.},
	urldate = {2020-12-21},
	booktitle = {Extended {Abstracts} of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Copič Pucihar, Klen and Sandor, Christian and Kljun, Matjaž and Huerst, Wolfgang and Plopski, Alexander and Taketomi, Takafumi and Kato, Hirokazu and Leiva, Luis A.},
	month = may,
	year = {2019},
	keywords = {augmented reality, google soli, micro-gesture recognition, millimeter-wave radar},
	pages = {1--6},
	file = {Čopič Pucihar et al. - 2019 - The Missing Interface Micro-Gestures on Augmented.pdf:C\:\\Users\\asluyters\\Zotero\\storage\\42B36A47\\Čopič Pucihar et al. - 2019 - The Missing Interface Micro-Gestures on Augmented.pdf:application/pdf},
}


@article{Copic:2022,
    author    = {Copič Pucihar, Klen and Attygalle, Nuwan T. and Kljun, Matjaž and Sandor, Christian and Leiva, Luis A.},
    title     = {{Solids on Soli: Millimetre-Wave Radar Sensing through Materials}},
    journal   = {Proc. {ACM} Human-Computer Interaction},
    volume    = {6},
    number    = {{EICS}},
    pages     = {1--47},
    year      = {2022},
    articleno = {156},
    numpages  = {21},
    month     = jun,
    url       = {https://luis.leiva.name/web/docs/papers/soli-eics2022-preprint.pdf},
    doi       = {10.1145/3532212}
}


@phdthesis{Cornet:2023,
    title = {{Human-Computer Interaction with an Armband by Electromyography}},
    author = {Cornet, Erwin},
    abstract = {Thalmic Lab’s Myo Armband is a wearable device that arrived on the market several years ago. It embeds both hardware and software to transmit data that is continuously generated by sensors to a computer linked to it in order to recognize a predefined set of gestures performed by the user. The device has been studied several times in the past, in various contexts and with different techniques. However, the product was recently discontinued, and the support and documentation gone with it. The objective of this thesis is to recreate the pipeline, from the extraction of data to the insertion of processed samples into a custom framework designed for gesture recognition, emphasizing on modularity and reusability. This task requires to explore the possibility of re-engineering the entire pipeline from scratch, or with what is reminiscent of the past, for the benefit of anyone who would be in the same circumstances. It is also an opportunity to extend the set of gestures and evaluate the behaviour of the system with custom parameters. In this work, we will talk low-level programming of communication protocols, data gathering, GUI, processing of data, properties of sensors, and more.},
    Keywords = {Myo , Armband , QuantumLeap , Wearable Device , Gesture Recognition},
    language = {Anglais},
    year = {2023},
    url = {http://hdl.handle.net/2078.1/thesis:43237},
    school = {UCL - Ecole polytechnique de Louvain},
}


@inproceedings{Costabile:2008,
    author = {Costabile, Maria Francesca and Mussio, Piero and Parasiliti Provenza, Loredana and Piccinno, Antonio},
    title = {{End Users as Unwitting Software Developers}},
    year = {2008},
    isbn = {9781605580340},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1370847.1370849},
    doi = {10.1145/1370847.1370849},
    abstract = {The widespread use of personal software systems and the boom of the so-called Web 2.0 is erasing the distinctions between those who create software products and those who use or consume them. End users are increasingly involved in the design and development of the tools they use. Unfortunately, there is a high incidence of errors in applications developed by end users. In this paper, a view on end-user development is outlined, which identifies the communication gap between end users and professional software developers as one main source of errors. The spectrum of users that lie between pure end users and professional developers is examined. In particular, the focus is on a particular type of end users that are very active in shaping software tools to their needs without being aware that they are programming: in short, they are unwitting programmers. Their characteristics and their need of appropriate development techniques and environments are analyzed. Finally, the meta-design participatory approach we have developed is briefly described to show how it fills the communication gap and well supports the activities of unwitting programmers.},
    booktitle = {Proceedings of the 4th International Workshop on End-User Software Engineering},
    pages = {6–10},
    numpages = {5},
    keywords = {end-user development, meta-design, end user, user classification},
    venue = {Leipzig, Germany},
    series = {WEUSE '08},
}


@inproceedings{Coyette:2007,
    author="Coyette, Adrien and Schimke, Sascha and Vanderdonckt, Jean and Vielhauer, Claus",
    editor="Baranauskas, C{\'e}cilia and Palanque, Philippe and Abascal, Julio and Barbosa, Simone Diniz Junqueira",
    title="Trainable Sketch Recognizer for Graphical User Interface Design",
    booktitle="Proc. of 11th {IFIP} {TC} 13 International Conference on Human-Computer Interaction, {INTERACT} '07",
    venue  = {Rio de Janeiro, Brazil},
    dates     = {September 10-14, 2007},
    year="2007",
    publisher="Springer",
    address="Berlin, Heidelberg",
    pages="124--135",
    abstract="In this paper we present a new algorithm for automatic recognition of hand drawn sketches based on the Levenshtein distance. The purpose for drawing sketches in our application is to create graphical user interfaces in a similar manner as the well established paper sketching. The new algorithm is trainable by every user and improves the recognition performance of the techniques which were used before for widget recognition. In addition, this algorithm ay serve for recognizing other types of sketches, such as letters, figures, and commands. In this way, there is no modality disruption at sketching time.",
    isbn="978-3-540-74796-3",
}


@article{Creed:2018,
    author = {Creed, Chris},
    title = {{Assistive technology for disabled visual artists: exploring the impact of digital technologies on artistic practice}},
    journal = {Disability \& Society},
    volume = {33},
    number = {7},
    pages = {1103-1119},
    year = {2018},
    publisher = {Routledge},
    doi = {10.1080/09687599.2018.1469400},
    URL = {https://doi.org/10.1080/09687599.2018.1469400},
    eprint = {https://doi.org/10.1080/09687599.2018.1469400},
}


%===========================================================
% References starting by D
%===========================================================
@inproceedings{Daniels:2014,
    author    = {Daniels, Zachary A. and Stinson, Steven R. and Tian, Shenchi and Mulbry, Evan and Chen, Brian Y.},
    booktitle = {Proceedings of the 2014 Workshop on Mobile Augmented Reality and Robotic Technology-Based Systems},
    title     = {{A Gesture-Based Interface for the Exploration and Classification of Protein Binding Cavities}},
    year      = {2014},
    address   = {New York, NY, USA},
    pages     = {47–50},
    publisher = {Association for Computing Machinery},
    series    = {MARS '14},
    doi       = {10.1145/2609829.2609838},
    isbn      = {9781450328234},
    keywords  = {gesture-recognition, human computer interaction, protein structure analysis},
    venue  = {Bretton Woods, New Hampshire, USA},
    numpages  = {4},
    url       = {https://doi.org/10.1145/2609829.2609838},
}


@article{DeCoster:2016,  
    author={De Coster, Albéric and Tran, Anh Phuong and Lambot, S{\'{e}}bastien},  
    journal={IEEE Transactions on Geoscience and Remote Sensing},   
    title={{Fundamental Analyses on Layered Media Reconstruction Using GPR and Full-Wave Inversion in Near-Field Conditions}},   
    year={2016},  
    volume={54},  
    number={9},  
    pages={5143-5158},  
    doi={10.1109/TGRS.2016.2556862},
}


@ARTICLE{DeCoster:2018a,
    author={De Coster, Albéric and Lambot, S{\'{e}}bastien},
    journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
    title={{Fusion of Multifrequency GPR Data Freed From Antenna Effects}}, 
    year={2018},
    volume={11},
    number={2},
    pages={664-674},
    doi={10.1109/JSTARS.2018.2790419},
}


@article{DeCoster:2019,
    author    = {De Coster, Alberic and Lambot, S{\'{e}}bastien},
    title     = {{Full-Wave Removal of Internal Antenna Effects and Antenna-Medium Interactions  for Improved Ground-Penetrating Radar Imaging}},
    journal   = {{IEEE} Trans. Geosci. Remote. Sens.},
    volume    = {57},
    number    = {1},
    pages     = {93--103},
    year      = {2019},
    url       = {https://doi.org/10.1109/TGRS.2018.2852486},
    doi       = {10.1109/TGRS.2018.2852486},
    timestamp = {Tue, 12 May 2020 16:46:42 +0200},
    biburl    = {https://dblp.org/rec/journals/tgrs/CosterL19.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Dekker:2017,
	title = {Gesture recognition with a low power {FMCW} radar and a deep convolutional neural network},
	doi = {10.23919/EURAD.2017.8249172},
	abstract = {Gesture recognition with radar enables remote control of consumer devices such as audio equipment, television sets and gaming consoles. In this paper, experimental results of hand gesture recognition with a low power FMCW radar and a deep convolutional neural network (CNN) are presented. The FMCW radar operates in the 24 GHz ISM frequency band and has an effective isotropic radiated power level of 0 dBm. Since low power consumption is a key aspect for application in consumer devices, the FMCW radar has only one receive channel which is different from other FMCW radars with multiple receive channels that have been described in literature. The recognition of gestures is performed with a deep convolutional neural network that is trained and tested with micro-Doppler spectrograms yielding excellent recognition performance in a simple test case consisting of 3 different gestures. A comparison of the training and test results for an amplitude spectrogram and a complex-valued spectrogram as the CNN input shows that in this test case there is no major benefit of using the phase information in the spectrogram.},
	booktitle = {Proceedings of the {European} {Radar} {Conference}},
	series={{EURAD} '17},
	author = {Dekker, Bastiaan and Jacobs, Sebastiaan and Kossen, A. S. and Kruithof, Maarten C. and Huizing, Albert G. and Geurts, M.},
	month = oct,
	year = {2017},
	keywords = {24 GHz, 24 GHz ISM frequency band, amplitude spectrogram, CNN, complex-valued spectrogram, consumer devices, convolution, convolutional neural network, CW radar, deep convolutional neural network, deep learning, Doppler radar, feedforward neural nets, FM radar, FMCW, frequency 24.0 GHz, gesture, gesture recognition, Gesture recognition, hand gesture recognition, low power consumption, low power FMCW radar, low power radar device, micro-Doppler spectrograms, Optical sensors, phase information, power level, radar, Radar, Radar antennas, recognition, remote control, Spectrogram, telecontrol, Training},
	pages = {163--166},
}


@inproceedings{Delamare:2016,
    author = {Delamare, William and Janssoone, Thomas and Coutrix, C\'{e}line and Nigay, Laurence},
    title = {{Designing 3D Gesture Guidance: Visual Feedback and Feedforward Design Options}},
    year = {2016},
    isbn = {9781450341318},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2909132.2909260},
    doi = {10.1145/2909132.2909260},
    booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
    pages = {152–159},
    numpages = {8},
    keywords = {Guidance, 3D hand gesture, Feedback, Feedforward},
    venue = {Bari, Italy},
    series = {AVI ’16},
}


@inproceedings{Delamare:2019,
    author = {Delamare, William and Silpasuwanchai, Chaklam and Sarcar, Sayan and Shiraki, Toshiaki and Ren, Xiangshi},
    title = {{On Gesture Combination: An Exploration of a Solution to Augment Gesture Interaction}},
    year = {2019},
    isbn = {9781450368919},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3343055.3359706},
    doi = {10.1145/3343055.3359706},
    abstract = {Current gesture interaction paradigm mainly involves a one-to-one gesture-command mapping. This leads to memorability issues regarding (1) the mapping - as each new command requires a new gesture, and (2) the gestures specifics (e.g., motion paths) - that can be complex to leverage the recognition of several gestures. We explore the concept of combining 3D gestures when interacting in smart environments. We first propose a design space to characterize the temporal and spatial combination aspects, and the gesture types used by the combination. We then report results from three user studies in the context of smart TV interaction. The first study reveals that end-users can create gesture sets with combinations fully optimized to reuse gestures. The second study shows that combining gestures can lead to improved memorability compared to single gestures. The third study reveals that preferences for gestures combination appear when single gestures have an abstract gesture-command mapping.},
    booktitle = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
    pages = {135–146},
    numpages = {12},
    keywords = {two-handed interaction, mid-air interaction, gestural interaction, gesture combination, freehand gestures},
    venue = {Daejeon, Republic of Korea},
    series = {ISS '19},
}


@inproceedings{DePrisco:2016,
    author    = {De Prisco, Roberto and Malandrino, Delfina and Zaccagnino, Gianluca and Zaccagnino, Rocco},
    booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
    title     = {{Natural User Interfaces to Support and Enhance Real-Time Music Performance}},
    year      = {2016},
    address   = {New York, NY, USA},
    pages     = {204–211},
    publisher = {Association for Computing Machinery},
    series    = {AVI '16},
    doi       = {10.1145/2909132.2909249},
    isbn      = {9781450341318},
    keywords  = {Music Performance, Neural Networks, Evaluation, Natural User Interfaces},
    venue  = {Bari, Italy},
    numpages  = {8},
    url       = {https://doi.org/10.1145/2909132.2909249},
}


@inproceedings{DeSmedt:2017,
    author = {De Smedt, Quentin and Wannous, Hazem and Vandeborre, Jean-Philippe and Guerry, Joris and Saux, Bertrand Le and Filliat, David},
    title = {{3D Hand Gesture Recognition Using a Depth and Skeletal Dataset: SHREC’17 Track}},
    year = {2017},
    publisher = {Eurographics Association},
    address = {Goslar, DEU},
    url = {https://doi.org/10.2312/3dor.20171049},
    doi = {10.2312/3dor.20171049},
    booktitle = {Proceedings of the Workshop on 3D Object Retrieval},
    pages = {33–38},
    numpages = {6},
    venue = {Lyon, France},
    series = {3Dor ’17},
}
  
 
@article{Dessi:2020,
    author = {Dess\`{\i}, Stefano and Spano, Lucio Davide},
    title = {{DG3: Exploiting Gesture Declarative Models for Sample Generation and Online Recognition}},
    year = {2020},
    issue_date = {June 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {4},
    number = {EICS},
    url = {https://doi.org/10.1145/3397870},
    doi = {10.1145/3397870},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = jun,
    articleno = {82},
    numpages = {21},
    keywords = {\$-family, sub-part identification, online recognition, gestures, strokes, template recognition, sample generation},
}


@article{Dey:2001,
    author    = {Dey, Anind K.},
    title     = {{Understanding and Using Context}},
    journal   = {Pers. Ubiquitous Comput.},
    volume    = {5},
    number    = {1},
    pages     = {4--7},
    year      = {2001},
    url       = {https://doi.org/10.1007/s007790170019},
    doi       = {10.1007/s007790170019},
    timestamp = {Thu, 09 Jul 2020 22:41:35 +0200},
    biburl    = {https://dblp.org/rec/journals/puc/Dey01.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Dingler:2018,
    author = {Dingler, Tilman and Rzayev, Rufat and Shirazi, Alireza Sahami and Henze, Niels},
    title = {{Designing Consistent Gestures Across Device Types: Eliciting RSVP Controls for Phone, Watch, and Glasses}},
    year = {2018},
    isbn = {9781450356206},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3173574.3173993},
    doi = {10.1145/3173574.3173993},
    abstract = {In the era of ubiquitous computing, people expect applications to work across different devices. To provide a seamless user experience it is therefore crucial that interfaces and interactions are consistent across different device types. In this paper, we present a method to create gesture sets that are consistent and easily transferable. Our proposed method entails 1) the gesture elicitation on each device type, 2) the consolidation of a unified gesture set, and 3) a final validation by calculating a transferability score. We tested our approach by eliciting a set of user-defined gestures for reading with Rapid Serial Visual Presentation (RSVP) of text for three device types: phone, watch, and glasses. We present the resulting, unified gesture set for RSVP reading and show the feasibility of our method to elicit gesture sets that are consistent across device types with different form factors.},
    booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
    pages = {1–12},
    numpages = {12},
    keywords = {rsvp, gesture elicitation, consistency, transferability, design methods},
    venue = {Montreal QC, Canada},
    series = {CHI '18},
}


@inproceedings{Dong:2015,
    author = {Dong, Haiwei and Figueroa, Nadia and El Saddik, Abdulmotaleb},
    title = {{An Elicitation Study on Gesture Attitudes and Preferences Towards an Interactive Hand-Gesture Vocabulary}},
    year = {2015},
    isbn = {9781450334594},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2733373.2806385},
    doi = {10.1145/2733373.2806385},
    abstract = {With the introduction of new depth sensing technologies, interactive hand-gesture devices are rapidly emerging. However, the hand-gestures used in these devices do not follow a common vocabulary, making certain control command device-specific. In this paper we present an initial effort to create a standardized interactive hand-gesture vocabulary for the next generation of television applications. We conduct a user-elicitation study using a survey in order to define a common vocabulary for specific control commands, such as Volume up/down, Menu open/close, etc. This survey is entirely user-oriented and thus it has two phases. In the first phase, we ask open questions about specific commands. In the second phase, we use the answers suggested from the first phase to create a multiple choice questionnaire. Based on the results from the survey, we study the gesture attitudes and preferences between gender groups, and between age groups with a quantitative and qualitative statistical analysis. Finally, the hand-gesture vocabulary is derived after applying an agreement analysis on the user-elicited gestures. The proposed methodology for gesture set design is comparable with existing methodologies and yields higher agreement levels than relevant user-elicited studies in the field.},
    booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
    pages = {999–1002},
    numpages = {4},
    keywords = {hand-gesture interaction, kinect, preferences and attitudes},
    venue = {Brisbane, Australia},
    series = {MM '15},
}


@inproceedings{Drossis:2013,
    author="Drossis, Giannis and Grammenos, Dimitris and Birliraki, Chryssi and Stephanidis, Constantine",
    editor="Stephanidis, Constantine",
    title={{MAGIC: Developing a Multimedia Gallery Supporting mid-Air Gesture-Based Interaction and Control}},
    booktitle="HCI International 2013 - Posters' Extended Abstracts",
    year="2013",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="303--307",
    abstract="Touchless remote interaction empowers users to interact with systems at a distance without the burden of actually coming to physical contact with any tangible object. The research presented in this paper focuses on motion-based interaction in public spaces through hand detection using Microsoft's Kinect, in order to allow natural interaction in mid-air. The paper presents the development of a system that allows browsing and exploring large collections of multimedia information (images and videos).",
    isbn="978-3-642-39473-7",
}


@inproceedings{Du:2019,
	title = {Design of {Gesture} {Recognition} {System} {Based} on {77GHz} {Millimeter} {Wave} {Radar}},
	doi = {10.1109/ICMMT45702.2019.8992849},
	abstract = {In this paper a gesture recognition system based on millimeter wave radar (MMW) will be presented. The radio frequency (RF) transceiver circuit is built with the TI-manufactured AWR1443 highly integrated frequency modulated continuous wave (FMCW) RF transceiver chip which has an operating frequency of 76-81 GHz, a 3-channel transmitter, and a 4-channel receptor. The antenna array is constructed with a 1 x 16 microstrip antenna. The deep learning is conducted using TensorFlow on the sample data acquired from an ADC sampling done at the radar front-end section, which ultimately realizes the function of gesture recognition.},
	booktitle = {2019 {International} {Conference} on {Microwave} and {Millimeter} {Wave} {Technology} ({ICMMT})},
	author = {Du, Chong Yang and Wang, Xin Huai and Yuan, Zuan Xing and Xu, Yin},
	month = may,
	year = {2019},
	keywords = {1 x 16 microstrip antenna, 3-channel transmitter, 4-channel receptor, antenna arrays, continuous wave RF transceiver chip, CW radar, FM radar, frequency 76.0 GHz to 81.0 GHz, frequency modulation, gesture recognition, gesture recognition system, microstrip antenna arrays, microstrip antennas, millimeter wave radar, millimetre wave radar, operating frequency, radar antennas, radar front-end section, radio frequency transceiver circuit, road vehicle radar, TI-manufactured AWR1443 highly integrated frequency},
	pages = {1--3},
}


@article{Du:2020,
	title = {Enhanced {Multi}-{Channel} {Feature} {Synthesis} for {Hand} {Gesture} {Recognition} {Based} on {CNN} {With} a {Channel} and {Spatial} {Attention} {Mechanism}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3010063},
	abstract = {Millimeter-wave (MMW) radar hand gesture recognition technology is becoming important in many electronic device control applications. Currently, most existing approaches utilize the radical and micro-Doppler features from single-channel MMW radar, which ignores the different importance of the information contained in the micro-Doppler feature background or target areas. In this paper, we propose an algorithm for hand gesture recognition jointly using multi-channel signatures. The algorithm blends the information of both micro-Doppler features and instantaneous angles (azimuth and elevation) to accomplish hand gesture recognition performed with the convolutional neural network (CNN). To have a better features fusion and make CNN focus on the most important target signal regions and suppress the unnecessary noise areas, we apply the channel and spatial attention-based feature refinement modules. We also employ gesture movement mechanism-based data augmentation for more effective training to alleviate potential overfitting. Extensive experiments demonstrate the effectiveness and superiorities of the proposed algorithm. This method achieves a correct classification rate of 96.61\%, approximately 5\% higher than that of the single-channel-based recognition strategy as measured based on MMW radar datasets.},
	journal = {IEEE Access},
	author = {Du, Chuan and Zhang, Lei and Sun, Xiping and Wang, Junxu and Sheng, Jialian},
	year = {2020},
	keywords = {Azimuth, channel and spatial attention mechanism, CNN, convolutional neural nets, convolutional neural network, data augmentation, electronic device control applications, enhanced multichannel feature synthesis, feature extraction, features fusion, gesture movement mechanism-based data augmentation, gesture recognition, Gesture recognition, Hand gesture recognition, image classification, microDoppler features, Millimeter wave radar, millimeter-wave radar hand gesture recognition technology, MMW radar datasets, multi-channel signatures, multichannel signatures, Radar antennas, Sensors, single-channel MMW radar, single-channel-based recognition strategy, spatial attention-based feature refinement modules, target areas, target signal regions, Task analysis},
	pages = {144610--144620},
}

  
@book{Duda:2000,
    publisher = {Wiley \& Sons, New York},
    author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
    title = {Pattern Classification},
    year = {2000},
}


@article{Dumas:2019,
    author = {Dumas, Catherine and Erdelez, Sanda and Pomerantz, Jeffrey and Parthiban, Vik},
    title = {{Usability testing of LUI: A new human-computer interface for large displays}},
    journal = {Proceedings of the Association for Information Science and Technology},
    volume = {56},
    number = {1},
    pages = {645-647},
    keywords = {usability testing, gesture-based technology, extended reality},
    doi = {https://doi.org/10.1002/pra2.118},
    url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/pra2.118},
    eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.118},
    abstract = {ABSTRACT A usability study was conducted on a new human-computer web interface for large displays created by researchers from the Media Lab at MIT (Massachusetts Institute of Technology). Data was collected by observing 12 subjects who were asked interact with a large screen monitor using a set of gestures. The subjects were also asked to complete a survey after they participated. The purpose of the study was to discover new methods of how people interact with a large display using gestures. By understanding how people use this interface, novel applications that reflect user preferences can be designed. Our findings discovered usability and technology issues that were given to the creators of the web-interface that was evaluated in this study. We also created usability testing protocol that will be used in future studies of this gesture-based application.},
    year = {2019},
}




%===========================================================
% References starting by E
%===========================================================
@misc{Ecma:2017,
    author = "ECMA International",
    title = "Standard ECMA-404 -- The JSON Data Interchange Syntax",
    month = dec,
    year = "2017",
    edition = "2nd",
    url = "https://www.ecma-international.org/publications/standards/Ecma-404.htm",
}


@inproceedings{Eggimann:2019,
	title = {Low {Power} {Embedded} {Gesture} {Recognition} {Using} {Novel} {Short}-{Range} {Radar} {Sensors}},
	doi = {10.1109/SENSORS43011.2019.8956617},
	abstract = {This work proposes a low-power high-accuracy embedded hand-gesture recognition using low power short-range radar sensors. The hardware and software match the requirements for battery-operated wearable devices. A 2D Convolutional Neural Network (CNN) using range frequency Doppler features is combined with a Temporal Convolutional Neural Network (TCN) for time sequence prediction. The final algorithm has a model size of only 45723 parameters, yielding a memory footprint of only 91kB. Two datasets containing 11 challenging hand gestures performed by 26 different people have been recorded containing a total of 20210 gesture instances. On the 11 hands, gestures and an accuracy of 87\% (26 users) and 92\% (single user) have been achieved. Furthermore, the prediction algorithm has been implemented in the GAP8 Parallel Ultra-Low-Power processor by GreenWaves Technologies, showing that live-prediction is feasible with only 21mW of power consumption for the full gesture prediction neural network.},
	booktitle = {2019 {IEEE} {SENSORS}},
	author = {Eggimann, Manuel and Erb, Jonas and Mayer, Philipp and Magno, Michele. and Benini, Luca},
	month = oct,
	year = {2019},
	note = {ISSN: 2168-9229},
	keywords = {2D convolutional neural network using range frequency Doppler features, battery-operated wearable devices, challenging hand gestures, convolutional neural nets, Embedded Artificial Intelligence, GAP8 parallel ultra-low-power processor, gesture instances, gesture prediction neural network, gesture recognition, Gesture recognition, Low power, low power short-range radar sensors, low-power high-accuracy embedded hand-gesture recognition, Mini-Radar sensors, novel short-range radar sensors, power consumption, prediction algorithm, radar detection, temporal convolutional neural network, time sequence prediction, Wearable, wearable computers},
	pages = {1--4},
}


@inproceedings{Ehrnsperger:2019,
	title = {Performance {Investigation} of {Machine} {Learning} {Algorithms} for {Simple} {Human} {Gesture} {Recognition} {Employing} an {Ultra} {Low} {Cost} {Radar} {System}},
	abstract = {Radar based gesture recognition offers great opportunities to increase user-friendliness of countless applications at home, in transportation and for industries. Here, not only data-intensive image and video processing, but also 1D multior single-channel time-series signals are in focus. We examine classical machine learning (ML) approaches and compare them in a reproducible manner. We evaluate the performance of naive methods-such as threshold detection (THD)-and classical ML methods-such as the support vector machine (SVM). The performance is hereby judged by elements such as accuracy, false-positive rate (FPR), training and prediction time, hardware (HW) requirements and real-time capabilities as well as the size of the classifier. To create the library needed for the given investigation, a two channel continuous wave (CW) modulated radar system with carrier frequency of 10 GHz has been employed. We conclude that naive methods are outperformed by all investigated classical ML methodologies. The results in terms of accuracy and FPR are satisfactory. However, there are large differences between naive and ML methods in terms of HW requirements and real time performance. In conclusion, classical ML methods fulfil the defined requirements satisfactorily, only the real-time performance on low-performance HW is limited due to the required computing power. Thus, the algorithms are a good choice for gesture recognition-of 1D multior single-channel time-series signals-if applied correctly.},
	booktitle = {2019 {Kleinheubach} {Conference}},
	author = {Ehrnsperger, Matthias G. and Hoese, Henri L. and Siart, Uwe and Eibert, Thomas F.},
	month = sep,
	year = {2019},
	keywords = {1D multior single-channel time-series signals, Artificial Intelligence, channel continuous wave modulated radar system, classical machine learning approaches, classical ML methods, countless applications, data-intensive image, Feature extraction, FPR, frequency 10.0 GHz, gesture recognition, Gesture recognition, Gesture Recognition, Hardware, hardware requirements, HW requirements, investigated classical ML methodologies, learning (artificial intelligence), Low-Cost-Radar, low-performance HW, Machine Learning, naive methods, Neural Networks, prediction time, Radar, radar based gesture recognition, real-time capabilities, real-time performance, Real-time systems, reproducible manner, simple human gesture recognition, support vector machine, support vector machines, Support vector machines, threshold detection, Training, ultra low cost radar system, user-friendliness, video processing},
	pages = {1--4},
}


@inproceedings{Ehrnsperger:2020,
	title = {Real-{Time} {Gesture} {Recognition} with {Shallow} {Convolutional} {Neural} {Networks} {Employing} an {Ultra} {Low} {Cost} {Radar} {System}},
	abstract = {Ultra-low-cost radar hardware (HW) in combination with low-cost processing units is investigated in order to create and evaluate a holistic ultra-low-cost gesture recognition system. We study the real-time performance of novel machine learning (nML) methods: neural networks (NNs), in particular shallow architectures of convolutional neural networks (CNNs). The real-time performance of each approach is judged by computational complexity, prediction time, accuracy, and false-positive rate (FPR). As HW, a two-channel radar system with continuous wave (CW) modulation at a carrier frequency of 10 GHz has been employed throughout the investigations. The algorithms are designed, trained, evaluated, and juxtaposed. The results show that the classification process on low-cost HW is feasible and allows to achieve accuracies of 97.9\% and FPRs of 1.72\%, all of which with a response time of less than 180 ms.},
	booktitle = {2020 {German} {Microwave} {Conference} ({GeMiC})},
	author = {Ehrnsperger, Matthias G. and Brenner, Thomas and Siart, Uwe and Eibert, Thomas F.},
	month = mar,
	year = {2020},
	note = {ISSN: 2167-8022},
	keywords = {artificial intelligence, continuous wave modulation, convolutional neural nets, false-positive rate, gesture recognition, holistic ultra-low-cost gesture recognition system, learning (artificial intelligence), low-cost, machine learning, neural networks, radar, real-time, real-time gesture recognition, shallow architectures, shallow convolutional neural networks, two-channel radar system, ultra low cost radar system, ultra-low-cost radar hardware},
	pages = {88--91},
}


@article{Ekman:1969,
    url = {https://doi.org/10.1515/semi.1969.1.1.49},
    title = {{The Repertoire of Nonverbal Behavior: Categories, Origins, Usage, and Coding}},
    author = {Ekman, Paul and Friesen, Wallace V.},
    pages = {49--98},
    volume = {1},
    number = {1},
    journal = {Semiotica},
    doi = {doi:10.1515/semi.1969.1.1.49},
    year = {1969},
    lastchecked = {2023-10-16},
}


@article{Erazo:2020,
   author = {Erazo, Ana Belén and Pérez Medina, Jorge Luis},
   title = {{Algorithmic Efficiency of Stroke Gesture Recognizers: a Comparative Analysis}},
   journal = {International Journal on Advanced Science, Engineering and Information Technology},
   volume = {10},
   number = {2},
   year = {2020},
   pages = {438--446},
   keywords = {gesture interaction; gesture recognition; algorithmic efficiency; stroke analysis.},
   issn = {2088-5334},
   publisher = {INSIGHT - Indonesian Society for Knowledge and Human Development},
   url = {http://ijaseit.insightsociety.org/index.php?option=com_content\&view=article\&id=9\&Itemid=1\&article_id=10807},
   doi = {10.18517/ijaseit.10.2.10807},
}


@book{Escalera:2017,
    title = {{Gesture Recognition}},
    author = {Escalera, Sergio and Guyon, Isabelle and Athitsos, Vassilis},
    publisher = {Springer International Publishing},
    address = {Berlin},
    isbn = {978-3-319-57020-4, 978-3-319-57021-1},
    year = {2017},
    series = {The Springer Series on Challenges in Machine Learning},
    edition = {1},
    volume = {},
    doi = {10.1007/978-3-319-57021-1},
    url = {https://www.springer.com/gp/book/9783319570204}
}


%===========================================================
% References starting by F
%===========================================================
@inproceedings{Feldt:2010,
    author    = {Feldt, Robert and Magazinius, Ana},
    title     = {{Validity Threats in Empirical Software Engineering Research - An Initial Survey}},
    booktitle = {Proceedings of the 22nd International Conference on Software Engineering {\&} Knowledge Engineering (SEKE'2010), Redwood City, San Francisco Bay, CA, USA, July 1 - July 3, 2010},
    pages     = {374--379},
    publisher = {Knowledge Systems Institute Graduate School},
    year      = {2010},
    timestamp = {Thu, 12 Mar 2020 11:30:50 +0100},
    biburl    = {https://dblp.org/rec/conf/seke/FeldtM10.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Feng:2019,
	title = {Hand {Gesture} {Recognition} with {Ensemble} {Time}-{Frequency} {Signatures} {Using} {Enhanced} {Deep} {Convolutional} {Neural} {Network}},
	doi = {10.1109/APSIPAASC47483.2019.9023254},
	abstract = {Hand gesture recognition using radar has been widely applied to control electronic appliances, military appliances and so on. In this paper, we investigate the feasibility of recognizing hand gestures using fused multiple time-frequency signatures, which ensembles micro-Doppler signatures, range-time signatures and angle-time signatures on spectrograms, with an Enhanced Deep Convolutional Neural Network (EDCNN). Several typical gestures included Tick, Double pushing, Rotating clockwise, and Rotating counterclockwise, were measured using Mm-wave radar and their spectrograms investigated. Therein EDCNN was employed to classify the spectrograms, with 80\% of the data utilized for training and the remaining 20\% for validation. Simulation said that the classification accuracy of the proposed method was found to be 96.2\%.},
	booktitle = {2019 {Asia}-{Pacific} {Signal} and {Information} {Processing} {Association} {Annual} {Summit} and {Conference} ({APSIPA} {ASC})},
	author = {Feng, Xiang and Song, Qun and Guo, Qingfang and Liu, Duo and Zhao, Zhanfeng and Zhao, Yinan},
	month = nov,
	year = {2019},
	note = {ISSN: 2640-0103},
	keywords = {angle-time signatures, Convolution, convolutional neural nets, Convolutional neural networks, Doppler radar, electronic appliances, enhanced deep convolutional neural network, ensemble time-frequency signatures, Feature extraction, gesture recognition, Gesture recognition, hand gesture recognition, hand gestures, image classification, learning (artificial intelligence), microDoppler signatures, military appliances, range-time signatures, Spectrogram, spectrograms, time-frequency analysis},
	pages = {1602--1605},
}


@article{Fennedy:2021,
    author = {Fennedy, Katherine and Hartmann, Jeremy and Roy, Quentin and Perrault, Simon Tangi and Vogel, Daniel},
    title = {{OctoPocus in VR: Using a Dynamic Guide for 3D Mid-Air Gestures in Virtual Reality}},
    year = {2021},
    issue_date = {Dec. 2021},
    publisher = {IEEE Educational Activities Department},
    address = {USA},
    volume = {27},
    number = {12},
    issn = {1077-2626},
    url = {https://doi.org/10.1109/TVCG.2021.3101854},
    doi = {10.1109/TVCG.2021.3101854},
    abstract = {Bau and Mackays OctoPocus dynamic guide helps novices learn, execute, and remember 2D surface gestures. We adapt OctoPocus to 3D mid-air gestures in Virtual Reality (VR) using an optimization-based recognizer, and by introducing an optional exploration mode to help visualize the spatial complexity of guides in a 3D gesture set. A replication of the original experiment protocol is used to compare OctoPocus in VR with a VR implementation of a crib-sheet. Results show that despite requiring 0.9s more reaction time than crib-sheet, OctoPocus enables participants to execute gestures 1.8s faster with 13.8 percent more accuracy during training, while remembering a comparable number of gestures. Subjective ratings support these results, 75 percent of participants found OctoPocus easier to learn and 83 percent found it more accurate. We contribute an implementation and empirical evidence demonstrating that an adaptation of the OctoPocus guide to VR is feasible and beneficial.},
    journal = {IEEE Transactions on Visualization and Computer Graphics},
    month = {dec},
    pages = {4425–4438},
    numpages = {14},
}


@article{Ferreira:2019,
    author     = {Ferreira, Pedro M. and Cardoso, Jaime S. and Rebelo, Ana},
    journal    = {Multimedia Tools Appl.},
    title      = {{On the Role of Multimodal Learning in the Recognition of Sign Language}},
    year       = {2019},
    issn       = {1380-7501},
    month      = apr,
    number     = {8},
    pages      = {10035–10056},
    volume     = {78},
    address    = {USA},
    doi        = {10.1007/s11042-018-6565-5},
    issue_date = {April 2019},
    keywords   = {Convolutional neural networks, Sign language recognition, Leap motion, Kinect, Multimodal learning},
    numpages   = {22},
    publisher  = {Kluwer Academic Publishers},
    url        = {https://doi.org/10.1007/s11042-018-6565-5},
}


@article{Fhager:2019,
	title = {Pulsed {Millimeter} {Wave} {Radar} for {Hand} {Gesture} {Sensing} and {Classification}},
	volume = {3},
	issn = {2475-1472},
	doi = {10.1109/LSENS.2019.2953022},
	abstract = {A pulsed millimeter wave radar operating at a frame rate of 144 Hz is utilized to record 2160 scattering signatures of 12 generic hand gestures. Gesture recognition is achieved by machine learning, utilizing transfer learning on a pretrained convolutional neural network. This yields excellent classification results with a validation accuracy of 99.5\%, based on a 60\% training versus 40\% validation split. The corresponding confusion matrix is also presented, showing a high level of classification orthogonality between the tested gestures. This is the first demonstration where data from a pulsed millimeter wave radar is used for gesture recognition by machine learning. It proves that the range-time envelope representation of high frame-rate data from a pulsed radar is suitable for hand gesture recognition. Further improvements are expected for more complex detection schemes and tailored neural networks.},
	number = {12},
	journal = {IEEE Sensors Letters},
	author = {Fhager, Lars Ohlsson and Heunisch, Sebastian and Dahlberg, Hannes and Evertsson, Anton and Wernersson, Lars-Erik},
	month = dec,
	year = {2019},
	keywords = {12 generic hand gestures, 60\% training versus 40\% validation split, classification, convolutional neural nets, convolutional neural network, Convolutional neural networks, frequency 144.0 Hz, gesture recognition, gesture sensing, hand gesture recognition, hand gesture sensing, high frame-rate data, image classification, learning (artificial intelligence), machine learning, matrix algebra, Microwave/millimeter wave sensors, Millimeter wave measurements, millimeter wave radar, Millimeter wave radar, millimetre wave radar, pulsed millimeter wave radar, pulsed radar, Radar imaging, Sensors, tested gestures, transfer learning, yields excellent classification results},
	pages = {1--4},
}


@inproceedings{Filho:2018,
    author    = {Filho, Ivo Alu\'{\i}zio Stinghen and Chen, Estevam Nicolas and da Silva Junior, Jucimar Maia and da Silva Barboza, Ricardo},
    booktitle = {ACM SIGGRAPH 2018 Posters},
    title     = {{Gesture Recognition Using Leap Motion: A Comparison between Machine Learning Algorithms}},
    year      = {2018},
    address   = {New York, NY, USA},
    publisher = {Association for Computing Machinery},
    series    = {SIGGRAPH '18},
    articleno = {65},
    doi       = {10.1145/3230744.3230750},
    isbn      = {9781450358170},
    keywords  = {leap motion, machine learning, motion capture, virtual reality},
    venue  = {Vancouver, British Columbia, Canada},
    numpages  = {2},
    url       = {https://doi.org/10.1145/3230744.3230750},
}


@inproceedings{Flintoff:2018, 
    author={Flintoff, Zak and Johnston, Bruno and Liarokapis, Minas},  
    booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   
    title={{Single-Grasp, Model-Free Object Classification using a Hyper-Adaptive Hand, Google Soli, and Tactile Sensors}},   
    year={2018},  
    volume={},  
    number={},  
    pages={1943-1950},  
    doi={10.1109/IROS.2018.8594166},
}


@article{Fonk:2021,
    author    = {Fonk, Robin and Schneeweiss, Sean and Simon, Ulrich and Engelhardt, Lucas},
    title     = {{Hand Motion Capture from a 3D Leap Motion Controller for a Musculoskeletal Dynamic Simulation}},
    journal   = {Sensors},
    volume    = {21},
    number    = {4},
    pages     = {1199},
    year      = {2021},
    url       = {https://doi.org/10.3390/s21041199},
    doi       = {10.3390/s21041199},
    timestamp = {Tue, 23 Mar 2021 14:14:02 +0100},
    biburl    = {https://dblp.org/rec/journals/sensors/FonkSSE21.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Fu:2016,
    author    = {Fu, Zeqing and Deng, Yiyi and Jia, Xin and Gao, Bin and Zhu, Xiaoming and Luo, Yanlin},
    booktitle = {Proceedings of the 15th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in Industry - Volume 1},
    title     = {{Automated Brain Extraction and Associated 3D Inspection Layers for the Rhesus Macaque MRI Datasets}},
    year      = {2016},
    address   = {New York, NY, USA},
    pages     = {261–269},
    publisher = {Association for Computing Machinery},
    series    = {VRCAI '16},
    doi       = {10.1145/3013971.3013984},
    isbn      = {9781450346924},
    keywords  = {volume visualization, Rhesus macaque, brain extraction, 3D inspection, bimanual gesture interaction},
    venue  = {Zhuhai, China},
    numpages  = {9},
    url       = {https://doi.org/10.1145/3013971.3013984},
}


@inproceedings{Fruchard:2018,
    author = {Fruchard, Bruno and Lecolinet, Eric and Chapuis, Olivier},
    title = {{How Memorizing Positions or Directions Affects Gesture Learning?}},
    year = {2018},
    isbn = {9781450356947},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3279778.3279787},
    doi = {10.1145/3279778.3279787},
    abstract = {Various techniques have been proposed to faster command selection. Many of them either rely on directional gestures (e.g. Marking menus) or pointing gestures using a spatially-stable arrangement of items (e.g. FastTap). Both types of techniques are known to leverage memorization, but not necessarily for the same reasons. In this paper, we investigate whether using directions or positions affects gesture learning. Our study shows that, while recall rates are not significantly different, participants used the novice mode more often and spent more time while learning commands with directional gestures, and they also reported more physical and mental efforts. Moreover, this study also highlights the importance of semantic relationships between gestural commands and reports on the memorization strategies that were elaborated by the participants.},
    booktitle = {Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces},
    pages = {107–114},
    numpages = {8},
    keywords = {pointing, spatial memory, user study, command selection, memorization, gestures},
    venue = {Tokyo, Japan},
    series = {ISS '18},
}


@article{Fruchard:2020,
    author = {Fruchard, Bruno and Lecolinet, Eric and Chapuis, Olivier},
    title = {{Side-Crossing Menus: Enabling Large Sets of Gestures for Small Surfaces}},
    year = {2020},
    issue_date = {November 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {4},
    number = {ISS},
    url = {https://doi.org/10.1145/3427317},
    doi = {10.1145/3427317},
    abstract = {Supporting many gestures on small surfaces allows users to interact remotely with complex environments such as smart homes, large remote displays, or virtual reality environments, and switching between them (e.g., AR setup in a smart home). Providing eyes-free gestures in these contexts is important as this avoids disrupting the user's visual attention. However, very few techniques enable large sets of commands on small wearable devices supporting the user's mobility and even less provide eyes-free interaction. We present Side-Crossing Menus (SCM), a gestural technique enabling large sets of gestures on \%small interactive surfaces like a smartwatch. Contrary to most gestural techniques, SCM relies on broad and shallow menus that favor small and rapid gestures. We demonstrate with a first experiment that users can efficiently perform these gestures eyes-free aided with tactile cues; 95\% accuracy after training 20 minutes on a representative set of 30 gestures among 172. In a second experiment, we focus on the learning of SCM gestures and do not observe significant differences with conventional Multi-stroke Marking Menus in gesture accuracy and recall rate. As both techniques utilize contrasting menu structures, our results indicate that SCM is a compelling alternative for enhancing the input capabilities of small surfaces.},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = nov,
    articleno = {189},
    numpages = {19},
    keywords = {eyes-free interactions, small surface, gestural interaction, marking menus},
}


%===========================================================
% References starting by G
%===========================================================
@inproceedings{Galea:2018,
  author    = {Galea, Claire and Porter, Chris},
  booktitle = {Proceedings of the 32nd International BCS Human Computer Interaction Conference},
  title     = {{Accessible Choral Ensembles for Visually Impaired Singers}},
  year      = {2018},
  address   = {Swindon, GBR},
  publisher = {BCS Learning \& Development Ltd.},
  series    = {HCI '18},
  articleno = {37},
  doi       = {10.14236/ewic/HCI2018.37},
  keywords  = {gesture recognition and interpretation, realtime communications, human-computer interaction in choral activities},
  venue  = {Belfast, United Kingdom},
  numpages  = {10},
  url       = {https://doi.org/10.14236/ewic/HCI2018.37},
}


@article{Gao:2021,
    AUTHOR = {Gao, Lan and Dachena, Chiara and Wu, Kaijun and Fedeli, Alessandro and Pastorino, Matteo and Randazzo, Andrea and Wu, Xiaoping and Lambot, S{\'{e}}bastien},
    TITLE = {{Full-Wave Modeling and Inversion of UWB Radar Data for Wave Propagation in Cylindrical Objects}},
    JOURNAL = {Remote Sensing},
    VOLUME = {13},
    YEAR = {2021},
    NUMBER = {12},
    article-NUMBER = {2370},
    URL = {https://www.mdpi.com/2072-4292/13/12/2370},
    ISSN = {2072-4292},
    ABSTRACT = {The nondestructive characterization of cylindrical objects is needed in many fields, such as medical diagnostics, tree trunk inspection, or concrete column testing. In this study, the radar equation of Lambot et al. is combined with cylindrical Green’s functions to fully model and invert ultra-wideband (UWB) ground-penetrating radar (GPR) data and retrieve the properties of cylindrical objects. Inversion is carried out using a lookup table (LUT) approach followed by local optimization to ensure retrieval of the global minimum of the objective function. Numerical experiments were conducted to analyze the capabilities of the developed inversion procedure to estimate the radius, permittivity, and conductivity of the cylinders. The full-wave model was validated in laboratory conditions on metallic and plastic pipes of different sizes. The adopted radar system consists of a lightweight vector network analyzer (VNA) connected to a single transmitting and receiving horn antenna. The numerical experiments highlighted the complexity of the inverse problem, mainly originating from the multiple propagation modes within cylindrical objects. The laboratory measurements demonstrated the accuracy of the forward modeling and reconstructions in far-field conditions.},
    DOI = {10.3390/rs13122370}
}


@article{Ghaffar:2019,
	title = {Hand {Pointing} {Gestures} {Based} {Digital} {Menu} {Board} {Implementation} {Using} {IR}-{UWB} {Transceivers}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2914410},
	abstract = {Digital menu boards (DMB) are convenient for customers as well as sellers. In this paper, we have implemented a DMB using IR-UWB transceivers. Unlike the traditional touch-based interfaces for menu selection, in our proposed system, users can select items from the menu without touching the screen. The screen is used to display the menu, and the users point to the specific menu item to select it. Multiple radar transceivers are used to create a virtual space divided into different grid blocks in front of the digital display. Patterns in the radar data are analyzed using a multiclass support vector machine (SVM) classifier and a histogram of oriented gradient descriptor. The system is trained at two different distances from the radar sensors in order to make it robust against distance changes. The proposed hand pointing-based DMB system was verified through different experiments, with different grid sizes, to investigate accuracy dependence on grid size. The results showed high accuracy; therefore, the system can be used in real-life scenarios.},
	journal = {IEEE Access},
	author = {Ghaffar, Asim and Khan, Faheem and Cho, Sung Ho},
	year = {2019},
	keywords = {Clutter, digital display, Digital menu board (DMB), digital menu board implementation, digital menu boards, Digital multimedia broadcasting, gesture recognition, Gesture recognition, gestures recognition, grid blocks, grid sizes, hand pointing gestures, hand pointing-based DMB system, histogram, impulse radio ultrawideband, interactive systems, IR-UWB transceivers, menu item, menu selection, multiclass support vector machine classifier, multiple radar transceivers, pattern analysis, pattern classification, Radar, radar data, radar receivers, radar sensors, screen, Sensors, support vector machines, Support vector machines, SVM, touch-based interfaces, transceivers, Two dimensional displays, ultra wideband communication, users point},
	pages = {58148--58157},
}


@inproceedings{Gheran:2018,
    author = {Gheran, Bogdan{-}Florin and Vanderdonckt, Jean and Vatavu, Radu-Daniel},
    editor    = {Koskinen, Ilpo and Lim, Youn{-}Kyung and Cerratto{-}Pargman, Teresa and Chow, Kenny K. N. and Odom, William},
    title     = {{Gestures for Smart Rings: Empirical Results, Insights, and Design Implications}},
    booktitle = {Proc. of the 2018 Designing Interactive Systems Conference},
    pages     = {623--635},
    isbn = {9781450351980},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    year      = {2018},
    url       = {https://doi.org/10.1145/3196709.3196741},
    doi       = {10.1145/3196709.3196741},
    timestamp = {Fri, 24 May 2019 07:44:04 +0200},
    biburl    = {https://dblp.org/rec/conf/ACMdis/GheranVV18.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    venue = {Hong Kong, China},
    series = {DIS ’18},
}


@inproceedings{Giachetti:2016,
    booktitle = {Eurographics Workshop on 3D Object Retrieval},
    editor = {Ferreira, A. and Giachetti, A. and Giorgi, D.},
    title = {{Shape Retrieval and 3D Gestural Interaction}},
    author = {Giachetti, Andrea and Caputo, Fabio Marco and Carcangiu, Alessandro and Scateni, Riccardo and Spano, Lucio Davide},
    year = {2016},
    publisher = {The Eurographics Association},
    ISSN = {1997-0471},
    ISBN = {978-3-03868-004-8},
    DOI = {10.2312/3dor.20161079},
}


@inproceedings{Gigie:2019,
	address = {New York, NY, USA},
	series = {{UbiComp}/{ISWC} '19 {Adjunct}},
	title = {{An agile approach for human gesture detection using synthetic radar data}},
	isbn = {978-1-4503-6869-8},
	url = {https://doi.org/10.1145/3341162.3349332},
	doi = {10.1145/3341162.3349332},
	abstract = {A large and diversified dataset is the cornerstone for the analysis of many real world systems. Data collection, especially involving living beings, is a time and effort consuming approach. Data augmentation using analytical models is gaining traction in computing systems because of data scarcity in real world applications. In this paper, we demonstrate a case study of data explosion for radar based human gesture detection. We present a physical model based simulation framework for obtaining radar signals corresponding to different human gestures. Radar based public datasets have not yet been so easily available because of its high cost and less availability. On the contrary, Kinect based datasets are easily available because of its market dominance and commercialization of devices such as Xbox Kinect. Thus, for the simulation framework, we have used the publicly available dataset for gaming gestures based on Kinect data (obtained from Microsoft Cambridge joint initiative). This Kinect data containing the coordinates of joint positions of human skeletons performing different gaming gestures is then fed to a radar based simulation framework to calculate the radar data. Furthermore, we have tuned different parameters in this simulation framework and exploded it to generate radar micro Doppler signatures in a controlled way, keeping in mind the physical constraints. On this exploded data, machine learning algorithms have been applied to evaluate gesture detection accuracy. The enlarged dataset for four gestures has shown an accuracy of around 94.7 \% for 10 fold cross validation in training phase. A comparison of simulated radar data with respect to experimentally obtained hardware radar data for different gestures has also been reported to show the relevance of the proposed method.},
	urldate = {2020-12-21},
	booktitle = {Adjunct {Proceedings} of the 2019 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} and {Proceedings} of the 2019 {ACM} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Gigie, Andrew and Rani, Smriti and Chowdhury, Arijit and Chakravarty, Tapas and Pal, Arpan},
	month = sep,
	year = {2019},
	keywords = {gesture detection, kinect datasets, physical model based synthetic data generation, radar simulation framework, synthetic data generation},
	pages = {558--564},
}


@phdthesis{Giot:2023,
    title = {{Radar-based gesture interaction: from sensing to recognition}},
    author = {Giot, Emile},
    abstract = {In the recent years, radar-based gesture recognition has seen an increase in popularity. Indeed, radar sensors have various advantages over the widely used wearable and image-based sensors. They are less intrusive and less sensitive to ambient and lighting conditions. Moreover, as the radar waves are able to propagate through materials depending on its permittivity, radar sensors are the go to candidates to perform gesture recognition through materials. In this work, an off-the-shelf radar, the Walabot, is used to acquire a dataset of hand gestures performed in front of three different material plates (wood, PVC and glass) placed in-between the radar and the participant. In order to evaluate the suitability of using the Walabot in such scenarios, multiple recognizers based on template matching are trained using the acquired hand gesture dataset and their accuracy at performing hand gesture recognition is evaluated. This thesis presents the complete methodology followed to perform hand gesture recognition through 3 different material plates (wood, PVC and glass), from the acquisition of the dataset using the Walabot and the processing of the radar data with the RadarSense pipeline, to the training and testing of the recognizers with the QuantumLeap framework.},
    Keywords = {Gesture recognition , Radar-based interaction , Template matching , Walabot , Human computer interaction , RadarSense , QuantumLeap},
    language = {Anglais},
    year = {2023},
    url = {http://hdl.handle.net/2078.1/thesis:43330},
    school = {UCL - Ecole polytechnique de Louvain},
    url = {http://hdl.handle.net/2078.1/thesis:43330},
}



@inproceedings{Glorot:2011,
    title = 	 {{Deep Sparse Rectifier Neural Networks}},
    author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
    booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
    pages = 	 {315--323},
    year = 	 {2011},
    editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
    volume = 	 {15},
    series = 	 {Proceedings of Machine Learning Research},
    address = 	 {Fort Lauderdale, FL, USA},
    month = 	 {11--13 Apr},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
    url = 	 {https://proceedings.mlr.press/v15/glorot11a.html},
    abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
}


@article{Goker:2017,
    author    = {G\"{o}ker, Pınar and G\"{u}lhal Bozkir, Memduha },
    title     = {{Determination of hand and palm surface areas as a percentage of body surface area in Turkish young adults}},
    journal   = {Trauma and Emergency Care},
    volume    = {2},
    number    = {4},
    pages     = {1--4},
    year      = {2017},
    month     = may,
    url       = {https://www.oatext.com/pdf/TEC-2-135.pdf},
    doi       = {10.15761/TEC.1000135},
}


@article{GomezDonoso:2017,
    author     = {Gomez-Donoso, Francisco and Orts-Escolano, Sergio and Garcia-Garcia, Alberto and Garcia-Rodriguez, Jose and Castro-Vargas, John Alejandro and Ovidiu-Oprea, Sergiu and Cazorla, Miguel},
    journal    = {Pattern Recogn. Lett.},
    title      = {{A Robotic Platform for Customized and Interactive Rehabilitation of Persons with Disabilities}},
    year       = {2017},
    issn       = {0167-8655},
    month      = nov,
    number     = {C},
    pages      = {105–113},
    volume     = {99},
    address    = {USA},
    doi        = {10.1016/j.patrec.2017.05.027},
    issue_date = {November 2017},
    numpages   = {9},
    publisher  = {Elsevier Science Inc.},
    url        = {https://doi.org/10.1016/j.patrec.2017.05.027},
}  


@inbook{Goodfellow:2016,
    author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
    title = {{Softmax Units for Multinoulli Output Distributions}},
    year = {2016},
    isbn = {978-0-26203561-3},
    publisher = {MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    address = {New York, NY, USA},
    booktitle = {Deep Learning},
    pages = {180–184},
}


@inproceedings{Goswami:2019,
	title = {Real-{Time} {Multi}-{Gesture} {Recognition} using 77 {GHz} {FMCW} {MIMO} {Single} {Chip} {Radar}},
	doi = {10.1109/ICCE.2019.8662006},
	abstract = {Innovations in CMOS radar has paved way for new functions like gesture-based human-machine interaction using radar for consumer and automotive electronics. Single chip radars which integrate the RF front-end and digital processing logic are fit for such applications due to their cost and form factor but are constrained in angular resolution, memory, and processing power. In this paper, we propose low complexity radar-based multi-gesture classification solution which overcomes these constraints to achieve 96\% accuracy for 6 gestures generalized across 8 users. The algorithm developed was found to consume only 8.4\% DSP cycles and 256KiB memory on Texas Instrument's AWR1642.},
	booktitle = {2019 {IEEE} {International} {Conference} on {Consumer} {Electronics} ({ICCE})},
	author = {Goswami, Piyali and Rao, Sandeep and Bharadwaj, Sachin and Nguyen, Amanda},
	month = jan,
	year = {2019},
	note = {ISSN: 2158-4001},
	keywords = {77 GHz CMOS Radar, angular resolution, automotive electronics, CMOS radar, consumer, CW radar, digital processing logic, digital signal processing chips, FM radar, FMCW MIMO single chip radar, form factor, frequency 77.0 GHz, gesture recognition, Gesture Recognition, gesture-based human-machine interaction, human computer interaction, image classification, low complexity radar-based multigesture classification solution, processing power, radar signal processing, real-time multigesture recognition, RF front-end, Single Chip Radar, single chip radars},
	pages = {1--4},
}


@article{Grier:2015,
	title = {How {High} is {High}? {A} {Meta}-{Analysis} of {NASA}-{TLX} {Global} {Workload} {Scores}},
	volume = {59},
	issn = {2169-5067},
	shorttitle = {How {High} is {High}?},
	url = {https://doi.org/10.1177/1541931215591373},
	doi = {10.1177/1541931215591373},
	abstract = {This paper presents a descriptive analysis of over 1000 global NASA Task Load Index (TLX; Hart \& Staveland, 1988) scores from over 200 publications. This analysis is similar to that which was suggested by Hart (2006). The frequency distributions and measures of central tendency presented will aid practitioners in understanding global NASA-TLX scores observed in system tests.},
	language = {en},
	number = {1},
	urldate = {2021-02-08},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Grier, Rebecca A.},
	month = sep,
	year = {2015},
	note = {Publisher: SAGE Publications Inc},
	pages = {1727--1731},
}


@inproceedings{Grijincu:2014,
    author = {Grijincu, Daniela and Nacenta, Miguel A. and Kristensson, Per Ola},
    title = {{User-Defined Interface Gestures: Dataset and Analysis}},
    year = {2014},
    isbn = {9781450325875},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2669485.2669511},
    doi = {10.1145/2669485.2669511},
    booktitle = {Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces},
    pages = {25–34},
    numpages = {10},
    keywords = {gesture elicitation, gesture annotation, gesture analysis methodology, gesture design, user-defined gestures, gesture datasets, gesture memorability},
    venue = {Dresden, Germany},
    series = {ITS ’14},
}


@inproceedings{Groenewald:2016,
    author = {Groenewald, Celeste and Anslow, Craig and Islam, Junayed and Rooney, Chris and Passmore, Peter and Wong, William},
    title = {{Understanding 3D Mid-Air Hand Gestures with Interactive Surfaces and Displays: A Systematic Literature Review}},
    year = {2016},
    publisher = {BCS Learning \& Development Ltd.},
    address = {Swindon, GBR},
    url = {https://doi.org/10.14236/ewic/HCI2016.43},
    doi = {10.14236/ewic/HCI2016.43},
    abstract = {3D gesture based systems are becoming ubiquitous and there are many mid-air hand gestures that exist for interacting with digital surfaces and displays. There is no well defined gesture set for 3D mid-air hand gestures which makes it difficult to develop applications that have consistent gestures. To understand what gestures exist we conducted the first comprehensive systematic literature review on mid-air hand gestures following existing research methods. The results of the review identified 65 papers where the mid-air hand gestures supported tasks for selection, navigation, and manipulation. We also classified the gestures according to a gesture classification scheme and identified how these gestures have been empirically evaluated. The results of the review provide a richer understanding of what mid-air hand gestures have been designed, implemented, and evaluated in the literature which can help developers design better user experiences for digital interactive surfaces and displays.},
    booktitle = {Proceedings of the 30th International BCS Human Computer Interaction Conference: Fusion!},
    articleno = {43},
    numpages = {13},
    keywords = {HCI, 3D gestures, interaction gestures, systematic literature review, mid-air hand gestures},
    venue = {Poole, United Kingdom},
    series = {HCI '16},
}


@article{Gu:2019,
    author={Gu, Changzhan and Wang, Jian and Lien, Jaime},
    journal={IEEE Microwave Magazine}, 
    title={{Motion Sensing Using Radar: Gesture Interaction and Beyond}}, 
    year={2019},
    month=aug,
    volume={20},
    number={8},
    pages={44-57},
    doi={10.1109/MMM.2019.2915490},
    url={https://ieeexplore.ieee.org/document/8755821},
}


@inproceedings{Gupta:2017,
    author = {Gupta, Aakar and Pietrzak, Thomas and Yau, Cleon and Roussel, Nicolas and Balakrishnan, Ravin},
    title = {{Summon and Select: Rapid Interaction with Interface Controls in Mid-Air}},
    year = {2017},
    isbn = {9781450346917},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3132272.3134120},
    doi = {10.1145/3132272.3134120},
    abstract = {Current freehand interactions with large displays rely on point \& select as the dominant paradigm. However, constant hand movement in air for pointer navigation leads to hand fatigue quickly. We introduce summon \& select, a new model for freehand interaction where, instead of navigating to the control, the user summons it into focus and then manipulates it. Summon \& select solves the problems of constant pointer navigation, need for precise selection, and out-of-bounds gestures that plague point \& select. We describe the design and conduct two studies to evaluate the design and compare it against point \& select in a multi-button selection study. The results show that summon \& select is significantly faster and has less physical and mental demand than point \& select.},
    booktitle = {Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces},
    pages = {52–61},
    numpages = {10},
    keywords = {Gestures, Freehand, Mid-air, Haptics},
    venue = {Brighton, United Kingdom},
    series = {ISS '17},
}


@inproceedings{Gurbuz:2020,
	title = {{ASL} {Recognition} {Based} on {Kinematics} {Derived} from a {Multi}-{Frequency} {RF} {Sensor} {Network}},
	doi = {10.1109/SENSORS47125.2020.9278864},
	abstract = {As a means for leveraging technology in the design of Deaf spaces, this paper presents initial results on American Sign Language (ASL) recognition using RF sensing. RF sensors are non-contact, non-invasive, and protective of privacy, making them of special interest for use in personal areas. Using just the kinematic properties of signing as captured by the micro-Doppler signatures of a multi-frequency RF sensor network, this paper shows that native and imitation signing can be differentiated with \%99 accuracy, while up to 20 ASL signs are recognized with an accuracy of \%72 or higher.},
	booktitle = {2020 {IEEE} {Sensors}},
	author = {Gurbuz, Sevgi Z. and Gurbuz, Ali C. and Malaia, Evie A. and Griffin, Darrin J. and Crawford, Chris and Kurtoglu, Emre and Rahman, Mohammad Mahbubur and Aksu, Ridvan and Mdrafi, Robiulhossain},
	month = oct,
	year = {2020},
	note = {ISSN: 2168-9229},
	keywords = {American sign language, Assistive technology, Discrete cosine transforms, Feature extraction, gesture recognition, Gesture recognition, Radar, radar micro-Doppler, Radio frequency, RF sensing, Support vector machines},
	pages = {1--4},
}


@article{Gusenbauer:2020,
    author = {Gusenbauer, Michael and Haddaway, Neal R.},
    title = {{Which academic search systems are suitable for systematic reviews or meta-analyses? Evaluating retrieval qualities of Google Scholar, PubMed, and 26 other resources}},
    journal = {Research Synthesis Methods},
    volume = {11},
    number = {2},
    pages = {181-217},
    keywords = {academic search systems, discovery, evaluation, information retrieval, systematic review, systematic search},
    doi = {https://doi.org/10.1002/jrsm.1378},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1378},
    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jrsm.1378},
    abstract = {Rigorous evidence identification is essential for systematic reviews and meta-analyses (evidence syntheses) because the sample selection of relevant studies determines a review's outcome, validity, and explanatory power. Yet, the search systems allowing access to this evidence provide varying levels of precision, recall, and reproducibility and also demand different levels of effort. To date, it remains unclear which search systems are most appropriate for evidence synthesis and why. Advice on which search engines and bibliographic databases to choose for systematic searches is limited and lacking systematic, empirical performance assessments. This study investigates and compares the systematic search qualities of 28 widely used academic search systems, including Google Scholar, PubMed, and Web of Science. A novel, query-based method tests how well users are able to interact and retrieve records with each system. The study is the first to show the extent to which search systems can effectively and efficiently perform (Boolean) searches with regards to precision, recall, and reproducibility. We found substantial differences in the performance of search systems, meaning that their usability in systematic searches varies. Indeed, only half of the search systems analyzed and only a few Open Access databases can be recommended for evidence syntheses without adding substantial caveats. Particularly, our findings demonstrate why Google Scholar is inappropriate as principal search system. We call for database owners to recognize the requirements of evidence synthesis and for academic journals to reassess quality requirements for systematic reviews. Our findings aim to support researchers in conducting better searches for better evidence synthesis.},
    year = {2020},
}




@article{Guttman:1945,
    author    = {Guttman, Louis},
    title     = {{A basis for analyzing test-retest reliability}},
    journal   = {Psychometrika},
    volume    = {10},
    pages     = {255--282},
    year      = {1945},
    month     = dec,
    url       = {https://link.springer.com/article/10.1007/BF02288892},
    doi       = {10.1007/BF02288892},
}


@article{Guzsvinecz:2019,
    AUTHOR = {Guzsvinecz, Tibor and Szucs, Veronika and Sik-Lanyi, Cecilia},
    TITLE = {{Suitability of the Kinect Sensor and Leap Motion Controller—A Literature Review}},
    JOURNAL = {Sensors},
    VOLUME = {19},
    YEAR = {2019},
    NUMBER = {5},
    article-NUMBER = {1072},
    URL = {https://www.mdpi.com/1424-8220/19/5/1072},
    ISSN = {1424-8220},
    ABSTRACT = {As the need for sensors increases with the inception of virtual reality, augmented reality and mixed reality, the purpose of this paper is to evaluate the suitability of the two Kinect devices and the Leap Motion Controller. When evaluating the suitability, the authors's focus was on the state of the art, device comparison, accuracy, precision, existing gesture recognition algorithms and on the price of the devices. The aim of this study is to give an insight whether these devices could substitute more expensive sensors in the industry or on the market. While in general the answer is yes, it is not as easy as it seems: There are significant differences between the devices, even between the two Kinects, such as different measurement ranges, error distributions on each axis and changing depth precision relative to distance.},
    DOI = {10.3390/s19051072},
    numpages = {25},
}


%===========================================================
% References starting by H
%===========================================================
@InProceedings{Hansberger:2017,
    author="Hansberger, Jeffrey T.
    and Peng, Chao
    and Mathis, Shannon L.
    and Areyur Shanthakumar, Vaidyanath
    and Meacham, Sarah C.
    and Cao, Lizhou
    and Blakely, Victoria R.",
    editor="Lackey, Stephanie
    and Chen, Jessie",
    title={{Dispelling the Gorilla Arm Syndrome: The Viability of Prolonged Gesture Interactions}},
    booktitle="Virtual, Augmented and Mixed Reality",
    year="2017",
    publisher="Springer International Publishing",
    address="Cham",
    pages="505--520",
    abstract="The use of gestures as a way to interact with computer systems has shown promise as a natural way to interact and manipulate digital information. However, users performing mid-air gestures for even moderate periods of time experience arm fatigue and discomfort, earning its name of the gorilla arm syndrome. Based on the natural use of hands during communication, a new gesture vocabulary was created that supports the arms while the user performs the gestures. A repeated measures within subject design was conducted where participants interacted with a custom video game using 3 types of input for 30 min each, (1) keyboard, (2) mid-air gestures and (3) supported gestures. Three measures of exertion were collected, (1) time, (2) energy expenditure, and (3) perceived exertion. The newly designed supported gestures required significantly less physical and perceived effort than the mid-air gestures and required similar exertion as the keyboard condition.",
    isbn="978-3-319-57987-0",
}



@incollection{Hart:1988,
    title = {{Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research}},
    editor = "Hancock, Peter A. and Meshkati, Najmedin",
    series = "Advances in Psychology",
    publisher = "North-Holland",
    volume = "52",
    pages = "139 - 183",
    year = "1988",
    booktitle = "Human Mental Workload",
    issn = "0166-4115",
    doi = "https://doi.org/10.1016/S0166-4115(08)62386-9",
    url = "http://www.sciencedirect.com/science/article/pii/S0166411508623869",
    author = "Hart, Sandra G. and Staveland, Lowell E.",
    abstract = "The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload.",
}


@book{Hastie:2009,
    author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
    title     = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition}},
    series    = {Springer Series in Statistics},
    publisher = {Springer},
    year      = {2009},
    url       = {https://doi.org/10.1007/978-0-387-84858-7},
    doi       = {10.1007/978-0-387-84858-7},
    isbn      = {9780387848570},
    timestamp = {Fri, 17 Jul 2020 16:12:43 +0200},
    biburl    = {https://dblp.org/rec/books/lib/HastieTF09.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Hayashi:2021,
    author = {Hayashi, Eiji and Lien, Jaime and Gillian, Nicholas and Giusti, Leonardo and Weber, Dave and Yamanaka, Jin and Bedal, Lauren and Poupyrev, Ivan},
    title = {{RadarNet: Efficient Gesture Recognition Technique Utilizing a Miniature Radar Sensor}},
    year = {2021},
    isbn = {9781450380966},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3411764.3445367},
    doi = {10.1145/3411764.3445367},
    abstract = {Gestures are a promising candidate as an input modality for ambient computing where
    conventional input modalities such as touchscreens are not available. Existing works
    have focused on gesture recognition using image sensors. However, their cost, high
    battery consumption, and privacy concerns made cameras challenging as an always-on
    solution. This paper introduces an efficient gesture recognition technique using a
    miniaturized 60 GHz radar sensor. The technique recognizes four directional swipes
    and an omni-swipe using a radar chip (6.5 \texttimes{} 5.0 mm) integrated into a mobile phone.
    We developed a convolutional neural network model efficient enough for battery powered
    and computationally constrained processors. Its model size and inference time is less
    than 1/5000 compared to an existing gesture recognition technique using radar. Our
    evaluations with large scale datasets consisting of 558,000 gesture samples and 3,920,000
    negative samples demonstrated our algorithm’s efficiency, robustness, and readiness
    to be deployed outside of research laboratories. },
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
    articleno = {5},
    numpages = {14},
    keywords = {mobile, gesture recognition, radar sensing, deep learning},
    venue = {Yokohama, Japan},
    series = {CHI '21},
}


@article{Hazra:2018,
	title = {Robust {Gesture} {Recognition} {Using} {Millimetric}-{Wave} {Radar} {System}},
	volume = {2},
	issn = {2475-1472},
	doi = {10.1109/LSENS.2018.2882642},
	abstract = {Gesture recognition is one of the most intuitive forms of human-computer interface. Gesture sensing can replace interfaces such as touch and clicks needed for interacting with a device. In this article, we present a short-range compact 60-GHz mm-wave radar sensor that is sensitive to fine dynamic hand motions. A series of range-Doppler images are extracted and processed using a long recurrent all-convolution neural network for real-time dynamic hand gesture recognition. Furthermore, we make use of novel data augmentation techniques for the proposed gesture recognition system to generalize for multiple users and operating environments. The results show accurate classification performance requiring very low processor footprint facilitating implementation in embedded platforms with real-time user feedback.},
	number = {4},
	journal = {IEEE Sensors Letters},
	author = {Hazra, Souvik and Santra, Avik},
	month = dec,
	year = {2018},
	keywords = {60-GHz mm-wave radar, data augmentation techniques, dynamic hand motions, Feature extraction, frequency 60 GHz, gesture recognition, Gesture recognition, gesture recognition system, gesture sensing, Human computer interaction, human-computer interface, human–computer interface, Millimeter wave radar, millimetric-wave radar system, neural nets, neural network, radar imaging, Radar imaging, range-Doppler images, real-time dynamic hand gesture recognition, real-time user feedback, robust gesture recognition, sensing, Sensor signals processing, short-range compact mm-wave radar sensor},
	pages = {1--4},
}


@article{Hazra:2019a,
	title = {Short-{Range} {Radar}-{Based} {Gesture} {Recognition} {System} {Using} {3D} {CNN} {With} {Triplet} {Loss}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2938725},
	abstract = {Gesture recognition is the most intuitive form of human computer-interface. Gesture sensing can replace interfaces such as touch and clicks needed for interacting with a device. Gesture recognition in a practical scenario is an open-set classification, i.e. the recognition system should classify correct known gestures while rejecting arbitrary unknown gestures during inference. To address the issue of gesture recognition in an open set, we present, in this paper, a novel distance-metric based meta-learning approach to learn embedding features from a video of range-Doppler images generated by hand gestures at the radar receiver. Further, k-Nearest Neighbor (kNN) is used to classify known gestures, distance-thresholding is used to reject unknown gesture motions and clustering is used to add new custom gestures on-the-fly without explicit model re-training. We propose to use 3D Deep Convolutional Neural Network (3D-DCNN) architecture to learn the embedding model using distance-based triplet-loss similarity metric. We demonstrate our approach to correctly classify gestures using short-range 60-GHz compact short-range radar sensor achieving an overall classification accuracy of 94.5\% over six fine-grained gestures under challenging practical environments, while rejecting other unknown gestures with 0.935 F1 score, and capable of adding new gestures on-the-fly without an explicit model re-training.},
	journal = {IEEE Access},
	author = {Hazra, Souvik and Santra, Avik},
	year = {2019},
	keywords = {arbitrary unknown gestures, convolutional neural nets, correct known gestures, custom gestures on-the-fly, Data models, Deep learning, distance-based triplet-loss similarity, distance-metric based meta-learning approach, Doppler radar, Feature extraction, fine-grained gestures, gesture recognition, Gesture recognition, gesture sensing, hand gestures, human computer interaction, human-machine interface, image classification, learning (artificial intelligence), mm-wave radar, nearest neighbour methods, open-set classification, pattern classification, Radar, Receivers, short-range 60-GHz compact short-range radar sensor, short-range radar-based gesture recognition system, Three-dimensional displays, triplet loss, unknown gesture motions},
	pages = {125623--125633},
}


@inproceedings{Hazra:2019b,
	title = {Radar {Gesture} {Recognition} {System} in {Presence} of {Interference} using {Self}-{Attention} {Neural} {Network}},
	doi = {10.1109/ICMLA.2019.00230},
	abstract = {Gesture recognition provides an easy, convenient and intuitive way of remotely controlling several consumer electronics devices such as audio devices, television sets, projector or gaming consoles. In recent years, radar sensors have been shown to be effective sensing modality to sense and recognize fine-grained dynamic finger-gestures in watch or smartphone and thus offers an user-friendly human-computer interface in ultrashort range applications. However, hand-gesture recognition from a farther distance such as to control consumer devices like TV or projector pose challenge particularly arising due to interferences from multiple humans in the field of view. In this paper, we present a novel unguided spatio-Doppler attention mechanism to enable hand-gesture recognition in presence of multiple humans using a low power, compact 60-GHz FMCW radar operated in 500MHz ISM frequency band. The spatio-Doppler mechanism in 2D deep convolutional neural network with long short term memory (2D CNN-LSTM) makes use of the range-Doppler images and range-angle images. We experimentally present the classification accuracy of 94.75\% of our proposed system on test dataset using eight gestures, namely wave, push forward, pull, left swipe, right swipe, clockwise rotate, anti-clockwise rotate, cross, in presence of interfering people, such as walking or arbitrary movements.},
	booktitle = {2019 18th {IEEE} {International} {Conference} {On} {Machine} {Learning} {And} {Applications} ({ICMLA})},
	author = {Hazra, Souvik and Santra, Avik},
	month = dec,
	year = {2019},
	keywords = {2D CNN-LSTM, 2D deep convolutional neural network, audio devices, bandwidth 500.0 MHz, Chirp, consumer electronics, consumer electronics devices, CW radar, Doppler effect, Doppler radar, effective sensing modality, fine-grained dynamic finger-gestures, FM radar, FMCW radar, frequency 60 GHz, gesture recognition, Gesture recognition, Gesture Recognition, hand-gesture recognition, human computer interaction, Human-Computer Interface, image sensors, ISM frequency band, long short term memory, millimetre wave radar, mm-wave radar, neural nets, radar computing, radar gesture recognition system, radar imaging, radar sensors, range-angle images, range-Doppler images, self-attention neural network, Sensors, spatio-Doppler mechanism, television sets, Two dimensional displays, ultrashort range applications, unguided spatio-Doppler attention, user-friendly human-computer interface},
	pages = {1409--1414},
}


@article{Heideman:1984,  
    author={Heideman, Michael T. and Johnson, Don H. and Burrus, C. Sidney}, 
    journal={IEEE ASSP Magazine},   
    title={{Gauss and the history of the fast fourier transform}},   
    year={1984}, 
    month = oct,
    volume={1},  
    number={4},  
    pages={14-21},  
    doi={10.1109/MASSP.1984.1162257},
    url={https://ieeexplore.ieee.org/document/1162257},
}


@article{HernandezVela:2014,
    title = {{Probability-based Dynamic Time Warping and Bag-of-Visual-and-Depth-Words for Human Gesture Recognition in RGB-D}},
    journal = {Pattern Recognition Letters},
    volume = {50},
    pages = {112-121},
    year = {2014},
    note = {Depth Image Analysis},
    issn = {0167-8655},
    doi = {https://doi.org/10.1016/j.patrec.2013.09.009},
    url = {https://www.sciencedirect.com/science/article/pii/S0167865513003450},
    author = {Hernández-Vela, Antonio and Bautista, Miguel Ángel and Perez-Sala, Xavier and Ponce-López, Víctor and Escalera, Sergio and Baró, Xavier and Pujol, Oriol and Angulo, Cecilio},
    keywords = {RGB-D, Bag-of-Words, Dynamic Time Warping, Human Gesture Recognition},
    abstract = {We present a methodology to address the problem of human gesture segmentation and recognition in video and depth image sequences. A Bag-of-Visual-and-Depth-Words (BoVDW) model is introduced as an extension of the Bag-of-Visual-Words (BoVW) model. State-of-the-art RGB and depth features, including a newly proposed depth descriptor, are analysed and combined in a late fusion form. The method is integrated in a Human Gesture Recognition pipeline, together with a novel probability-based Dynamic Time Warping (PDTW) algorithm which is used to perform prior segmentation of idle gestures. The proposed DTW variant uses samples of the same gesture category to build a Gaussian Mixture Model driven probabilistic model of that gesture class. Results of the whole Human Gesture Recognition pipeline in a public data set show better performance in comparison to both standard BoVW model and DTW approach.},
}


@inproceedings{Herold:2012,
    booktitle = {Eurographics Workshop on Sketch-Based Interfaces and Modeling},
    editor = {Singh, Karan and Kara, Levent Burak},
    title = {{The One Cent Recognizer: A Fast, Accurate, and Easy-to-Implement Handwritten Gesture Recognition Technique}},
    author = {Herold, James and Stahovich, Thomas F.},
    year = {2012},
    publisher = {The Eurographics Association},
    ISSN = {1812-3503},
    ISBN = {978-3-905674-42-2},
    DOI = {10.2312/SBM/SBM12/039-046},
}


@inproceedings{Hincapie:2014,
    author = {Hincapi\'{e}-Ramos, Juan David and Guo, Xiang and Moghadasian, Paymahn and Irani, Pourang},
    title = {{Consumed endurance: a metric to quantify arm fatigue of mid-air interactions}},
    year = {2014},
    isbn = {9781450324731},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2556288.2557130},
    doi = {10.1145/2556288.2557130},
    abstract = {Mid-air interactions are prone to fatigue and lead to a feeling of heaviness in the upper limbs, a condition casually termed as the gorilla-arm effect. Designers have often associated limitations of their mid-air interactions with arm fatigue, but do not possess a quantitative method to assess and therefore mitigate it. In this paper we propose a novel metric, Consumed Endurance (CE), derived from the biomechanical structure of the upper arm and aimed at characterizing the gorilla-arm effect. We present a method to capture CE in a non-intrusive manner using an off-the-shelf camera-based skeleton tracking system, and demonstrate that CE correlates strongly with the Borg CR10 scale of perceived exertion. We show how designers can use CE as a complementary metric for evaluating existing and designing novel mid-air interactions, including tasks with repetitive input such as mid-air text-entry. Finally, we propose a series of guidelines for the design of fatigue-efficient mid-air interfaces.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {1063–1072},
    numpages = {10},
    keywords = {consumed endurance, endurance, gorilla-arm, mid-air interactions, mid-air text-entry, seato mid-air keyboard},
    location = {Toronto, Ontario, Canada},
    series = {CHI '14},
}


@article{Hinderks:2019,
    author = {Hinderks, Andreas and Schrepp, Martin and Domínguez Mayo, Francisco José and Escalona, M.J. and Thomaschewski, Jörg},
    year = {2019},
    month = jul,
    pages = {38--44},
    title = {{Developing a UX KPI based on the User Experience Questionnaire}},
    volume = {65},
    journal = {Computer Standards \& Interfaces},
    doi = {10.1016/j.csi.2019.01.007},
}


@inproceedings{Hoffman:2010,  
    author={Hoffman, Michael and Varcholik, Paul and LaViola, Joseph J.},   
    booktitle={Proc. of the IEEE Virtual Reality Conference},
    series={VR '10}, 
    url={https://ieeexplore.ieee.org/abstract/document/5444813},
    doi={https://doi.org/10.1109/VR.2010.5444813},
    venue={Boston, MA, USA},
    dates={20-24 March 2010},
    publisher={IEEE Computer Society Press},
    address={Los Alamitos, USA},
    title={{Breaking the status quo: Improving 3D gesture recognition with spatially convenient input devices}},   
    year={2010},   
    volume={},  
    number={},  
    pages={59-66},
}


@inproceedings{Holz:2011,
    author = {Holz, Christian and Wilson, Andrew},
    title = {{Data Miming: Inferring Spatial Object Descriptions from Human Gesture}},
    year = {2011},
    isbn = {9781450302289},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1978942.1979060},
    doi = {10.1145/1978942.1979060},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {811–820},
    numpages = {10},
    keywords = {shape descriptions, object retrieval, gestures, depth camera, 3D modeling},
    venue = {Vancouver, BC, Canada},
    series = {CHI ’11},
}


@inproceedings{Hoste:2014,
    author    = {Hoste, Lode and Signer, Beat},
    editor    = {Echtler, Florian and Hoste, Lode and Kammer, Dietrich and Signer, Beat and Vanacken, Davy},
    title     = {{Criteria, Challenges and Opportunities for Gesture Programming Languages}},
    booktitle = {Proceedings of the Workshop on Engineering Gestures for Multimodal Interfaces Co-located with the 6th {ACM} {SIGCHI} Symposium on Engineering Interactive Computing Systems, EGMI@EICS 2014, Rome, Italy, June 17, 2014},
    series    = {{CEUR} Workshop Proceedings},
    volume    = {1190},
    pages     = {22--29},
    publisher = {CEUR-WS.org},
    year      = {2014},
    url       = {http://ceur-ws.org/Vol-1190/paper4.pdf},
    timestamp = {Tue, 28 May 2019 16:23:27 +0200},
    biburl    = {https://dblp.org/rec/conf/eics/HosteS14.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Hu:2022,
    AUTHOR = {Hu, Yuan and Chen, Lei and Wang, Zhibin and Pan, Xiang and Li, Hao},
    TITLE = {{Towards a More Realistic and Detailed Deep-Learning-Based Radar Echo Extrapolation Method}},
    JOURNAL = {Remote Sensing},
    VOLUME = {14},
    YEAR = {2022},
    NUMBER = {1},
    article-NUMBER = {24},
    URL = {https://www.mdpi.com/2072-4292/14/1/24},
    ISSN = {2072-4292},
    ABSTRACT = {Deep-learning-based radar echo extrapolation methods have achieved remarkable progress in the precipitation nowcasting field. However, they suffer from a common notorious problem - they tend to produce blurry predictions. Although some efforts have been made in recent years, the blurring problem is still under-addressed. In this work, we propose three effective strategies to assist deep-learning-based radar echo extrapolation methods to achieve more realistic and detailed prediction. Specifically, we propose a spatial generative adversarial network (GAN) and a spectrum GAN to improve image fidelity. The spatial and spectrum GANs aim at penalizing the distribution discrepancy between generated and real images from the spatial domain and spectral domain, respectively. In addition, a masked style loss is devised to further enhance the details by transferring the detailed texture of ground truth radar sequences to extrapolated ones. We apply a foreground mask to prevent the background noise from transferring to the outputs. Moreover, we also design a new metric termed the power spectral density score (PSDS) to quantify the perceptual quality from a frequency perspective. The PSDS metric can be applied as a complement to other visual evaluation metrics (e.g., LPIPS) to achieve a comprehensive measurement of image sharpness. We test our approaches with both ConvLSTM baseline and U-Net baseline, and comprehensive ablation experiments on the SEVIR dataset show that the proposed approaches are able to produce much more realistic radar images than baselines. Most notably, our methods can be readily applied to any deep-learning-based spatiotemporal forecasting models to acquire more detailed results.},
    DOI = {10.3390/rs14010024},
}


@article{Huang:2019,
    title={{Gesture-based system for next generation natural and intuitive interfaces}},
    volume={33},
    DOI={10.1017/S0890060418000045},
    number={1},
    journal={Artificial Intelligence for Engineering Design, Analysis and Manufacturing},
    publisher={Cambridge University Press},
    author={Huang, Jinmiao and Jaiswal, Prakhar and Rai, Rahul},
    year={2019},
    pages={54–68},
}


@inproceedings{Huesser:2021,
    author="Huesser, Cloe and Schubiger, Simon and {\c{C}}{\"o}ltekin, Arzu",
    editor="Ardito, Carmelo and Lanzilotti, Rosa and Malizia, Alessio and Petrie, Helen and Piccinno, Antonio and Desolda, Giuseppe and Inkpen, Kori",
    title="Gesture Interaction in Virtual Reality",
    booktitle="Human-Computer Interaction -- INTERACT 2021",
    year="2021",
    publisher="Springer International Publishing",
    address="Cham",
    pages="151--160",
    abstract="We explore gestures as interaction methods in virtual reality (VR). We detect hand and body gestures using human pose estimation based on off-the-shelf optical camera images using machine learning, and obtain reliable gesture recognition without additional sensors. We then employ an avatar to prompt users to learn and use gestures to communicate. Finally, to understand how well gestures serve as interaction methods, we compare the studied gesture-based interaction methods with baseline common interaction modalities in VR (controllers, gaze interaction) in a pilot study including usability testing.",
    isbn="978-3-030-85613-7",
}


%===========================================================
% References starting by I
%===========================================================
@article{IEEE:2020,  
    author={Institute of Electrical and Electronics Engineers},  
    journal={IEEE Std 521-2019 (Revision of IEEE Std 521-2002)},   
    title={{IEEE Standard Letter Designations for Radar-Frequency Bands}},   
    year={2020},  
    volume={},  
    number={},  
    pages={1-15},  
    doi={10.1109/IEEESTD.2020.8999849},
    url={https://ieeexplore.ieee.org/servlet/opac?punumber=8999827},
}


@inproceedings{Igarashi:1999,
    author = {Igarashi, Takeo and Matsuoka, Satoshi and Tanaka, Hidehiko},
    title = {{Teddy: A Sketching Interface for 3D Freeform Design}},
    year = {1999},
    isbn = {0201485605},
    publisher = {ACM Press/Addison-Wesley Publishing Co.},
    address = {USA},
    url = {https://doi.org/10.1145/311535.311602},
    doi = {10.1145/311535.311602},
    booktitle = {Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques},
    pages = {409–416},
    numpages = {8},
    keywords = {gestures, 3D modeling, sketching, inflation, chordal axes, pen-based systems, design},
    series = {SIGGRAPH ’99},
}


@online{Imec:2019,
  author = {Interuniversity Microelectronics Centre (IMEC)},
  title = {{140 GHz radar for gesture recognition technology and driver monitoring}},
  year = {2019},
  url = {https://www.imec-int.com/en/expertise/radar-sensing-systems/140ghz-radar-modules},
  urldate = {2019-02-03},
}


@inproceedings{Ishak:2018,
	title = {Human {Motion} {Training} {Data} {Generation} for {Radar} {Based} {Deep} {Learning} {Applications}},
	doi = {10.1109/ICMIM.2018.8443559},
	abstract = {Radar sensors are utilized for detection and classification purposes in various applications. In order to use deep learning techniques, lots of training data are required. Accordingly, lots of measurements and labelling tasks are then needed. For the purpose of pre-training or examining first ideas before bringing them into reality, synthetic radar data are of great help. In this paper, a workflow for automatically generating radar data of human gestures is presented, starting with creating the desired animations until synthesizing radar data and getting the final required dataset. The dataset could then be used for training deep learning models. A classification scenario applying this workflow is also introduced.},
	booktitle = {2018 {IEEE} {MTT}-{S} {International} {Conference} on {Microwaves} for {Intelligent} {Mobility} ({ICMIM})},
	author = {Ishak, Karim and Appenrodt, Nils and Dickmann, Jrgen and Waldschmidt, Christian},
	month = apr,
	year = {2018},
	keywords = {Animation, classification scenario, computer animation, Databases, deep learning models, deep learning techniques, gesture recognition, human gestures, human motion training data generation, image classification, image motion analysis, labelling tasks, learning (artificial intelligence), object detection, radar applications, radar based deep learning applications, Radar cross-sections, radar data synthesis, Radar imaging, radar sensors, Sensors, Spectrogram, synthetic radar data},
	pages = {1--4},
}


@techreport{iso25010,
    author = "ISO",
    title = {{ISO/IEC 25010 - Software Quality Product Standard}},
    institution = "International Standard Organization",
    type = "standard",
    address = "Geneva",
    year = "2019",
    url="https://iso25000.com/index.php/en/iso-25000-standards/iso-25010?limit=3\&limitstart=0",
}


@techreport{iso9241,
    author = "ISO",
    title = {{ISO/IEC 9241, Ergonomics of human-system interaction — Part 11: Usability: Definitions and concepts}},
    institution = "International Standard Organization",
    type = "standard",
    address = "Geneva",
    year = "2018",
    url="https://www.iso.org/obp/ui/\#iso:std:iso:9241:-11:en",
}


%===========================================================
% References starting by J
%===========================================================
@inproceedings{Jakobsen:2015,
    author = {Jakobsen, Mikkel R. and Jansen, Yvonne and Boring, Sebastian and Hornb\ae{}k, Kasper},
    title = {{Should I Stay or Should I Go? Selecting Between Touch and Mid-Air Gestures for Large-Display Interaction}},
    year = {2015},
    isbn = {978-3-319-22697-2},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-319-22698-9_31},
    doi = {10.1007/978-3-319-22698-9_31},
    abstract = {Users can interact with large displays in many ways, including touch and mid-air gestures. However, it remains unclear how these ways compare and when users choose one over the other. In a first experiment, we compare touch and mid-air gestures to identify their relative performance for target acquisition. In a second experiment, participants choose freely between touch and mid-air gestures and we artificially require movement to simulate situations where mid-air is considered beneficial. Results from the first experiment show mid-air to be overall slower than touch depending on the task; in the second experiment, participants mostly chose touch in particular for selecting small targets and they rarely switched between mid-air and touch. Results also show that when faced with an increasing cost of using touch in the form of movement, participants chose mid-air over touch; touch remains as fast as mid-air on average.},
    booktitle = {Human-Computer Interaction – INTERACT 2015},
    pages = {455–473},
    numpages = {19},
    keywords = {Freehand gestures, Touch, Large display, User study, Mid-air},
}


@inproceedings{Jang:2017,
    author = {Jang, Sujin and Stuerzlinger, Wolfgang and Ambike, Satyajit and Ramani, Karthik},
    title = {{Modeling Cumulative Arm Fatigue in Mid-Air Interaction based on Perceived Exertion and Kinetics of Arm Motion}},
    year = {2017},
    isbn = {9781450346559},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3025453.3025523},
    doi = {10.1145/3025453.3025523},
    abstract = {Quantifying cumulative arm muscle fatigue is a critical factor in understanding, evaluating, and optimizing user experience during prolonged mid-air interaction. A reasonably accurate estimation of fatigue requires an estimate of an individual's strength. However, there is no easy-to-access method to measure individual strength to accommodate inter-individual differences. Furthermore, fatigue is influenced by both psychological and physiological factors, but no current HCI model provides good estimates of cumulative subjective fatigue. We present a new, simple method to estimate the maximum shoulder torque through a mid-air pointing task, which agrees with direct strength measurements. We then introduce a cumulative fatigue model informed by subjective and biomechanical measures. We evaluate the performance of the model in estimating cumulative subjective fatigue in mid-air interaction by performing multiple cross-validations and a comparison with an existing fatigue metric. Finally, we discuss the potential of our approach for real-time evaluation of subjective fatigue as well as future challenges.},
    booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
    pages = {3328–3339},
    numpages = {12},
    keywords = {perceived exertion, mid-air interaction, maximum arm strength, cumulative fatigue model, biomechanical arm model},
    location = {Denver, Colorado, USA},
    series = {CHI '17},
}


@article{Jiang:2018,
    author     = {Jiang, Xianta and Xiao, Zhen Gang and Menon, Carlo},
    journal    = {Virtual Real.},
    title      = {{Virtual Grasps Recognition Using Fusion of Leap Motion and Force Myography}},
    year       = {2018},
    issn       = {1359-4338},
    month      = nov,
    number     = {4},
    pages      = {297–308},
    volume     = {22},
    address    = {Berlin, Heidelberg},
    doi        = {10.1007/s10055-018-0339-2},
    issue_date = {November 2018},
    keywords   = {Leap Motion, Hand gesture recognition, VR, Grasp classification, Force myography},
    numpages   = {12},
    publisher  = {Springer-Verlag},
    url        = {https://doi.org/10.1007/s10055-018-0339-2},
}


@article{Jonard:2019,
    author    = {Jonard, Fran{\c{c}}ois and Andr{\'{e}}, Fr{\'{e}}d{\'{e}}ric and Pinel, Nicolas and Warren, Craig and Vereecken, Harry and Lambot, S{\'{e}}bastien},
    title     = {{Modeling of Multilayered Media Green's Functions With Rough Interfaces}},
    journal   = {{IEEE} Trans. Geosci. Remote. Sens.},
    volume    = {57},
    number    = {10},
    pages     = {7671--7681},
    year      = {2019},
    url       = {https://doi.org/10.1109/TGRS.2019.2915676},
    doi       = {10.1109/TGRS.2019.2915676},
    timestamp = {Tue, 12 May 2020 16:46:43 +0200},
    biburl    = {https://dblp.org/rec/journals/tgrs/JonardAPWVL19.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


%===========================================================
% References starting by K
%===========================================================
@article{Kane:2016,
    title = "A Framework to Plot and Recognize Hand Motion Trajectories towards Development of Non-tactile Interfaces",
    journal = "Procedia Computer Science",
    volume = "84",
    pages = "6 - 13",
    year = "2016",
    note = "Proceeding of the Seventh International Conference on Intelligent Human Computer Interaction (IHCI 2015)",
    issn = "1877-0509",
    doi = "https://doi.org/10.1016/j.procs.2016.04.059",
    url = "http://www.sciencedirect.com/science/article/pii/S1877050916300734",
    author = "Kane, Lalit and Khanna, Pritee",
    keywords = "dynamic gestures, trajectory recognition, finger detection, reference vector",
    abstract = "This work demonstrates a real time framework to recognize trajectories articulated in the air using bare hand motion. A frontend is established to plot the trajectories as well as to spot the interleaved dynamic gestures. Finger detection based controls and trajectory plotting velocity help to spot the gesture boundaries. Trajectories are described through a unique Equi-Polar Signature (EPS) derived from circular grid normalization of trajectory points. EPS is invariant to translation, scale, rotation and stroke directions. k-Nearest Neighbor (KNN) classification strategy recognizes EPSs of digits 0-9 and operator symbols ‘+’, ‘–’, ‘×’, and ‘/’. Unlike previous path alignment algorithms, the proposed EPS scheme executes in linear time and fits to real-time constraints. On a customized depth video dataset of 2280 trajectories, 94.1 recognition accuracy is achieved.",
}


@inproceedings{Kang:2019,
	title = {Mining {Spatio}-{Temporal} {Features} from {mmW} {Radar} echoes for {Hand} {Gesture} {Recognition}},
	doi = {10.1109/APMC46564.2019.9038713},
	abstract = {Human gesture recognition is a new way of interaction and a new application direction of millimeter wave radar. Compared with Doppler radar, FMCW radar can eliminate Doppler frequency interference of moving targets at different distances and accurately obtain the velocity-range information during gesture motion. In this paper, we use the 77GHz millimeter wave radar to extract the time variation characteristics of the Doppler frequency of the gesture. The convolutional neural network was selected to classify the gesture mining spatiotemporal features of the five volunteers. The experimental results show that the feature can describe the gesture velocity change information well and can significantly improve the versatility of the network by adding small amount data of more volunteers data to establish a personal dataset.},
	booktitle = {2019 {IEEE} {Asia}-{Pacific} {Microwave} {Conference} ({APMC})},
	author = {Kang, Zhang and Shengchang, Lan and Guiyuan, Zhang},
	month = dec,
	year = {2019},
	keywords = {application direction, convolutional neural nets, convolutional neural network, CW radar, data mining, Doppler effect, doppler frequency, Doppler radar, feature extraction, Feature extraction, FM radar, FMCW radar, gesture motion, gesture recognition, Gesture recognition, gesture velocity change information, hand gesture recognition, human gesture recognition, image classification, image motion analysis, millimeter wave radar, Millimeter wave radar, mmW radar echoes, personal dataset, radar computing, radar cross-sections, Radar imaging, spatio-temporal feature mining, time variation characteristics, velocity-range information},
	pages = {93--95},
}


@article{Kendall:1939,
    ISSN = {00034851},
    URL = {http://www.jstor.org/stable/2235668},
    author = {Kendall, M. G. and Babington Smith, B.},
    journal = {The Annals of Mathematical Statistics},
    number = {3},
    pages = {275--287},
    publisher = {Institute of Mathematical Statistics},
    title = {{The Problem of m Rankings}},
    urldate = {2023-10-16},
    volume = {10},
    year = {1939},
}


@inbook{Kendon:1980,
    url = {https://doi.org/10.1515/9783110813098.207},
    title = {{Gesticulation and Speech: Two Aspects of the Process of Utterance}},
    booktitle = {The Relationship of Verbal and Nonverbal Communication},
    author = {Kendon, Adam},
    publisher = {De Gruyter Mouton},
    address = {Berlin, New York},
    pages = {207--228},
    doi = {doi:10.1515/9783110813098.207},
    isbn = {9783110813098},
    year = {1980},
    lastchecked = {2023-02-02},
}


@article{Kendon:1988,
    author = {Kendon, Adam},
    year = {1988},
    month = jan,
    pages = {131--141},
    title = {{How gestures can become like words}},
    journal = {Crosscultural Perspectives in Nonverbal Communication},
}


@article{Kern:2020,
	title = {Robust {Doppler}-{Based} {Gesture} {Recognition} {With} {Incoherent} {Automotive} {Radar} {Sensor} {Networks}},
	volume = {4},
	issn = {2475-1472},
	doi = {10.1109/LSENS.2020.3033586},
	abstract = {In this letter, the capabilities of an incoherent radar sensor network for robust Doppler-based gesture recognition are investigated, and a significant performance boost is demonstrated. A comprehensive dataset is recorded with an incoherent sensor network consisting of three time-synchronized 77GHz frequency-modulated continuous wave radars. Based on this dataset, we show that differential Doppler features obtained from the varying viewing angles result in a significant multistatic gain for classification, particularly for high intraclass variations and low Doppler frequencies. For the most complex dataset, cross-user validation accuracy of a convolutional neural network with optimized data fusion is improved by 7.4\% to an overall value of 87.1\%, which we regard to be high as gestures are not designed for distinguishability but reflect everyday control and communication signals.},
	number = {11},
	journal = {IEEE Sensors Letters},
	author = {Kern, Nicolai and Steiner, Maximilian and Lorenzin, Ramona and Waldschmidt, Christian},
	month = nov,
	year = {2020},
	keywords = {autonomous driving, Chirp, Doppler effect, Doppler radar, gesture recognition, Microwave/millimeter wave sensors, multistatic radar, radar sensor network, Sensor systems, Spectrogram},
	pages = {1--4},
}


@inproceedings{Khalaf:2019,
    author = {Khalaf, Ahmed S. and Alharthi, Sultan A. and Dolgov, Igor and Toups, Z O.},
    title = {{A Comparative Study of Hand Gesture Recognition Devices in the Context of Game Design}},
    year = {2019},
    isbn = {9781450368919},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3343055.3360758},
    doi = {10.1145/3343055.3360758},
    abstract = {Gesture recognition devices provide a new means for natural human-computer interaction. However, when selecting these devices for games, designers might find it challenging to decide which gesture recognition device will work best. In the present research, we compare three vision-based, hand gesture devices: Leap Motion, Microsoft's Kinect, and Intel's RealSense. We developed a simple hand-gesture based game to evaluate performance, cognitive demand, comfort, and player experience of using these gesture devices. We found that participants' preferred and performed much better using Leap Motion and Kinect compared to using RealSense. Leap Motion also outperformed or was equivalent to Kinect. These findings suggest that not all gesture recognition devices can be suitable for games and that designers need to make better decisions when selecting gesture recognition devices and designing gesture based games to insure the usability, accuracy, and comfort of such games.},
    booktitle = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
    pages = {397–402},
    numpages = {6},
    keywords = {realsense., games, gesture recognition devices, kinect, hand gesture, leap motion},
    venue = {Daejeon, Republic of Korea},
    series = {ISS '19},
}


@article{Khan:2017,
	title = {Hand-{Based} {Gesture} {Recognition} for {Vehicular} {Applications} {Using} {IR}-{UWB} {Radar}},
	volume = {17},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/17/4/833},
	doi = {10.3390/s17040833},
	abstract = {Modern cars continue to offer more and more functionalities due to which they need a growing number of commands. As the driver tries to monitor the road and the graphic user interface simultaneously, his/her overall efficiency is reduced. In order to reduce the visual attention necessary for monitoring, a gesture-based user interface is very important. In this paper, gesture recognition for a vehicle through impulse radio ultra-wideband (IR-UWB) radar is discussed. The gestures can be used to control different electronic devices inside a vehicle. The gestures are based on human hand and finger motion. We have implemented a real-time version using only one radar sensor. Studies on gesture recognition using IR-UWB radar have rarely been carried out, and some studies are merely simple methods using the magnitude of the reflected signal or those whose performance deteriorates largely due to changes in distance or direction. In this study, we propose a new hand-based gesture recognition algorithm that works robustly against changes in distance or direction while responding only to defined gestures by ignoring meaningless motions. We used three independent features, i.e., variance of the probability density function (pdf) of the magnitude histogram, time of arrival (TOA) variation and the frequency of the reflected signal, to classify the gestures. A data fitting method is included to differentiate between gesture signals and unintended hand or body motions. We have used the clustering technique for the classification of the gestures. Moreover, the distance information is used as an additional input parameter to the clustering algorithm, such that the recognition technique will not be vulnerable to distance change. The hand-based gesture recognition proposed in this paper would be a key technology of future automobile user interfaces.},
	number = {4},
	journal = {Sensors},
	author = {Khan, Faheem and Leem, Seong Kyu and Cho, Sung Ho},
	year = {2017},
    pages = {1--18}
}


@article{Khan:2020,
	title = {In-{Air} {Continuous} {Writing} {Using} {UWB} {Impulse} {Radar} {Sensors}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.2994281},
	abstract = {We developed an impulse radio ultra-wideband (IR-UWB) radar-based system that can recognize alphanumeric characters in midair without the need for any handheld device. The hardware consists of four IR-UWB radar sensors set up with a rectangular geometry. Writing a single character in midair results in artifacts that make some characters look similar on a position trajectory-based (x, y) plane, which makes them difficult to classify. Thus, we developed an algorithm that transforms 2D coordinate image data into trigonometric ratios (i.e., tangents) and plots them against the time axis to obtain unique images for training a convolutional neural network. An extended Kalman filter is used to obtain the 2D trajectories of hand motions. To evaluate our proposed method, we first applied it to characters that may be written in midair very simply without creating artifacts and compared its performance with that of a state-of-the-art digit classification algorithm. Then, we considered combining characters written midair with and without artifacts. After the individual character recognition, we combined the characters into words. We defined a specific marker based on an energy threshold to detect the start and end of a character for midair writing. The energy level was found to change drastically when the hand is pulled in and out of the radar plane. The proposed method was found to outperform the current state of the art at character classification when artifacts are present in the images.},
	journal = {IEEE Access},
	author = {Khan, Faheem and Leem, Seong Kyu and Cho, Sung Ho},
	year = {2020},
	keywords = {2D coordinate image data, 2D trajectories, alphabet writing, alphanumeric character recognition, character classification, character recognition, Clutter, convolutional neural nets, convolutional neural network, digit classification algorithm, energy level, energy threshold, extended Kalman filter, gesture recognition, Gesture recognition, hand motions, handheld device, image classification, image filtering, image motion analysis, impulse radio ultra-wideband, impulse radio ultrawideband radar-based system, in-air continuous writing, in-air writing, IR-UWB radar sensors, Kalman filters, midair writing, nonlinear filters, pattern analysis, position trajectory-based (x, radar computing, Radar cross-sections, radar imaging, radar plane, rectangular geometry, Sensors, single character, specific marker, Trajectory, trigonometric ratios, ultra wideband radar, UWB impulse radar sensors, Writing, y) plane},
	pages = {99302--99311},
}


@article{Khushaba:2022,
    author    = {Khushaba, Rami N. and Hill, Andrew John},
    title     = {{Radar-Based Materials Classification Using Deep Wavelet Scattering Transform: A Comparison of Centimeter vs. Millimeter Wave Units}},
    journal   = {{IEEE} Robotics Autom. Lett.},
    volume    = {7},
    number    = {2},
    pages     = {2016--2022},
    year      = {2022},
    url       = {https://doi.org/10.1109/LRA.2022.3143200},
    doi       = {10.1109/LRA.2022.3143200},
    timestamp = {Fri, 01 Apr 2022 11:23:02 +0200},
    biburl    = {https://dblp.org/rec/journals/ral/KhushabaH22.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Kim:2016,
	title = {Hand {Gesture} {Recognition} {Using} {Micro}-{Doppler} {Signatures} {With} {Convolutional} {Neural} {Network}},
	volume = {4},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2016.2617282},
	abstract = {In this paper, we investigate the feasibility of recognizing human hand gestures using micro-Doppler signatures measured by Doppler radar with a deep convolutional neural network (DCNN). Hand gesture recognition using radar can be applied to control electronic appliances. Compared with an optical recognition system, radar can work regardless of light conditions and it can be embedded in a case. We classify ten different hand gestures, with only micro-Doppler signatures on spectrograms without range information. The ten gestures, which included swiping from left to right, swiping from right to left, rotating clockwise, rotating counterclockwise, pushing, double pushing, holding, and double holding, were measured using Doppler radar and their spectrograms investigated. A DCNN was employed to classify the spectrograms, with 90\% of the data utilized for training and the remaining 10\% for validation. After five-fold validation, the classification accuracy of the proposed method was found to be 85.6\%. With seven gestures, the accuracy increased to 93.1\%.},
	journal = {IEEE Access},
	author = {Kim, Youngwook and Toomajian, Brian},
	year = {2016},
	keywords = {deep convolutional neural network, deep convolutional neural networks, Doppler radar, electronic appliances, gesture recognition, Gesture recognition, Hand gesture, hand gesture recognition, Laser radar, micro-Doppler signatures, microDoppler signatures, neural nets, Neural networks, radar computing, radar imaging, Spectrogram, Spectrograms},
	pages = {7125--7130},
}


@article{Kim:2017b,
	title = {A {Hand} {Gesture} {Recognition} {Sensor} {Using} {Reflected} {Impulses}},
	volume = {17},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2017.2679220},
	abstract = {This paper introduces a hand gesture recognition sensor using ultra-wideband impulse signals, which are reflected from a hand. The reflected waveforms in time domain are determined by the reflection surface of a target. Thus every gesture has its own reflected waveform. Thus we propose to use machine learning, such as convolutional neural network (CNN) for the gesture classification. The CNN extracts its own feature and constructs classification model then classifies the reflected waveforms. Six hand gestures from american sign language (ASL) are used for an experiment and the result shows more than 90\% recognition accuracy. For fine movements, a rotating plaster model is measured with 10° step. An average recognition accuracy is also above 90\%.},
	number = {10},
	journal = {IEEE Sensors Journal},
	author = {Kim, Seo Yul and Han, Hong Gul and Kim, Jin Woo and Lee, Sanghoon and Kim, Tae Wook},
	month = may,
	year = {2017},
	keywords = {american sign language, Antenna measurements, CNN, convolutional neural network, detection, gesture classification, gesture recognition, Gesture recognition, Gesture sensor, hand gesture recognition sensor, image sensors, impulse radio, Intelligent sensors, neural nets, palmprint recognition, reflected impulses, reflection surface, rotating plaster model, Shape, Support vector machines, time domain, Time-domain analysis, ultra-wideband impulse signals},
	pages = {2975--2976},
}


@inproceedings{Kim:2017a,
	title = {Application of {Doppler} radar for the recognition of hand gestures using optimized deep convolutional neural networks},
	doi = {10.23919/EuCAP.2017.7928465},
	abstract = {In this paper, we investigate the optimal structure of deep convolutional neural networks for classifying human hand gestures using Doppler radar. When hand motions are captured by Doppler radar, the unique micro-Doppler signatures can be observed in the spectrogram. If the signature is distinguishable by a classifier, then the hand gesture can be used for controlling electronics and as an input modality for a human-computer interface. To classiiy signatures in the spectrogram, we propose the use of a deep convolutional neural network (DCNN) as a classifier. DCNN is a powerful classifier that extracts features as well as class boundaries through a training process. We measured seven hand gestures performed in front of Doppler radar while generating spectrograms. To identify an optimal structure, we trained several DCNNs by changing hyperparameters, such as the number of convolutional layers, the number of filters, and the filter size. The classification accuracy obtained from the optimal DCNN structure was approximately 87\%.},
	booktitle = {2017 11th {European} {Conference} on {Antennas} and {Propagation} ({EUCAP})},
	author = {Kim, Youngwook and Toomajian, Brian},
	month = mar,
	year = {2017},
	keywords = {Convolution, convolutional layers, deep convolutional neural networks, Doppler effect, Doppler radar, feature extraction, filters, gesture recognition, Gesture recognition, hand gesture, hand gesture recognition, human hand gesture classification, human-computer interface, hyperparameters, image classification, micro-Doppler signatures, neural nets, Neural networks, radar imaging, sensing, spectrogram, Spectrogram},
	pages = {1258--1260},
}


@inproceedings{Kiselev:2019,
    author    = {Kiselev, Vasilij and Khlamov, Maxim and Chuvilin, Kirill},
    booktitle = {Proceedings of the 24th Conference of Open Innovations Association FRUCT},
    title     = {{Hand Gesture Recognition with Multiple Leap Motion Devices}},
    year      = {2019},
    address   = {Helsinki, Uusimaa, FIN},
    publisher = {FRUCT Oy},
    series    = {FRUCT'24},
    articleno = {23},
    keywords  = {leap motion, unity, Human computer interaction (HCI), gesture recognition, vr interfaces, Tracking},
    venue  = {Moscow, Russia},
    numpages  = {7},
}


@article{Kitchenham:2010,
	title = "Systematic literature reviews in software engineering -- A tertiary study",
	journal = "Information and Software Technology",
	volume = "52",
	number = "8",
	pages = "792-805",
	year = "2010",
	issn = "0950-5849",
	doi = "https://doi.org/10.1016/j.infsof.2010.03.006",
	url = "http://www.sciencedirect.com/science/article/pii/S0950584910000467",
	author = "Kitchenham, Barbara and  Pretorius, Rialette and Budgen, David and  Brereton, O. Pearl and  Turner, Mark and  Niazi, Mahmood and  Linkman, Stephen",
	keywords = "Systematic literature review, Mapping study, Software engineering, Tertiary study",
}


@inproceedings{Kohavi:1995,
    author    = {Kohavi, Ron},
    title     = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
    booktitle = {Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence},
    series = {IJCAI 95},
    venue={Montr{\'{e}}al Qu{\'{e}}bec, Canada},
    dates={August 20-25 1995},
    pages     = {1137--1145},
    publisher = {Morgan Kaufmann},
    year      = {1995},
    url       = {http://ijcai.org/Proceedings/95-2/Papers/016.pdf},
    timestamp = {Tue, 20 Aug 2019 16:17:30 +0200},
    biburl    = {https://dblp.org/rec/conf/ijcai/Kohavi95.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Kohlsdorf:2011,  
    author={Kohlsdorf, Daniel and Starner, Thad E. and Ashbrook, Daniel},  
    booktitle={Face and Gesture 2011},
    title={{MAGIC 2.0: A web tool for false positive prediction and prevention for gesture recognition systems}},   
    year={2011},  
    volume={},  
    number={},  
    pages={1-6},
}


@article{Kohlsdorf:2013,
	title = {{MAGIC} summoning: towards automatic suggesting and testing of gestures with low probability of false positives during use},
	volume = {14},
	issn = {1532-4435},
	shorttitle = {{MAGIC} summoning},
	abstract = {Gestures for interfaces should be short, pleasing, intuitive, and easily recognized by a computer. However, it is a challenge for interface designers to create gestures easily distinguishable from users' normal movements. Our tool MAGIC Summoning addresses this problem. Given a specific platform and task, we gather a large database of unlabeled sensor data captured in the environments in which the system will be used (an "Everyday Gesture Library" or EGL). The EGL is quantized and indexed via multi-dimensional Symbolic Aggregate approXimation (SAX) to enable quick searching. MAGIC exploits the SAX representation of the EGL to suggest gestures with a low likelihood of false triggering. Suggested gestures are ordered according to brevity and simplicity, freeing the interface designer to focus on the user experience. Once a gesture is selected, MAGIC can output synthetic examples of the gesture to train a chosen classifier (for example, with a hidden Markov model). If the interface designer suggests his own gesture and provides several examples, MAGIC estimates how accurately that gesture can be recognized and estimates its false positive rate by comparing it against the natural movements in the EGL. We demonstrate MAGIC's effectiveness in gesture selection and helpfulness in creating accurate gesture recognizers.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Kohlsdorf, Daniel and Starner, Thad E.},
	month = jan,
	year = {2013},
	keywords = {continuous recognition, false positives, gesture recognition, gesture spotting},
	pages = {209--242},
}


@book{Konar:2018,
    title = {{Gesture recognition : principles, techniques and applications}},
    author = {Konar, Amit and Saha, Sriparna},
    publisher = {Springer},
    address = {Berlin},
    isbn = {978-3-319-62210-1,3319622102,978-3-319-62212-5},
    year = {2018},
    series = {Studies in computational intelligence},
    edition = {1},
    volume = {724},
    doi = {10.1007/978-3-319-62212-5},
    url = {https://www.springer.com/gp/book/9783319622101},
}


@inproceedings{Koutsabasis:2016,
    author = {Koutsabasis, Panayiotis and Domouzis, Chris K.},
    title = {{Mid-Air Browsing and Selection in Image Collections}},
    year = {2016},
    isbn = {9781450341318},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2909132.2909248},
    doi = {10.1145/2909132.2909248},
    abstract = {Image collections are a common interaction pattern for 2D interfaces, however mid-air user interaction with collections has received little attention. We present a controlled experiment (within-groups, n=24) comparing three sets of hand gestures for mid-air browsing and selection in image collections, that were identified out of an elicitation study, using MS Kinect. Each set includes cursor-less gestures for browsing (sideways hand extension, wheel and swipe) and for selection/deselection (hand-up/hand-down). Task success was universal with high accuracy and few errors for all gestures. Sideways extension outperforms swipe and perceived effort for this gesture is significantly lower. Both gestures outperform wheel. We suggest that from a usability perspective, sideways hand extension should be preferred for browsing image galleries, if no other contextual factors apply. Also, the results of the elicitation study, in which most users proposed the swipe gesture for browsing, were not confirmed by the controlled usability experiment. This suggests a combined use of elicitation studies with rigorous usability testing, especially when gestures for particular user interface design patterns are sought.},
    booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
    pages = {21–27},
    numpages = {7},
    keywords = {Browsing, Usability, Mid-Air Interaction, Selection, Gestures, Kinect, Elicitation, Image Collections},
    venue = {Bari, Italy},
    series = {AVI '16},
}


@article{Koutsabasis:2019,
    author = {Koutsabasis, Panayiotis and Vogiatzidakis, Panagiotis},
    year  = {2019},
    title = {{Empirical Research in Mid-Air Interaction: A Systematic Review}},
    journal = {International Journal of Human–Computer Interaction},
    volume = {35},
    number = {18},
    pages = {1747-1768},
    publisher = {Taylor \& Francis},
    doi = {10.1080/10447318.2019.1572352},
    url = {https://doi.org/10.1080/10447318.2019.1572352},
}


@inproceedings{Kratz:2010,
    author = {Kratz, Sven and Rohs, Michael},
    title = {{A \$3 Gesture Recognizer: Simple Gesture Recognition for Devices Equipped with 3D Acceleration Sensors}},
    year = {2010},
    isbn = {9781605585154},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1719970.1720026},
    doi = {10.1145/1719970.1720026},
    booktitle = {Proceedings of the 15th International Conference on Intelligent User Interfaces},
    pages = {341–344},
    numpages = {4},
    keywords = {user interfaces, classifier, rapid prototyping, recognition rates, 3D gestures, gesture recognition},
    venue = {Hong Kong, China},
    series = {IUI ’10},
}


@inproceedings{Kratz:2011,
    author = {Kratz, Sven and Rohs, Michael},
    title = {{Protractor3D: A Closed-Form Solution to Rotation-Invariant 3D Gestures}},
    year = {2011},
    isbn = {9781450304191},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1943403.1943468},
    doi = {10.1145/1943403.1943468},
    booktitle = {Proceedings of the 16th International Conference on Intelligent User Interfaces},
    pages = {371–374},
    numpages = {4},
    keywords = {rotation invariance, gesture recognition, nearest neighbor approach, gesture-based interaction, template matching},
    venue = {Palo Alto, CA, USA},
    series = {IUI ’11},
}


@inproceedings{Kratz:2015,
    author = {Kratz, Sven and Back, Maribeth},
    title = {{Towards Accurate Automatic Segmentation of IMU-Tracked Motion Gestures}},
    year = {2015},
    isbn = {9781450331463},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2702613.2732922},
    doi = {10.1145/2702613.2732922},
    booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
    pages = {1337–1342},
    numpages = {6},
    keywords = {gesture annotation, motion gestures, gesture recognition, gesture segmentation, IMU, motion data},
    venue = {Seoul, Republic of Korea},
    series = {CHI EA ’15},
}


@inproceedings{Kratz:2016,
    author = {Kratz, Sven and Wiese, Jason},
    title = {{GestureSeg: Developing a Gesture Segmentation System Using Gesture Execution Phase Labeling by Crowd Workers}},
    year = {2016},
    isbn = {9781450343220},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2933242.2933261},
    doi = {10.1145/2933242.2933261},
    booktitle = {Proceedings of the 8th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
    pages = {61–72},
    numpages = {12},
    venue = {Brussels, Belgium},
    series = {EICS ’16},
}


@article{Krishnamurthy:2015,
    title = {{A gesture-free geometric approach for mid-air expression of design intent in 3D virtual pottery}},
    journal = {Computer-Aided Design},
    volume = {69},
    pages = {11-24},
    year = {2015},
    issn = {0010-4485},
    doi = {https://doi.org/10.1016/j.cad.2015.06.006},
    url = {https://www.sciencedirect.com/science/article/pii/S001044851500086X},
    author = {Krishnamurthy, Vinayak and Ramani, Karthik},
    keywords = {Hand-based shape modeling, Mid-air interactions, Virtual pottery, Gestures, Natural user interfaces, Mesh deformation},
    abstract = {The advent of depth cameras has enabled mid-air interactions for shape modeling with bare hands. Typically, these interactions employ a finite set of pre-defined hand gestures to allow users to specify modeling operations in virtual space. However, human interactions in real world shaping processes (such as pottery or sculpting) are complex, iterative, and continuous. In this paper, we show that the expression of user intent in shaping processes can be derived from the geometry of contact between the hand and the manipulated object. Specifically, we describe the design and evaluation of a geometric interaction technique for bare-hand mid-air virtual pottery. We model the shaping of a pot as a gradual and progressive convergence of the pot’s profile to the shape of the user’s hand represented as a point-cloud (PCL). Thus, a user does not need to learn, know, or remember any gestures to interact with our system. Our choice of pottery simplifies the geometric representation, allowing us to systematically study how users use their hands and fingers to express the intent of deformation during a shaping process. Our evaluations demonstrate that it is possible to enable users to express their intent for shape deformation without the need for a fixed set of gestures for clutching and deforming a shape.},
}
  
  
@inproceedings{Kristensson:2004,
    author = {Kristensson, Per-Ola and Zhai, Shumin},
    title = {{SHARK2: A Large Vocabulary Shorthand Writing System for Pen-based Computers}},
    booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
    series = {UIST '04},
    year = {2004},
    isbn = {1-58113-957-8},
    venue = {Santa Fe, NM, USA},
    pages = {43--52},
    numpages = {10},
    url = {http://doi.acm.org/10.1145/1029632.1029640},
    doi = {10.1145/1029632.1029640},
    acmid = {1029640},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {gesture recognition, shorthand, shorthand recognition, stenography, text input},
} 


@inproceedings{Kristensson:2011,
    author    = {Kristensson, Per Ola and Denby, Leif C.},
    editor    = {Hammond, Tracy and Nealen, Andrew},
    title     = {{Continuous Recognition and Visualization of Pen Strokes and Touch-Screen Gestures}},
    booktitle = {Sketch Based Interfaces and Modeling, Vancouver, BC, Canada, 5-7 August 2011. Proceedings},
    pages     = {95--102},
    publisher = {Eurographics Association},
    year      = {2011},
    url       = {https://doi.org/10.2312/SBM/SBM11/095-102},
    doi       = {10.2312/SBM/SBM11/095-102},
    timestamp = {Tue, 06 Nov 2018 11:06:57 +0100},
    biburl    = {https://dblp.org/rec/conf/sbm/KristenssonD11.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Kulhandjian:2019,
	title = {Sign {Language} {Gesture} {Recognition} {Using} {Doppler} {Radar} and {Deep} {Learning}},
	doi = {10.1109/GCWkshps45667.2019.9024607},
	abstract = {In this paper, we study American sign language (ASL) hand gesture recognition using Doppler radar. A set of ASL hand gesture motions are captured as micro- Doppler signals using a microwave X-band Doppler radar transceiver. We apply joint time-frequency analysis and observe the presence of the micro- Doppler signatures in the spectrogram. The micro- Doppler signatures of different hand gestures are analyzed using Matlab. Each hand gesture is observed to contain unique spectral characteristics. Based on unique spectral characteristics, we investigate the classification of ASL essential short phrases including emergency signals. For recognizing and characterizing the presence of micro-Doppler signatures in spectrogram we explore deep convolution neural network (DCNN) algorithm. We show that the DCNN algorithm can classify different sign language gestures based on the presence of micro- Doppler signatures in the spectrogram with fairly high accuracy. Experimental results reveal that utilizing 80\% of data for training, and the remaining 20\% for validation purposes in DCNN algorithm a validation accuracy of 87.5\% is achieved. To further improve the recognition system, we apply a very deep learning algorithm VGG-16 using transfer learning, which improves the validation accuracy to 95\%.},
	booktitle = {2019 {IEEE} {Globecom} {Workshops} ({GC} {Wkshps})},
	author = {Kulhandjian, Hovannes and Sharma, Prakshi and Kulhandjian, Michel and D'Amours, Claude},
	month = dec,
	year = {2019},
	keywords = {American sign language hand gesture recognition, ASL hand gesture motions, Assistive technology, Classification algorithms, Convolution, convolutional neural nets, deep convolution neural network algorithm, Doppler radar, feature extraction, gesture recognition, Gesture recognition, hand gestures, image classification, learning (artificial intelligence), micro Doppler signals, micro Doppler signatures, microDoppler signatures, microwave X-band Doppler radar transceiver, natural language processing, radar computing, sign language gesture recognition, sign language recognition, Signal processing algorithms, Spectrogram, time-frequency analysis, transceivers, unique spectral characteristics},
	pages = {1--6},
}


@article{Kumar:2017a,
    author     = {Kumar, Pradeep and Saini, Rajkumar and Roy, Partha Pratim and Dogra, Debi Prosad},
    journal    = {Multimedia Tools Appl.},
    title      = {{3D Text Segmentation and Recognition Using Leap Motion}},
    year       = {2017},
    issn       = {1380-7501},
    month      = aug,
    number     = {15},
    pages      = {16491–16510},
    volume     = {76},
    address    = {USA},
    doi        = {10.1007/s11042-016-3923-z},
    issue_date = {August 2017},
    keywords   = {3D air-writing, Gesture on air, Written text segmentation, Dynamic features, Touchless interfaces},
    numpages   = {20},
    publisher  = {Kluwer Academic Publishers},
    url        = {https://doi.org/10.1007/s11042-016-3923-z},
}


@article{Kumar:2017b,
    author     = {Kumar, Pradeep and Gauba, Himaanshu and Roy, Partha Pratim and Dogra, Debi Prosad},
    journal    = {Pattern Recogn. Lett.},
    title      = {{Coupled HMM-Based Multi-Sensor Data Fusion for Sign Language Recognition}},
    year       = {2017},
    issn       = {0167-8655},
    month      = jan,
    number     = {C},
    pages      = {1–8},
    volume     = {86},
    address    = {USA},
    issue_date = {January 2017},
    keywords   = {Bayesian classification, Depth sensors, Hidden Markov model (Coupled HMM, HMM), Sign language recognition},
    numpages   = {8},
    publisher  = {Elsevier Science Inc.},
}


@article{Kumar:2018,
    author     = {Kumar, Pradeep and Roy, Partha Pratim and Dogra, Debi Prosad},
    journal    = {Inf. Sci.},
    title      = {{Independent Bayesian Classifier Combination Based Sign Language Recognition Using Facial Expression}},
    year       = {2018},
    issn       = {0020-0255},
    month      = feb,
    number     = {C},
    pages      = {30–48},
    volume     = {428},
    address    = {USA},
    doi        = {10.1016/j.ins.2017.10.046},
    issue_date = {February 2018},
    keywords   = {Sign language recognition, Hidden Markov model (HMM), Depth sensors, Bayesian combination},
    numpages   = {19},
    publisher  = {Elsevier Science Inc.},
    url        = {https://doi.org/10.1016/j.ins.2017.10.046},
}


@inproceedings{Kunwar:2022,
    author    = {Kunwar, Utkarsh and Borar, Sheetal and Berghofer, Moritz and Kylm{\"{a}}l{\"{a}}, Julia and Aslan, Ilhan and Leiva, Luis A. and Oulasvirta, Antti},
    editor    = {Jacucci, Giulio and Kaski, Samuel and Conati, Cristina and Stumpf, Simone and Ruotsalo, Tuukka and Gajos, Krzysztof},
    title     = {{Robust and Deployable Gesture Recognition for Smartwatches}},
    booktitle = {Proceedings of 27th International Conference on Intelligent User Interfaces},
    series = {{IUI} '22},
    venue = {Helsinki, Finland},
    dates={March 22 - 25, 2022},
    pages     = {277--291},
    publisher = {{ACM}},
    year      = {2022},
    url       = {https://doi.org/10.1145/3490099.3511125},
    doi       = {10.1145/3490099.3511125},
    timestamp = {Thu, 23 Jun 2022 19:58:07 +0200},
    biburl    = {https://dblp.org/rec/conf/iui/KunwarBBKALO22.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Kurtenbach:1997,
    author = {Kurtenbach, Gordon and Fitzmaurice, George and Baudel, Thomas and Buxton, Bill},
    title = {{The Design of a GUI Paradigm Based on Tablets, Two-hands, and Transparency}},
    booktitle = {Proceedings of the ACM International Conference on Human Factors in Computing Systems},
    series = {CHI '97},
    year = {1997},
    isbn = {0-89791-802-9},
    venue = {Atlanta, Georgia, USA},
    pages = {35--42},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/258549.258574},
    doi = {10.1145/258549.258574},
    acmid = {258574},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {divided attention, marking menus, tablets, task integration, toolglass, transparency, two-handed input},
} 


@article{Kysh:2013,
    author = "Kysh, Lynn",
    title = {{Difference between a systematic review and a literature review}},
    year = "2013",
    month = "8",
    url = "https://figshare.com/articles/Difference_between_a_systematic_review_and_a_literature_review/766364",
    doi = "10.6084/m9.figshare.766364.v1",
}


%===========================================================
% References starting by L
%===========================================================
@phdthesis{Lahousse:2022,
    title = {{Ring-based gestural interaction: from elicitation to recognition}},
    author = {Lahousse, Arnaud},
    abstract = {Since the beginning of the computer age, interaction with the computer has been a challenge. The coming of artificial intelligence, and moreover, machine learning, have created new possible computer interfaces like gesture recognition. This field started with 2D gestures, but this part of gesture recognition has already been extensively studied. The researchers now focus on the 3D gestures. However, the devices used for those studies often are expensive and cumbersome for end-users. It is in this context that this work takes place. We have found an easier and cheaper sensor that can record 3D gestures; an accelerometer mounted on a ring. The goal is to find a way to use hand gesture recognition with this cheap and user-friendly device to see if it could be used in an end-user application.},
    Keywords = {Ring Gesture , QuantumLeap , Recognition , Jackknife , $P3 , Elicitation , Ring-Based},
    language = {Anglais},
    year = {2022},
    url = {http://hdl.handle.net/2078.1/thesis:36013},
    school = {UCL - Ecole polytechnique de Louvain},
}


@article{Lambot:2004,
    author={Lambot, S{\'{e}}bastien and Slob, Evert C. and van den Bosch, Idesbald and Stockbroeckx, Benoit and Vanclooster, Marnik},
    journal={IEEE Transactions on Geoscience and Remote Sensing}, 
    title={{Modeling of ground-penetrating Radar for accurate characterization of subsurface electric properties}}, 
    year={2004},
    volume={42},
    number={11},
    pages={2555-2568},
    doi={10.1109/TGRS.2004.834800},
}


@article{Lambot:2006,
    Author = {Lambot, S{\'{e}}bastien and Weiherm\"uller, L. and Huisman, J. A. and Vereecken, H. and Vanclooster, M. and Slob, Evert C.},
    Title = {{Analysis of air-launched ground-penetrating radar techniques to measure the soil surface water content}},
    Journal = {Water Resources Research},
    Volume = {42},
    doi = {10.1029/2006WR005097},
    Year = {2006}, 
}


@article{Lambot:2014,
    author={Lambot, S{\'{e}}bastien and Andr{\'{e}}, Fr{\'{e}}d{\'{e}}ric},
    journal={IEEE Transactions on Geoscience and Remote Sensing}, 
    title={{Full-Wave Modeling of Near-Field Radar Data for Planar Layered Media Reconstruction}}, 
    year={2014},
    volume={52},
    number={5},
    pages={2295-2303},
    doi={10.1109/TGRS.2013.2259243},
}


@inproceedings{Lan:2017,
	title = {A hand gesture recognition system based on {24GHz} radars},
	doi = {10.1109/ISANP.2017.8228827},
	abstract = {This paper presented a gesture recognition system for human-computer interaction based on 24GHz radars. We describe this system designed for frequently used gesture detection like hand pushing, hand pulling, hand lifting and hand shaking. Decision tree was established to classify these original signals into the four sets of gestures. Through a set of tests on this gesture-recognition system, we proposed that this system could achieve an overall accuracy rate higher than 92\%.},
	booktitle = {2017 {International} {Symposium} on {Antennas} and {Propagation} ({ISAP})},
	author = {Lan, Shengchang and He, Zonglong and Tang, Haoyu and Yao, Kai and Yuan, Wenshuang},
	month = oct,
	year = {2017},
	keywords = {24GHz radar, 24GHz radars, Classification algorithms, decision tree, decision trees, Decision trees, Doppler effect, Doppler radar, frequency 24.0 GHz, frequently used gesture detection, gesture recognition, Gesture recognition, hand gesture recognition system, human computer interaction, Human computer interaction, human-computer interaction, radar signal processing, signal classification},
	pages = {1--2},
}


@inproceedings{Lan:2018a,
	title = {Hand {Gesture} {Recognition} {Using} {Convolutional} {Neural} {Networks}},
	doi = {10.1109/USNC-URSI.2018.8602809},
	abstract = {This paper introduced a hand gesture recognition method based on convolutional neural networks (CNNs). The recognition scenario consisted in a three dimensional radar array to transmit and receive 24GHz continuous electromagnetic (EM) wave, and convert the scattered EM wave to the intermediate frequency (IF) signals. This paper used the the processed frequency spectrum as the input to the CNN. Then the CNN feature detection layer learned through data training, avoiding supervised feature extraction while learning implicitly from training data. It highlighted these features through convolution operating, pooling and a softmax function. Results showed that this system could achieve a high recognition accuracy rate higher than 96\%.},
	booktitle = {2018 {USNC}-{URSI} {Radio} {Science} {Meeting} ({Joint} with {AP}-{S} {Symposium})},
	author = {Lan, Shengchang and He, Zonglong and Chen, Weichu and Chen, Lijia},
	month = jul,
	year = {2018},
	keywords = {24GHz continuous electromagnetic wave, CNN feature detection layer, Convolution, convolutional neural nets, convolutional neural networks, Convolutional neural networks, data training, EM wave scattering, feature extraction, frequency 24.0 GHz, Frequency modulation, frequency spectrum, gesture recognition, Gesture recognition, hand gesture recognition method, learning (artificial intelligence), Neurons, Radar, recognition scenario, Sensors, spectral analysis},
	pages = {147--148},
}


@inproceedings{Lan:2018b,
	title = {Hand {Gesture} {Recognition} using a {Three}-dimensional 24 {GHz} {Radar} {Array}},
	doi = {10.1109/MWSYM.2018.8439658},
	abstract = {This paper proposed a hand gesture recognition method using a three-dimensional radar array. This array consisted of three radars to transmit and receive 24 GHz continuous electromagnetic (EM) wave. The integrated mixer heterodyned the scattered EM wave to the intermediate frequency (IF) with a local oscillator. Temporal and frequency signatures including I/Q signal magnitude difference, phase difference and spectra power integral were proposed as the features of recognition. A decision tree algorithm was constructed as the recognition classifier. 2000 training samples and 4000 untrained samples were used in the experiment. Results showed that this system could achieve a high recognition accuracy rate higher than 92\%.},
	booktitle = {2018 {IEEE}/{MTT}-{S} {International} {Microwave} {Symposium} - {IMS}},
	author = {Lan, Shengchang and He, Zonglong and Yao, Kai and Chen, Weichu},
	month = jun,
	year = {2018},
	note = {ISSN: 2576-7216},
	keywords = {24 GHz radar array, continuous electromagnetic wave, Decision tree, decision trees, Decision trees, Doppler frequency, Doppler radar, frequency 24.0 GHz, Frequency modulation, frequency signatures, gesture recognition, Hand gesture recognition, hand gesture recognition method, high recognition accuracy rate, I/Q signal magnitude difference, Indexes, integrated mixer, intermediate frequency, local oscillator, microwave oscillators, phase difference, Radar, radar signal processing, recognition classifier, Sensors, signal classification, spectra power integral, three-dimensional radar array, Thumb},
	pages = {138--140},
}


@inproceedings{Laput:2014,
    author    = {Laput, Gierad and Xiao, Robert and Chen, Xiang 'Anthony' and Hudson, Scott E. and Harrison, Chris},
    editor    = {Benko, Hrvoje and Dontcheva, Mira and Wigdor, Daniel},
    title     = {{Skin buttons: cheap, small, low-powered and clickable fixed-icon laser projectors}},
    booktitle = {Proceedings of the 27th Annual {ACM} Symposium on User Interface Software and Technology},
    series    = {UIST '14},
    venue  = {Honolulu, HI, USA},
    dates     = {October 5-8, 2014},
    pages     = {389--394},
    publisher = {{ACM}},
    year      = {2014},
    url       = {https://doi.org/10.1145/2642918.2647356},
    doi       = {10.1145/2642918.2647356},
    timestamp = {Thu, 12 Dec 2019 07:57:40 +0100},
    biburl    = {https://dblp.org/rec/conf/uist/LaputXCHH14.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inbook{Laput:2019,
    author = {Laput, Gierad and Harrison, Chris},
    title = {{Sensing Fine-Grained Hand Activity with Smartwatches}},
    year = {2019},
    isbn = {9781450359702},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3290605.3300568},
    abstract = {Capturing fine-grained hand activity could make computational experiences more powerful and contextually aware. Indeed, philosopher Immanuel Kant argued, "the hand is the visible part of the brain." However, most prior work has focused on detecting whole-body activities, such as walking, running and bicycling. In this work, we explore the feasibility of sensing hand activities from commodity smartwatches, which are the most practical vehicle for achieving this vision. Our investigations started with a 50 participant, in-the-wild study, which captured hand activity labels over nearly 1000 worn hours. We then studied this data to scope our research goals and inform our technical approach. We conclude with a second, in-lab study that evaluates our classification stack, demonstrating 95.2\% accuracy across 25 hand activities. Our work highlights an underutilized, yet highly complementary contextual channel that could unlock a wide range of promising applications.},
    booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
    pages = {1–13},
    numpages = {13},
}


@inproceedings{LaViola:2003,
	address = {New York, NY, USA},
	series = {{EGVE} '03},
	title = {Double exponential smoothing: an alternative to {Kalman} filter-based predictive tracking},
	isbn = {978-1-58113-686-9},
	shorttitle = {Double exponential smoothing},
	url = {https://doi.org/10.1145/769953.769976},
	doi = {10.1145/769953.769976},
	abstract = {We present novel algorithms for predictive tracking of user position and orientation based on double exponential smoothing. These algorithms, when compared against Kalman and extended Kalman filter-based predictors with derivative free measurement models, run approximately 135 times faster with equivalent prediction performance and simpler implementations. This paper describes these algorithms in detail along with the Kalman and extended Kalman Filter predictors tested against. In addition, we describe the details of a predictor experiment and present empirical results supporting the validity of our claims that these predictors are faster, easier to implement, and perform equivalently to the Kalman and extended Kalman filtering predictors.},
	urldate = {2020-12-29},
	booktitle = {Proceedings of the workshop on {Virtual} environments 2003},
	publisher = {Association for Computing Machinery},
	author = {LaViola, Joseph J.},
	month = may,
	year = {2003},
	pages = {199--206},
}


@article{LaViola:2013,
    author = {LaViola, Joseph J.},
    title = {{3D Gestural Interaction: The State of the Field}},
    year = {2013},
    issue_date = {18 December 2013},
    publisher = {Hindawi},
    address = {London, UK},
    volume = {2013},
    numpages = {18},
    articleno = {514641},
    url = {https://www.hindawi.com/journals/isrn/2013/514641/},
    doi = {10.1155/2013/514641},
    journal = {International Scholarly Research Notices},
}


@article{Lazarou:2021,
    title = {{A novel shape matching descriptor for real-time static hand gesture recognition}},
    journal = {Computer Vision and Image Understanding},
    volume = {210},
    pages = {103241},
    year = {2021},
    issn = {1077-3142},
    doi = {https://doi.org/10.1016/j.cviu.2021.103241},
    url = {https://www.sciencedirect.com/science/article/pii/S1077314221000850},
    author = {Lazarou, Michalis and Li, Bo and Stathaki, Tania},
    keywords = {Hand, Gesture, Matching, Recognition, Shape},
    abstract = {The current state-of-the-art hand gesture recognition methodologies heavily rely in the use of machine learning. However there are scenarios that machine learning cannot be applied successfully, for example in situations where data is scarce. This is the case when one-to-one matching is required between a query and a dataset of hand gestures where each gesture represents a unique class. In situations where learning algorithms cannot be trained, classic computer vision techniques such as feature extraction can be used to identify similarities between objects. Shape is one of the most important features that can be extracted from images, however the most accurate shape matching algorithms tend to be computationally inefficient for real-time applications. In this work we present a novel shape matching methodology for real-time hand gesture recognition. Extensive experiments were carried out comparing our method with other shape matching methods with respect to accuracy and computational complexity. Our method outperforms the other methods and provides a good combination of accuracy and computational efficiency for real-time applications.},
}


@inproceedings{Ledo:2018,
    author = {Ledo, David and Houben, Steven and Vermeulen, Jo and Marquardt, Nicolai and Oehlberg, Lora and Greenberg, Saul},
    title = {{Evaluation Strategies for HCI Toolkit Research}},
    year = {2018},
    isbn = {9781450356206},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3173574.3173610},
    abstract = {Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.},
    booktitle = {Proceedings of the ACM Int. Conf. on Human Factors in Computing Systems},
    series = {CHI '18},
    pages = {1–17},
    numpages = {17},
}


@article{Lee:2007,
    author={Lee, Joo-Young and Choi, Jeong-Wha and Kim, Ho},
    journal={Journal of physiological anthropology}, 
    title={{Determination of hand surface area by sex and body shape using alginate}}, 
    year={2007},
    volume={26},
    number={4},
    pages={475–483},
    doi={10.2114/jpa2.26.475},
    url={https://www.jstage.jst.go.jp/article/jpa2/26/4/26_4_475/_article},
}


@inproceedings{Lee:2013,
    author = {Lee, Jinha and Olwal, Alex and Ishii, Hiroshi and Boulanger, Cati},
    title = {{SpaceTop: Integrating 2D and Spatial 3D Interactions in a See-through Desktop Environment}},
    year = {2013},
    isbn = {9781450318990},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2470654.2470680},
    doi = {10.1145/2470654.2470680},
    abstract = {SpaceTop is a concept that fuses spatial 2D and 3D interactions in a single workspace. It extends the traditional desktop interface with interaction technology and visualization techniques that enable seamless transitions between 2D and 3D manipulations. SpaceTop allows users to type, click, draw in 2D, and directly manipulate interface elements that float in the 3D space above the keyboard. It makes it possible to easily switch from one modality to another, or to simultaneously use two modalities with different hands. We introduce hardware and software configurations for co-locating these various interaction modalities in a unified workspace using depth cameras and a transparent display. We describe new interaction and visualization techniques that allow users to interact with 2D elements floating in 3D space. We present the results from a preliminary user study that indicates the benefit of such hybrid workspaces.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {189–192},
    numpages = {4},
    keywords = {3d ui, augmented reality, desktop management},
    venue = {Paris, France},
    series = {CHI '13},
}


@article{Lee:2020,
	title = {Improving {Classification} {Accuracy} of {Hand} {Gesture} {Recognition} {Based} on 60 {GHz} {FMCW} {Radar} with {Deep} {Learning} {Domain} {Adaptation}},
	volume = {9},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/9/12/2140},
	doi = {10.3390/electronics9122140},
	abstract = {With the recent development of small radars with high resolution, various human-computer interaction (HCI) applications using them have been developed. In particular, a method of applying a user's hand gesture recognition using a short-range radar to an electronic device is being actively studied. In general, the time delay and Doppler shift characteristics that occur when a transmitted signal that is reflected off an object returns are classified through deep learning to recognize the motion. However, the main obstacle in the commercialization of radar-based hand gesture recognition is that even for the same type of hand gesture, recognition accuracy is degraded due to a slight difference in movement for each individual user. To solve this problem, in this paper, the domain adaptation is applied to hand gesture recognition to minimize the differences among users' gesture information in the learning and the use stage. To verify the effectiveness of domain adaptation, a domain discriminator that cheats the classifier was applied to a deep learning network with a convolutional neural network (CNN) structure. Seven different hand gesture data were collected for 10 participants and used for learning, and the hand gestures of 10 users that were not included in the training data were input to confirm the recognition accuracy of an average of 98.8\%.},
	number = {12},
	journal = {Electronics},
	author = {Lee, Hyo Ryun and Park, Jihun and Suh, Young-Joo},
	year = {2020},
    pages = {1--24}
}


@inproceedings{Leem:2020b,
	title = {Remote {Authentication} {Using} an {Ultra}-{Wideband} {Radio} {Frequency} {Transceiver}},
	doi = {10.1109/CCNC46108.2020.9045438},
	abstract = {In this paper, we propose three new authentication methods using ultra-wideband (UWB) radio frequency (RF) signals. These mid-air-based authentication techniques are different from the conventional touch-based authentication approaches and provide a more convenient and safer user experience. In these methods, the impulse radio UWB transceiver receives the reflected RF signal corresponding to the movement pattern of the user, extracts the pattern information after noise removal, and converts it into image data suitable for use as classifier input through image signal processing. After that, it verifies whether the pattern is the authentication pattern of the user by employing a convolutional neural network and a one-class support vector machine. This report describes the three proposed authentication methods using the advantages of UWB RF signals, as well as their experimental verification. In the first authentication method, ShapeSec, authentication is performed by writing a simple shape pattern in the air. This technique is simple but very convenient. The second method, SignSec, correctly detects the handwriting of different users when signing and overcomes the security vulnerability of signing with the existing pens. Finally, BreatheSec is based on breathing patterns and takes advantage of the ability of a UWB RF transceiver to detect minute breath movements. The authentication pattern could not be imitated even after it was watched from the side, proving that the proposed method has excellent security characteristics. Experiments showed that each method has over 95\% accuracy.},
	booktitle = {2020 {IEEE} 17th {Annual} {Consumer} {Communications} {Networking} {Conference} ({CCNC})},
	author = {Leem, Seong Kyu and Khan, Faheem and Cho, Sung Ho},
	month = jan,
	year = {2020},
	note = {ISSN: 2331-9860},
	keywords = {authentication method, authentication pattern, authorisation, BreatheSec, breathing patterns, CNN, conventional touch-based authentication approaches, convolutional neural nets, convolutional neural network, image classification, image denoising, image signal processing, impulse radio UWB transceiver, IR-UWB, mid-air-based authentication techniques, movement pattern, noise removal, one-class support vector machine, pattern, pattern information extraction, Radio sensor, radio transceivers, reflected RF signal, remote authentication, security characteristics, ShapeSec, simple shape pattern, support vector machines, SVM, telecommunication computing, ultra wideband communication, ultra-wideband radio frequency signals, ultra-wideband radio frequency transceiver, user experience, UWB RF signals, UWB RF transceiver},
	pages = {1--8},
}


@article{Leem:2020a,
	title = {Detecting {Mid}-{Air} {Gestures} for {Digit} {Writing} {With} {Radio} {Sensors} and a {CNN}},
	volume = {69},
	issn = {1557-9662},
	doi = {10.1109/TIM.2019.2909249},
	abstract = {In this paper, we classify digits written in mid-air using hand gestures. Impulse radio ultrawideband (IR-UWB) radar sensors are used for data acquisition, with three radar sensors placed in a triangular geometry. Conventional radar-based gesture recognition methods use whole raw data matrices or a group of features for gesture classification using convolutional neural networks (CNNs) or other machine learning algorithms. However, if the training and testing data differ in distance, orientation, hand shape, hand size, or even gesture speed or the radar setup environment, these methods become less accurate. To develop a more robust gesture recognition method, we propose not using raw data for the CNN classifier, but instead employing the hand's mid-air trajectory for classification. The hand trajectory has a stereotypical shape for a given digit, regardless of the hand's orientation or speed, making its classification easy and robust. Our proposed method consists of three stages: signal preprocessing, hand motion localization, and tracking and transforming the trajectory data into an image to classify it using a CNN. Our proposed method outperforms conventional approaches because it is robust to changes in orientation, distance, and hand shape and size. Moreover, this method does not require building a huge training database of digits drawn by different users in different orientations; rather, we can use training databases already available in the image processing field. Overall, the proposed mid-air handwritten digit recognition system provides a user-friendly and accurate mid-air handwriting modality that does not place restrictions on users.},
	number = {4},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Leem, Seong Kyu and Khan, Faheem and Cho, Sung Ho},
	month = apr,
	year = {2020},
	keywords = {Clutter, CNN classifier, conventional radar-based gesture recognition methods, convolutional neural nets, Convolutional neural network (CNN), data acquisition, digit writing, feature extraction, gesture classification, gesture recognition, Gesture recognition, gesture speed, hand motion localization, hand shape, hand size, hand trajectory, handwritten character recognition, human computer interaction, human???computer interaction, image, image classification, image motion analysis, impulse radio ultrawideband (IR-UWB) radar, impulse radio ultrawideband radar sensors, IR-UWB, learning (artificial intelligence), localization, machine learning algorithms, mid-air gesture detection, mid-air handwriting, mid-air handwriting modality, mid-air handwritten digit recognition system, mid-air trajectory, object tracking, Radar imaging, radar setup environment, radio sensors, raw data matrices, robust gesture recognition method, sensor, Sensors, stereotypical shape, testing data, Training, Trajectory, trajectory data, triangular geometry, ultra wideband radar, wireless sensor networks},
	pages = {1066--1081},
}


@article{Leiva:2015,
    author = {Leiva, Luis A. and Mart\'{\i}n-Albo, Daniel and Plamondon, R\'{e}jean},
    title = {{Gestures \`{a} Go Go: Authoring Synthetic Human-Like Stroke Gestures Using the Kinematic Theory of Rapid Movements}},
    year = {2015},
    issue_date = {January 2016},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {7},
    number = {2},
    issn = {2157-6904},
    url = {https://doi.org/10.1145/2799648},
    doi = {10.1145/2799648},
    journal = {ACM Transactions on Intelligent Systems Technology},
    month = nov,
    articleno = {article 15},
    numpages = {29},
    keywords = {user interfaces, bootstrapping, unistrokes, gesture recognition, multistrokes, kinematics, multitouch, marks, rapid prototyping, strokes, Gesture synthesis, symbols},
}


@inproceedings{Leiva:2018,
    author = {Leiva, Luis A. and Mart\'{\i}n-Albo, Daniel and Vatavu, Radu-Daniel},
    title = {{GATO: Predicting Human Performance with Multistroke and Multitouch Gesture Input}},
    year = {2018},
    isbn = {9781450358989},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3229434.3229478},
    doi = {10.1145/3229434.3229478},
    booktitle = {Proceedings of the 20th ACM International Conference on Human-Computer Interaction with Mobile Devices and Services},
    articleno = {article 32},
    numpages = {11},
    keywords = {gesture input, touch gestures, stroke gestures, human performance, production time, kinematic theory},
    venue = {Barcelona, Spain},
    series = {MobileHCI ’18},
}


@inproceedings{Leiva:2020,
    author = {Leiva, Luis A. and Kljun, Matjaz and Sandor, Christian and Copic Pucihar, Klen},
    title = {{The Wearable Radar: Sensing Gestures Through Fabrics}},
    year = {2020},
    isbn = {9781450380522},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3406324.3410720},
    doi = {10.1145/3406324.3410720},
    abstract = {Recently, millimeter-wave radar-on-chip sensors such as Google Soli have become readily available in the mobile ecosystem. We envision such radar technology to be integrated into wearables to enable gesture-based interaction possibilities for users ‘on the go’, e.g. to control various devices such as phone, car infotainment system, etc. even when the sensor is occluded by some material such as fabrics. Towards achieving this vision, we developed a hybrid CNN+LSTM deep learning model, and conducted a systematic study investigating mid-air gesture recognition performance when the radar sensor was covered by three different fabrics (leather, wool, and cotton). We show that, when trained on no occluding material, the model performed worse than if trained with each of the three fabrics; however, this is only valid in the small data regime (N=20). When trained with large samples (N=200) on no occluding material, the model achieved remarkable performance also when the sensor was covered by each of the fabrics (95\% avg. accuracy, 99\% AUC). Our results show that sensing mid-air gestures through fabrics is both feasible and ready for practical applications, since it is not necessary to train a dedicated model for each type of fabric available in the market. We also contribute a repeatable procedure to systematically test mid-air gestures with radar technology, enabled by an experimental platform that we release with this paper.},
    booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},
    articleno = {17},
    numpages = {4},
    keywords = {Gestures, Radar, Soli, Wearables, Deep Learning, Fabrics},
    venue = {Oldenburg, Germany},
    series = {MobileHCI '20},
}


@inproceedings{Lewis:2009,
    author="Lewis, James R. and Sauro, Jeff",
    editor="Kurosu, Masaaki",
    title="The Factor Structure of the System Usability Scale",
    booktitle="Human Centered Design",
    year="2009",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="94--103",
    abstract="Since its introduction in 1986, the 10-item System Usability Scale (SUS) has been assumed to be unidimensional. Factor analysis of two independent SUS data sets reveals that the SUS actually has two factors -- Usable (8 items) and Learnable (2 items -- specifically, Items 4 and 10). These new scales have reasonable reliability (coefficient alpha of .91 and .70, respectively). They correlate highly with the overall SUS (r = .985 and .784, respectively) and correlate significantly with one another (r = .664), but at a low enough level to use as separate scales. A sensitivity analysis using data from 19 tests had a significant Test by Scale interaction, providing additional evidence of the differential utility of the new scales. Practitioners can continue to use the current SUS as is, but, at no extra cost, can also take advantage of these new scales to extract additional information from their SUS data. The data support the use of ``awkward'' rather than ``cumbersome'' in Item 8.",
    isbn="978-3-642-02806-9",
}


@article{Li:2009,
	title = {{Human motion recognition using ultra-wideband radar and cameras on mobile robot}},
	volume = {15},
	issn = {1006-4982, 1995-8196},
	url = {http://link.springer.com/10.1007/s12209-009-0067-5},
	doi = {10.1007/s12209-009-0067-5},
	language = {en},
	number = {5},
	urldate = {2020-12-21},
	journal = {Transactions of Tianjin University},
	author = {Li, Tuanjie and Ge, Mengmeng},
	month = oct,
	year = {2009},
	pages = {381--387},
}


@inproceedings{Li:2010,
     author = {Li, Yang},
     title = {{Protractor: A Fast and Accurate Gesture Recognizer}},
     booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
     series = {CHI '10},
     year = {2010},
     isbn = {978-1-60558-929-9},
     venue = {Atlanta, Georgia, USA},
     pages = {2169--2172},
     numpages = {4},
     url = {http://doi.acm.org/10.1145/1753326.1753654},
     doi = {10.1145/1753326.1753654},
     acmid = {1753654},
     publisher = {ACM},
     address = {New York, NY, USA},
     keywords = {gesture recognition, gesture-based interaction, nearest neighbor approach, template-based approach},
} 


@inproceedings{Li:2017a,
	title = {Sparsity-based dynamic hand gesture recognition using micro-{Doppler} signatures},
	doi = {10.1109/RADAR.2017.7944336},
	abstract = {In this paper, a sparsity-driven method of micro-Doppler analysis is proposed for dynamic hand gesture recognition with radar sensor. The sparse representation of the radar signal in the time-frequency domain is achieved through the Gabor dictionary, and then the micro-Doppler features are extracted by using the orthogonal matching pursuit (OMP) algorithm and fed into classifiers for dynamic hand gesture recognition. The proposed method is validated with real data measured with a K-band radar. Experiment results show that the proposed method outperforms the principal component analysis (PCA) algorithm, with the recognition accuracy higher than 90\%.},
	booktitle = {2017 {IEEE} {Radar} {Conference} ({RadarConf})},
	author = {Li, Gang and Zhang, Rui and Ritchie, Matthew and Griffiths, Hugh},
	month = may,
	year = {2017},
	note = {ISSN: 2375-5318},
	keywords = {Doppler radar, dynamic hand gesture recognition, feature extraction, Feature extraction, Gabor dictionary, gesture recognition, Gesture recognition, image classification, iterative methods, K-band radar, Matching pursuit algorithms, micro-Doppler analysis, micro-Doppler feature extraction, micro-Doppler signatures, OMP algorithm, orthogonal matching pursuit algorithm, palmprint recognition, PCA algorithm, principal component analysis, principal component analysis algorithm, Radar, radar receivers, radar sensor, radar signal sparse representation, sparse signal representation, sparsity-based dynamic hand gesture recognition, Thumb, time-frequency analysis, Time-frequency analysis, time-frequency domain},
	pages = {0928--0931},
}


@article{Li:2017b,
    author = {Li, Tianxing and Xiong, Xi and Xie, Yifei and Hito, George and Yang, Xing-Dong and Zhou, Xia},
    title = {{Reconstructing Hand Poses Using Visible Light}},
    year = {2017},
    issue_date = {September 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {1},
    number = {3},
    url = {https://doi.org/10.1145/3130937},
    doi = {10.1145/3130937},
    abstract = {Free-hand gestural input is essential for emerging user interactions. We present Aili, a table lamp reconstructing a 3D hand skeleton in real time, requiring neither cameras nor on-body sensing devices. Aili consists of an LED panel in a lampshade and a few low-cost photodiodes embedded in the lamp base. To reconstruct a hand skeleton, Aili combines 2D binary blockage maps from vantage points of different photodiodes, which describe whether a hand blocks light rays from individual LEDs to all photodiodes. Empowering a table lamp with sensing capability, Aili can be seamlessly integrated into the existing environment. Relying on such low-level cues, Aili entails lightweight computation and is inherently privacy-preserving. We build and evaluate an Aili prototype. Results show that Aili’s algorithm reconstructs a hand pose within 7.2 ms on average, with 10.2° mean angular deviation and 2.5-mm mean translation deviation in comparison to Leap Motion. We also conduct user studies to examine the privacy issues of Leap Motion and solicit feedback on Aili’s privacy protection. We conclude by demonstrating various interaction applications Aili enables.},
    journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
    month = sep,
    articleno = {71},
    numpages = {20},
    keywords = {Gestural input, 3D hand reconstruction, visible light sensing},
}


@article{Li:2017c,
    author     = {Li, Qiming and Huang, Chen and Lv, Shengqing and Li, Zeyu and Chen, Yimin and Ma, Lizhuang},
    journal    = {J. Med. Syst.},
    title      = {{An Human-Computer Interactive Augmented Reality System for Coronary Artery Diagnosis Planning and Training}},
    year       = {2017},
    issn       = {0148-5598},
    month      = oct,
    number     = {10},
    pages      = {1–11},
    volume     = {41},
    address    = {USA},
    doi        = {10.1007/s10916-017-0805-5},
    issue_date = {October 2017},
    keywords   = {Trainning, Coronary artery, Augmented reality, Diagnosis and preoperative planning, Human computer interaction},
    numpages   = {11},
    publisher  = {Plenum Press},
    url        = {https://doi.org/10.1007/s10916-017-0805-5},
}


@article{Li:2018a,
	title = {Sparsity-{Driven} {Micro}-{Doppler} {Feature} {Extraction} for {Dynamic} {Hand} {Gesture} {Recognition}},
	volume = {54},
	issn = {1557-9603},
	doi = {10.1109/TAES.2017.2761229},
	abstract = {In this paper, a sparsity-driven method of micro-Doppler analysis is proposed for dynamic hand gesture recognition with radar sensors. First, sparse representations of the echoes reflected from dynamic hand gestures are achieved through the Gaussian-windowed Fourier dictionary. Second, the micro-Doppler features of dynamic hand gestures are extracted using the orthogonal matching pursuit algorithm. Finally, the nearest neighbor classifier is combined with the modified Hausdorff distance to recognize dynamic hand gestures based on the sparse micro-Doppler features. Experiments with real radar data show that the recognition accuracy produced by the proposed method exceeds 96\% under moderate noise, and the proposed method outperforms the approaches based on principal component analysis and deep convolutional neural network with small training dataset.},
	number = {2},
	journal = {IEEE Transactions on Aerospace and Electronic Systems},
	author = {Li, Gang and Zhang, Rui and Ritchie, Matthew and Griffiths, Hugh},
	month = apr,
	year = {2018},
	keywords = {Aerodynamics, deep convolutional neural network, Doppler radar, dynamic hand gesture recognition, feature extraction, Feature extraction, Fourier transforms, Gaussian-windowed Fourier dictionary, gesture recognition, Gesture recognition, iterative methods, micro-Doppler analysis, micro-Doppler feature extraction, modified Hausdorff distance, nearest neighbor classifier, nearest neighbour methods, neural nets, orthogonal matching pursuit algorithm, principal component analysis, Radar, radar sensors, radar target recognition, sparse micro-Doppler features, sparse signal representation, sparsity-driven method, Thumb, Time-frequency analysis},
	pages = {655--665},
}


@article{Li:2018b,
	title = {Effect of sparsity-aware time–frequency analysis on dynamic hand gesture classification with radar micro-{Doppler} signatures},
	volume = {12},
	issn = {1751-8792},
	doi = {10.1049/iet-rsn.2017.0570},
	abstract = {Dynamic hand gesture recognition is of great importance in human-computer interaction. In this study, the authors investigate the effect of sparsity-driven time-frequency analysis on hand gesture classification. The time-frequency spectrogram is first obtained by sparsity-driven time-frequency analysis. Then three empirical micro-Doppler features are extracted from the time-frequency spectrogram and a support vector machine is used to classify six kinds of dynamic hand gestures. The experimental results on measured data demonstrate that, compared to traditional time-frequency analysis techniques, sparsity-driven time-frequency analysis provides improved accuracy and robustness in dynamic hand gesture classification.},
	number = {8},
	journal = {IET Radar, Sonar Navigation},
	author = {Li, Gang and Zhang, Shimeng and Fioranelli, Francesco and Griffiths, Hugh},
	year = {2018},
	keywords = {Doppler radar, dynamic hand gesture classification, dynamic hand gesture recognition, empirical microDoppler feature, feature extraction, gesture recognition, human-computer interaction, image classification, radar computing, radar imaging, radar microDoppler signature, sparse-aware time-frequency analysis, sparsity-driven time-frequency analysis, support vector machine, support vector machines, time-frequency analysis, time-frequency spectrogram extraction},
	pages = {815--820},
}


@inproceedings{Li:2019,
    author    = {Li, Feifei and Li, Yujun and Du, Baozhen and Xu, Hongji and Xiong, Hailiang and Chen, Min},
    booktitle = {Proceedings of the 2019 4th International Conference on Mathematics and Artificial Intelligence},
    title     = {{A Gesture Interaction System Based on Improved Finger Feature and WE-KNN}},
    year      = {2019},
    address   = {New York, NY, USA},
    pages     = {39–43},
    publisher = {Association for Computing Machinery},
    series    = {ICMAI 2019},
    doi       = {10.1145/3325730.3325759},
    isbn      = {9781450362580},
    keywords  = {Virtual interaction, Gesture recognition, K-nearest neighbor classifier based on entropy-weight allocation, Feature extraction},
    venue  = {Chegndu, China},
    numpages  = {5},
    url       = {https://doi.org/10.1145/3325730.3325759},
}


@article{Li:2020b,
	title = {Intelligent {Electromagnetic} {Sensing} with {Learnable} {Data} {Acquisition} and {Processing}},
	volume = {1},
	issn = {2666-3899},
	url = {http://www.sciencedirect.com/science/article/pii/S2666389920300064},
	doi = {https://doi.org/10.1016/j.patter.2020.100006},
	abstract = {Summary Electromagnetic (EM) sensing is a widespread contactless examination technique with applications in areas such as health care and the internet of things. Most conventional sensing systems lack intelligence, which not only results in expensive hardware and complicated computational algorithms but also poses important challenges for real-time in situ sensing. To address this shortcoming, we propose the concept of intelligent sensing by designing a programmable metasurface for data-driven learnable data acquisition and integrating it into a data-driven learnable data-processing pipeline. Thereby, a measurement strategy can be learned jointly with a matching data post-processing scheme, optimally tailored to the specific sensing hardware, task, and scene, allowing us to perform high-quality imaging and high-accuracy recognition with a remarkably reduced number of measurements. We report the first experimental demonstration of “learned sensing” applied to microwave imaging and gesture recognition. Our results pave the way for learned EM sensing with low latency and computational burden.},
	number = {1},
	journal = {Patterns},
	author = {Li, Hao-Yang and Zhao, Han-Ting and Wei, Meng-Lin and Ruan, Heng-Xin and Shuang, Ya and Cui, Tie Jun and Hougne, Philipp del and Li, Lianlin},
	year = {2020},
	keywords = {artificial neural network, intelligent electromagnetic, programmable metamaterials},
	pages = {100006},
}


@article{Li:2020a,
	title = {Hierarchical {Sensor} {Fusion} for {Micro}-{Gesture} {Recognition} {With} {Pressure} {Sensor} {Array} and {Radar}},
	volume = {4},
	issn = {2469-7257},
	doi = {10.1109/JERM.2019.2949456},
	abstract = {This paper presents a hierarchical sensor fusion approach for human micro-gesture recognition by combining an Ultra Wide Band (UWB) Doppler radar and wearable pressure sensors. First, the wrist-worn pressure sensor array (PSA) and Doppler radar are used to respectively identify static and dynamic gestures through a Quadratic-kernel SVM (Support Vector Machine) classifier. Then, a robust wrapper method is applied on the features from both sensors to search the optimal combination. Subsequently, two hierarchical approaches where one sensor acts as “enhancer” of the other are explored. In the first case, scores from Doppler radar related to the confidence level of its classifier and the prediction label corresponding to the posterior probabilities are utilized to maximize the static hand gestures classification performance by hierarchical combination with PSA data. In the second case, the PSA acts as an “enhancer” for radar to improve the dynamic gesture recognition. In this regard, different weights of the “enhancer” sensor in the fusion process have been evaluated and compared in terms of classification accuracy. A realistic cross-validation method is chosen to test one unknown participant with the model trained by data from others, demonstrating that this hierarchical fusion approach for static and dynamic gestures yields approximately 15\% improvement in classification accuracy in the best cases.},
	number = {3},
	journal = {IEEE Journal of Electromagnetics, RF and Microwaves in Medicine and Biology},
	author = {Li, Haobo and Liang, Xiangpeng and Shrestha, Aman and Liu, Yuchi and Heidari, Hadi and Le Kernec, Julien and Fioranelli, Francesco},
	month = sep,
	year = {2020},
	keywords = {classification accuracy, Doppler effect, Doppler radar, dynamic gesture recognition, dynamic gestures, enhancer sensor, feature extraction, Feature extraction, fusion process, gesture classification, gesture recognition, hierarchical combination, hierarchical fusion approach, hierarchical sensor fusion approach, image classification, machine learning, microgesture recognition, Multimodal sensing, pressure sensors, Pressure sensors, PSA data, Quadratic-kernel SVM classifier, radar computing, robust wrapper method, sensor arrays, Sensor arrays, sensor fusion, static gestures, static hand gestures classification performance, Support Vector Machine, support vector machines, Support vector machines, Ultra Wide Band Doppler radar, ultra wideband radar, UWB Doppler radar, wearable pressure sensors, wrist-worn pressure sensor array},
	pages = {225--232},
}


@INPROCEEDINGS{Li:2022,
    author={Li, Mingxue and Cong, Yang and Liu, Yuyang and Sun, Gan},
    booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
    title={{Class-Incremental Gesture Recognition Learning with Out-of-Distribution Detection}}, 
    year={2022},
    volume={},
    number={},
    pages={1503-1508},
    keywords={Training;Human computer interaction;Memory management;Fitting;Gesture recognition;Medical services;Task analysis},
    doi={10.1109/IROS47612.2022.9981167},
}



@article{Liang:2017,
    author     = {Liang, Hui and Chang, Jian and Kazmi, Ismail K. and Zhang, Jian J. and Jiao, Peifeng},
    journal    = {Vis. Comput.},
    title      = {{Hand Gesture-Based Interactive Puppetry System to Assist Storytelling for Children}},
    year       = {2017},
    issn       = {0178-2789},
    month      = apr,
    number     = {4},
    pages      = {517–531},
    volume     = {33},
    address    = {Berlin, Heidelberg},
    doi        = {10.1007/s00371-016-1272-6},
    issue_date = {April 2017},
    keywords   = {Serious game, Virtual puppet, Hand gesture recognizing, Digital storytelling},
    numpages   = {15},
    publisher  = {Springer-Verlag},
    url        = {https://doi.org/10.1007/s00371-016-1272-6},
}


@inproceedings{Liang:2020,
	title = {Enhanced {Hand} {Gesture} {Recognition} using {Continuous} {Wave} {Interferometric} {Radar}},
	doi = {10.1109/RADAR42522.2020.9114807},
	abstract = {Recently, radar micro-Doppler signatures have been extensively utilized for hand gesture recognition. As reported by existing works, recognition accuracy of different hand gestures is heavily affected by the aspect angle. In general, the accuracy deteriorates significantly with the increasing aspect angle. To solve this problem, we propose to utilize interferometric radar for hand gesture recognition in this paper, which is capable of providing two-dimensional micro-motions information, referred to as radial and transversal micro-motions. We record data of 9 different hand gestures in 4 aspect angles, where three empirical features are extracted from both Doppler and interferometric spectrograms and fed into support vector machine classifier for recognition. The experimental results demonstrate that hand gesture recognition using interferometric radar, 1) enhances recognition accuracy, 2) exhibits robustness against aspect angle, 3) recognizes horizontally symmetric gestures, by providing transversal micro-motion information and increasing spatial resolution.},
	booktitle = {2020 {IEEE} {International} {Radar} {Conference} ({RADAR})},
	author = {Liang, Huaiyuan and Wang, Xiangrong and Greco, Maria S. and Gini, Fulvio},
	month = apr,
	year = {2020},
	note = {ISSN: 2640-7736},
	keywords = {aspect angles, continuous wave interferometric radar, Doppler radar, Doppler spectrograms, enhanced hand gesture recognition, feature extraction, gesture recognition, hand gesture recognition, interferometric radar, interferometric spectrograms, interferometric spectrum, micro-Doppler spectrum, micromotion information, radar computing, radar imaging, radar microDoppler signatures, radar target recognition, radial micromotions, recognition accuracy, spatial resolution, support vector machine classifier, support vector machines, SVM, symmetric gestures, transversal micromotions, two-dimensional micromotions information},
	pages = {226--231},
}


@article{Lien:2016,
	title = {{Soli: ubiquitous gesture sensing with millimeter wave radar}},
	volume = {35},
	issn = {0730-0301},
	shorttitle = {Soli},
	url = {https://doi.org/10.1145/2897824.2925953},
	doi = {10.1145/2897824.2925953},
	abstract = {This paper presents Soli, a new, robust, high-resolution, low-power, miniature gesture sensing technology for human-computer interaction based on millimeter-wave radar. We describe a new approach to developing a radar-based sensor optimized for human-computer interaction, building the sensor architecture from the ground up with the inclusion of radar design principles, high temporal resolution gesture tracking, a hardware abstraction layer (HAL), a solid-state radar chip and system architecture, interaction models and gesture vocabularies, and gesture recognition. We demonstrate that Soli can be used for robust gesture recognition and can track gestures with sub-millimeter accuracy, running at over 10,000 frames per second on embedded hardware.},
	number = {4},
	urldate = {2020-12-21},
	journal = {ACM Transactions on Graphics},
	author = {Lien, Jaime and Gillian, Nicholas and Karagozler, M. Emre and Amihood, Patrick and Schwesig, Carsten and Olson, Erik and Raja, Hakim and Poupyrev, Ivan},
	month = jul,
	year = {2016},
	keywords = {gestures, interaction, radar, RF, sensors},
	pages = {142:1--142:19},
}


@inproceedings{Lingrand:2006,
    author="Lingrand, Diane and Renevier, Philippe and Pinna-D{\'e}ry, Anne-Marie and Cremaschi, Xavier and Lion, Stevens and Rouel, Jean-Guilhem and Jeanne, David and Cuisinaud, Philippe and Soula, Julien",
    editor="Calvary, Ga{\"e}lle and Pribeanu, Costin and Santucci, Giuseppe and Vanderdonckt, Jean",
    title="GestAction3D: A Platform for Studying Displacements and Deformations of 3D Objects Using Hands",
    booktitle="Computer-Aided Design of User Interfaces V",
    year="2007",
    publisher="Springer Netherlands",
    address="Dordrecht",
    pages="101--110",
    abstract="We present a low-cost hand-based device coupled with a 3D motion recovery engine and 3D visualization. This platform aims at studying ergonomic 3D interactions in order to manipulate and deform 3D models by interacting with hands on 3D meshes. Deformations are done using different modes of interaction that we will detail in the paper. Finger extremities are attached to vertices, edges or facets. Switching from one mode to another or changing the point of view is done using gestures. The determination of the more adequate gestures is part of the work",
    isbn="978-1-4020-5820-2",
}


@article{Liu:2009,
    author = {Liu, Jiayang and Zhong, Lin and Wickramasuriya, Jehan and Vasudevan, Venu},
    title = {{UWave: Accelerometer-Based Personalized Gesture Recognition and Its Applications}},
    year = {2009},
    issue_date = {December 2009},
    publisher = {Elsevier Science Publishers B. V.},
    address = {NLD},
    volume = {5},
    number = {6},
    issn = {1574-1192},
    url = {https://doi.org/10.1016/j.pmcj.2009.07.007},
    doi = {10.1016/j.pmcj.2009.07.007},
    journal = {Pervasive Mob. Comput.},
    month = dec,
    pages = {657–675},
    numpages = {19},
    keywords = {Acceleration, Personalized gesture, Dynamic time warping, User authentication, Gesture recognition},
}


@inproceedings{Liu:2015,
    author = {Liu, Mingyu and Nancel, Mathieu and Vogel, Daniel},
    title = {Gunslinger: Subtle Arms-down Mid-air Interaction},
    year = {2015},
    isbn = {9781450337793},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2807442.2807489},
    doi = {10.1145/2807442.2807489},
    abstract = {We describe Gunslinger, a mid-air interaction technique using barehand postures and gestures. Unlike past work, we explore a relaxed arms-down position with both hands interacting at the sides of the body. It features "hand-cursor" feedback to communicate recognized hand posture, command mode and tracking quality; and a simple, but flexible hand posture recognizer. Although Gunslinger is suitable for many usage contexts, we focus on integrating mid-air gestures with large display touch input. We show how the Gunslinger form factor enables an interaction language that is equivalent, coherent, and compatible with large display touch input. A four-part study evaluates Midas Touch, posture recognition feedback, pointing and clicking, and general usability.},
    booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software \& Technology},
    pages = {63–71},
    numpages = {9},
    keywords = {wearable, touch, large displays, gestures, barehand},
    location = {Charlotte, NC, USA},
    series = {UIST '15},
}


@article{Liu:2019,
	title = {Spectrum-{Based} {Hand} {Gesture} {Recognition} {Using} {Millimeter}-{Wave} {Radar} {Parameter} {Measurements}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2923122},
	abstract = {Radar sensors offer several advantages over optical sensors in the gesture recognition for remote control of electronic devices. In this paper, we investigate the feasibility of human gesture recognition using the spectra of radar measurement parameters. With the combination of radar theory and classification methods, we found that the frequencies of different gestures' parameters could be utilized as features for gesture recognition. Six kinds of periodic dynamic gestures are designed to avoid the complexity of defining and extracting the start and end of the dynamic gesture. In addition to the frequency ratio, we also extracted some features related to motion range and detection coherence to eliminate the interferences brought by the unintended gestures. The decision tree classifier designed on the basis of experimental phenomena can guarantee effective classification between different gestures, and in general, the correct recognition rate of each gesture is higher than 90\%. Finally, we collected the position and the Doppler velocity information of hand for classification by a W-band millimeter wave radar in the experiment and verified the usability of the proposed method.},
	journal = {IEEE Access},
	author = {Liu, Changjiang and Li, Yuanhao and Ao, Dongyang and Tian, Haiyan},
	year = {2019},
	keywords = {correct recognition rate, decision tree, decision tree classifier, decision trees, Doppler radar, Doppler velocity information, dynamic gesture, electronic devices, feature extraction, Feature extraction, gesture recognition, Gesture recognition, human gesture recognition, image classification, Millimeter wave radar, millimeter-wave, millimeter-wave radar parameter measurements, millimetre wave measurement, millimetre wave radar, motion detection, motion range, optical sensors, periodic dynamic gestures, Radar antennas, radar imaging, Radar measurements, radar sensors, radar theory, remote control, spectrum-based hand gesture recognition, unintended gestures, W-band millimeter wave radar},
	pages = {79147--79158},
}


@incollection{Liu:2020a,
	address = {Cham},
	title = {Long-{Range} {Gesture} {Recognition} {Using} {Millimeter} {Wave} {Radar}},
	volume = {12398},
	isbn = {978-3-030-64242-6 978-3-030-64243-3},
	url = {http://link.springer.com/10.1007/978-3-030-64243-3_3},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Green, {Pervasive}, and {Cloud} {Computing}},
	publisher = {Springer International Publishing},
	author = {Liu, Yu and Wang, Yuheng and Liu, Haipeng and Zhou, Anfu and Liu, Jianhua and Yang, Ning},
	editor = {Yu, Zhiwen and Becker, Christian and Xing, Guoliang},
	year = {2020},
	doi = {10.1007/978-3-030-64243-3_3},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {30--44},
}


@article{Liu:2020b,
	title = {Real-time {Arm} {Gesture} {Recognition} in {Smart} {Home} {Scenarios} via {Millimeter} {Wave} {Sensing}},
	volume = {4},
	url = {https://doi.org/10.1145/3432235},
	doi = {10.1145/3432235},
	abstract = {"In air" gesture recognition using millimeter wave (mmWave) radar and its applications in natural human-computer-interaction for smart home has shown its potential. However, the state-of-the-art works still fall short in terms of limited gesture space, vulnerable to surrounding interference, and off-line recognition. In this paper, we propose mHomeGes, a real-time mmWave arm gesture recognition system for practical smart home-usage. To this end, we first distill arm gesture's position and dynamic variation, and then custom-design a lightweight convolution neural network to recognize fine-grained gestures. Next, we propose a user discovery method to focus on the target human gesture, thus eliminating the adverse impact of surrounding interference. Finally, we design a hidden Markov model-based voting mechanism to handle continuous gesture signals at run-time, which leads to continuous gesture recognition in real-time. We implement mHomeGes on a commodity mmWave radar and also perform a user study, which demonstrates that mHomeGes achieves high recognition accuracy above 95.30\% in real-time across various smart home scenarios, regardless of the impact of surrounding movements, concurrent gestures, human physiological conditions, and outer packing materials. Moreover, we have also publicly archived a mmWave gesture data-set collected during developing mHomeGes, which consists of about 22,000 instances from 25 persons and may have an independent value of facilitating future research.},
	number = {4},
	urldate = {2020-12-21},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Liu, Haipeng and Wang, Yuheng and Zhou, Anfu and He, Hanyue and Wang, Wei and Wang, Kunpeng and Pan, Peilin and Lu, Yixuan and Liu, Liang and Ma, Huadong},
	month = dec,
	year = {2020},
	keywords = {Gesture Recognition, Human-Computer Interaction, Millimeter Wave Sensing, Smart Home},
	pages = {140:1--140:28},
}


@inproceedings{Long:1999,
    author = {Long, Allan Christian and Landay, James A. and Rowe, Lawrence A.},
    title = {{Implications for a Gesture Design Tool}},
    year = {1999},
    isbn = {0201485591},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/302979.302985},
    doi = {10.1145/302979.302985},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {40–47},
    numpages = {8},
    keywords = {gesture, pen-based user interface, PDA, user study, UI design},
    venue = {Pittsburgh, Pennsylvania, USA},
    series = {CHI ’99},
}


@article{Lopera:2007, 
    author={Lopera, Olga and Slob, Evert C. and Milisavljevic, Nada and Lambot, S{\'{e}}bastien},  
    journal={IEEE Transactions on Geoscience and Remote Sensing}, 
    title={{Filtering Soil Surface and Antenna Effects From GPR Data to Enhance Landmine Detection}},   
    year={2007},  
    volume={45},  
    number={3},  
    pages={707-717}, 
    month= mar,
    doi={10.1109/TGRS.2006.888136},
    url={https://ieeexplore.ieee.org/document/4106057},
}


@inproceedings{Lopez:2015,
    author="L{\'o}pez, Gustavo and Quesada, Luis and Guerrero, Luis A.",
    editor="Cleland, Ian and Guerrero, Luis and Bravo, Jos{\'e}",
    title="A Gesture-Based Interaction Approach for Manipulating Augmented Objects Using Leap Motion",
    booktitle="Ambient Assisted Living. ICT-based Solutions in Real Life Situations",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="231--243",
    isbn="978-3-319-26410-3",
}


@inproceedings{Lou:2018,
	title = {Gesture-{Radar}: {Enabling} {Natural} {Human}-{Computer} {Interactions} with {Radar}-{Based} {Adaptive} and {Robust} {Arm} {Gesture} {Recognition}},
	doi = {10.1109/SMC.2018.00726},
	abstract = {Human behavior recognition is an effective way to realize natural human-computer interactions. Existing wireless sensing enabled gesture recognition technologies require a single person environment or an absolutely fixed position between the device and the user, which is not practical for daily use. In this paper, we present a non-contact radar-based gesture recognition system, named Gesture-Radar, which is able to capture arm gestures with low environmental dependence using a single Doppler radar. Our prototype design of Gesture-Radar is based on the dual channel Doppler information which contains specific Doppler shift and some other information reflected from the user while performing a certain gesture, and concretely we propose a two-stage classification method to identify arm gestures. Experimental result shows while in a single user environment, Gesture-Radar achieves up to 96.4\% average classification accuracy for recognizing 5 different kinds of gestures and can work effectively while the distance between the user and the radar is within 3 meters. We also demonstrate that Gesture-Radar can be well adapted to multi-person environments.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Lou, Xinye and Yu, Zhiwen and Wang, Zhu and Zhang, Kaijie and Guo, Bin},
	month = oct,
	year = {2018},
	note = {ISSN: 2577-1655},
	keywords = {behavioural sciences computing, Conferences, Cybernetics, Doppler radar, Doppler shift, dual channel Doppler information, Dual channel information, gesture recognition, Gesture recognition, gesture recognition technologies, Gesture-Radar, human behavior recognition, human computer interaction, Humancomputer Interaction, image classification, natural human-computer interactions, noncontact radar-based gesture recognition system, radar-based adaptive recognition, robust arm Gesture recognition, two-stage classification method, wireless sensing, Wireless sensing},
	pages = {4291--4297},
}


%===========================================================
% References starting by M
%===========================================================
 @article{Madani:2015,
  author    = {Madani, Tahir Mustafa and Tahir, Muhammad and Ziauddin, Sheikh and Raza, Syed and Ahmed, Mirza},
  title     = {{An accelerometer-based approach to evaluate 3D unistroke gestures}},
  journal   = {The International Arab Journal of Information Technology},
  volume    = {12},
  number    = {4},
  pages     = {389--394},
  year      = {2015},
  url       = {http://iajit.org/index.php?option=com\_content\&task=blogcategory\&id=99\&Itemid=376},
  timestamp = {Wed, 27 Mar 2019 13:18:47 +0100},
  biburl    = {https://dblp.org/rec/journals/iajit/MadaniTZRA15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Madapana:2019,
    author = {Madapana, Naveen and Gonzalez, Glebys and Taneja, Rahul and Rodgers, Richard and Zhang, Lingsong and Wachs, Juan P.},
    year = {2019},
    month = {07},
    pages = {},
    title = {{Preference Elicitation: Obtaining Gestural Guidelines for PACS in Neurosurgery}},
    volume = {130},
    journal = {International Journal of Medical Informatics},
    doi = {10.1016/j.ijmedinf.2019.07.013},
}


@inproceedings{Madapana:2021,
  author    = {Madapana, Naveen and Wachs, Juan P.},
  title     = {{ZF-SSE:} {A} Unified Sequential Semantic Encoder for Zero-Few-Shot Learning},
  booktitle = {Proceedings of the 16th {IEEE} International Conference on Automatic Face and Gesture Recognition},
  series    = {FG 2021},
  venue  = {Jodhpur, India},
  dates     = {December 15-18, 2021},
  pages     = {1--8},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/FG52635.2021.9667025},
  doi       = {10.1109/FG52635.2021.9667025},
  timestamp = {Tue, 18 Jan 2022 17:38:50 +0100},
  biburl    = {https://dblp.org/rec/conf/fgr/MadapanaW21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Madapana:2022,
  author    = {Madapana, Naveen and Wachs, Juan P.},
  title     = {{JSE:} Joint Semantic Encoder for zero-shot gesture learning},
  journal   = {Pattern Anal. Appl.},
  volume    = {25},
  number    = {3},
  pages     = {679--692},
  year      = {2022},
  url       = {https://doi.org/10.1007/s10044-021-00992-y},
  doi       = {10.1007/s10044-021-00992-y},
  timestamp = {Mon, 08 Aug 2022 21:24:01 +0200},
  biburl    = {https://dblp.org/rec/journals/paa/MadapanaW22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Maghoumi:2019,
    author="Maghoumi, Mehran and LaViola, Joseph J.",
    editor="Bebis, George and Boyle, Richard and Parvin, Bahram and Koracin, Darko and Ushizima, Daniela and Chai, Sek and Sueda, Shinjiro and Lin, Xin and Lu, Aidong and Thalmann, Daniel and Wang, Chaoli and Xu, Panpan",
    title="DeepGRU: Deep Gesture Recognition Utility",
    booktitle="Advances in Visual Computing",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="16--31",
    abstract="We propose DeepGRU, a novel end-to-end deep network model informed by recent developments in deep learning for gesture and action recognition, that is streamlined and device-agnostic. DeepGRU, which uses only raw skeleton, pose or vector data is quickly understood, implemented, and trained, and yet achieves state-of-the-art results on challenging datasets. At the heart of our method lies a set of stacked gated recurrent units (GRU), two fully-connected layers and a novel global attention model. We evaluate our method on seven publicly available datasets, containing various number of samples and spanning over a broad range of interactions (full-body, multi-actor, hand gestures, etc.). In all but one case we outperform the state-of-the-art pose-based methods. For instance, we achieve a recognition accuracy of 84.9{\%} and 92.3{\%} on cross-subject and cross-view tests of the NTURGB+D dataset respectively, and also 100{\%} recognition accuracy on the UT-Kinect dataset. We show that even in the absence of powerful hardware, or a large amount of training data, and with as little as four samples per class, DeepGRU can be trained in under 10min while beating traditional methods specifically designed for small training sets, making it an enticing choice for rapid application prototyping and development.",
    isbn="978-3-030-33720-9",
}


@inproceedings{Maghoumi:2021,
  author    = {Maghoumi, Mehran and Taranta II, Eugene M. and LaViola, Joseph J.},
  editor    = {Hammond, Tracy and Verbert, Katrien and Parra, Dennis and Knijnenburg, Bart P. and O'Donovan, John and Teale, Paul},
  title     = {{DeepNAG: Deep Non-Adversarial Gesture Generation}},
  booktitle = {Proc. of 26th International Conference on Intelligent User Interfaces},
  venue = {College Station, TX, USA},
  dates={April 13-17, 2021},
  series={{IUI} '21},
  pages     = {213--223},
  publisher = {{ACM}},
  year      = {2021},
  url       = {https://doi.org/10.1145/3397481.3450675},
  doi       = {10.1145/3397481.3450675},
  timestamp = {Tue, 11 May 2021 09:15:59 +0200},
  biburl    = {https://dblp.org/rec/conf/iui/MaghoumiTL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Magrofuoco:2019,
    author    = {Magrofuoco, Nathan and P{\'{e}}rez{-}Medina, Jorge Luis and Roselli, Paolo and Vanderdonckt, Jean and Villarreal, Santiago},
    title     = {{Eliciting Contact-Based and Contactless Gestures With Radar-Based Sensors}},
    journal   = {{IEEE} Access},
    volume    = {7},
    pages     = {176982--176997},
    year      = {2019},
    url       = {https://doi.org/10.1109/ACCESS.2019.2951349},
    doi       = {10.1109/ACCESS.2019.2951349},
    timestamp = {Tue, 29 Dec 2020 18:18:47 +0100},
    biburl    = {https://dblp.org/rec/journals/access/MagrofuocoPRVV19.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Magrofuoco:2019a,
    author = {Magrofuoco, Nathan and Roselli, Paolo and Vanderdonckt, Jean and P\'{e}rez-Medina, Jorge Luis and Vatavu, Radu-Daniel},
    title = {{GestMan: A Cloud-Based Tool for Stroke-Gesture Datasets}},
    year = {2019},
    isbn = {9781450367455},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3319499.3328227},
    doi = {10.1145/3319499.3328227},
    abstract = {We introduce GestMan, a cloud-based GESTure MANagement tool to support the acquisition, design, and management of stroke-gesture datasets for interactive applications. GestMan stores stroke-gestures at multiple levels of representation, from individual samples to classes, clusters, and vocabularies and enables practitioners to process, analyze, classify, compile, and reconfigure sets of gesture commands according to the specific requirements of their applications, prototypes, and interactive systems. Our online tool enables acquisition of 2-D stroke-gestures via a HTML5-based user interface as well as 3-D touch+air and webcam-based gestures via dedicated mappers. GestMan implements five software quality characteristics of the ISO-25010 standard and employs a new mathematical formalization of stroke-gestures as vectors to support efficient computation of various gesture features.},
    booktitle = {Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
    articleno = {7},
    numpages = {6},
    keywords = {stroke-gestures, cloud computing, isoparameterization, gesture sets, gesture data management, tool, isometricity, isochronicity},
    venue = {Valencia, Spain},
    series = {EICS '19},
}


@article{Magrofuoco:2021,
    author = {Magrofuoco, Nathan and Roselli, Paolo and Vanderdonckt, Jean},
    title = {{Two-Dimensional Stroke Gesture Recognition: A Survey}},
    year = {2021},
    issue_date = {September 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {7},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3465400},
    doi = {10.1145/3465400},
    abstract = {The expansion of touch-sensitive technologies, ranging from smartwatches to wall screens, triggered a wider use of gesture-based user interfaces and encouraged researchers to invent recognizers that are fast and accurate for end-users while being simple enough for practitioners. Since the pioneering work on two-dimensional (2D) stroke gesture recognition based on feature extraction and classification, numerous approaches and techniques have been introduced to classify uni- and multi-stroke gestures, satisfying various properties of articulation-, rotation-, scale-, and translation-invariance. As the domain abounds in different recognizers, it becomes difficult for the practitioner to choose the right recognizer, depending on the application and for the researcher to understand the state-of-the-art. To address these needs, a targeted literature review identified 16 significant 2D stroke gesture recognizers that were submitted to a descriptive analysis discussing their algorithm, performance, and properties, and a comparative analysis discussing their similarities and differences. Finally, some opportunities for expanding 2D stroke gesture recognition are drawn from these analyses.},
    journal = {ACM Comput. Surv.},
    month = jul,
    articleno = {155},
    numpages = {36},
    keywords = {Gesture-based interfaces, stroke gestures, gesture recognition, touch gestures},
}


@article{Magrofuoco:2022:ARST,
    author    = {Magrofuoco, Nathan and Roselli, Paolo and Vanderdonckt, Jean},
    title     = {{\(\mathrm{\mu}\)}V: An Articulation, Rotation, Scaling, and Translation
               Invariant {(ARST)} Multi-stroke Gesture Recognizer},
    journal   = {Proc. {ACM} Hum. Comput. Interact.},
    volume    = {6},
    number    = {{EICS}},
    pages     = {150:1--150:25},
    year      = {2022},
    url       = {https://doi.org/10.1145/3532200},
    doi       = {10.1145/3532200},
    timestamp = {Tue, 21 Mar 2023 21:04:30 +0100},
    biburl    = {https://dblp.org/rec/journals/pacmhci/MagrofuocoRV22.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Magrofuoco:2022:CSUR,
    author    = {Magrofuoco, Nathan and Roselli, Paolo and Vanderdonckt, Jean},
    title     = {{Two-dimensional Stroke Gesture Recognition: A Survey}},
    journal   = {{ACM} Computing Surveys},
    volume    = {54},
    number    = {7},
    pages     = {155:1--155:36},
    year      = {2022},
    url       = {https://doi.org/10.1145/3465400},
    doi       = {10.1145/3465400},
    timestamp = {Mon, 27 Sep 2021 08:17:06 +0200},
    biburl    = {https://dblp.org/rec/journals/csur/MagrofuocoRV22.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Magrofuoco:2022:EICS,
    author    = {Magrofuoco, Nathan and Roselli, Paolo and Vanderdonckt, Jean},
    title     = {{µV: An Articulation, Rotation, Scaling, and Translation Invariant (ARST) Multi-stroke Gesture Recognizer}},
    journal   = {Proc. {ACM} Human-Computer Interaction},
    volume    = {6},
    number    = {{EICS}},
    pages     = {1--25},
    year      = {2022},
    url       = {https://doi.org/10.1145/3532200},
    doi       = {10.1145/3532200},
}


@article{Mahbub:2013,
    title = {{A template matching approach of one-shot-learning gesture recognition}},
    journal = {Pattern Recognition Letters},
    volume = {34},
    number = {15},
    pages = {1780-1788},
    year = {2013},
    issn = {0167-8655},
    doi = {https://doi.org/10.1016/j.patrec.2012.09.014},
    url = {https://www.sciencedirect.com/science/article/pii/S0167865512002991},
    author = {Mahbub, Upal and Imtiaz, Hafiz and Roy, Tonmoy and Rahman, Shafiur and Rahman Ahad, Atiqur},
    keywords = {Gesture recognition, Depth image, Motion history image, 2D Fourier transform},
    abstract = {This paper proposes a novel approach for gesture recognition from motion depth images based on template matching. Gestures can be represented with image templates, which in turn can be used to compare and match gestures. The proposed method uses a single example of an action as a query to find similar matches and thus termed one-shot-learning gesture recognition. It does not require prior knowledge about actions, foreground/background segmentation, or any motion estimation or tracking. The proposed method makes a novel approach to separate different gestures from a single video. Moreover, this method is based on the computation of space–time descriptors from the query video which measures the likeness of a gesture in a lexicon. These descriptor extraction methods include the standard deviation of the depth images of a gesture as well as the motion history image. Furthermore, two dimensional discrete Fourier transform is employed to reduce the effect of camera shift. The comparison is done based on correlation coefficient of the image templates and an intelligent classifier is proposed to ensure better recognition accuracy. Extensive experimentation is done on a very complicated and diversified dataset to establish the effectiveness of employing the proposed methods.},
}


@article{Mahmudi:2016,
    title = "Artist-oriented 3D character posing from 2D strokes",
    journal = "Computers \& Graphics",
    volume = "57",
    pages = "81 - 91",
    year = "2016",
    issn = "0097-8493",
    doi = "https://doi.org/10.1016/j.cag.2016.03.008",
    url = "http://www.sciencedirect.com/science/article/pii/S0097849316300218",
    author = "Mahmudi, Mentar and Harish, Pawan and Le Callennec, Benoît and Boulic, Ronan",
    keywords = "Artist-oriented interface, Sketch-based system, 3D character posing",
    abstract = "In this paper we present an intuitive tool suitable for 2D artists using touch-enabled pen tablets. An artist-oriented tool should be easy-to-use, real-time, versatile, and locally refinable. Our approach uses an interactive system for 3D character posing from 2D strokes. We employ a closed-form solution for the 2D strokes to 3D skeleton registration problem. We first construct an intermediate 2D stroke representation by extracting local features using meaningful heuristics. Then, we match 2D stroke segments to 3D bones. Finally, 3D bones are carefully realigned with the matched 2D stroke segments while enforcing important constraints such as bone rigidity and depth. Our technique is real-time and has a linear time complexity. It is versatile, as it works with any type of 2D stroke and 3D skeleton input. Finally, thanks to its coarse-to-fine design, it allows users to perform local refinements and thus keep full control over the final results. We demonstrate that our system is suitable for 2D artists using touch-enabled pen tablets by posing 3D characters with heterogeneous topologies (bipeds, quadrupeds, hands) in real-time.",
}


@inproceedings{Malysa:2016,
	title = {Hidden {Markov} model-based gesture recognition with {FMCW} radar},
	doi = {10.1109/GlobalSIP.2016.7905995},
	abstract = {In this paper we present experimental results for the development of a gesture recognition system using a 77 GHz FMCW radar system. We measure the micro-Doppler signature of a gesturing hand to construct an energy distribution in velocity space over time. A gesturing hand is fundamentally a dynamical system with unobservable “state” (i.e. the name of the gesture) which determines the sequence of associated observable velocity-energy distributions, so a Hidden Markov Model is used to for gesture recognition, a more tailored approach than the SVM classifiers used in previous work. We also describe a method for reducing the length of our feature vectors by a factor of 12 without hurting the recognition performance, by reparameterizing them in terms of a sum of Gaussians.},
	booktitle = {2016 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing} ({GlobalSIP})},
	author = {Malysa, Greg and Wang, Dan and Netsch, Lorin and Ali, Murtaza},
	month = dec,
	year = {2016},
	keywords = {associated observable velocity-energy distribution, Chirp, CW radar, Feature extraction, FM radar, FMCW radar system, Gaussian sum, gesture recognition, Gesture recognition, gesturing hand, hidden Markov model-based gesture recognition, hidden Markov models, Hidden Markov models, microDoppler signature, millimeter wave radar, radar computing, radar imaging, Radar imaging, support vector machines, SVM classifiers, Training, unobservable state},
	pages = {1017--1021},
}


@inproceedings{Mapari:2016,
  author    = {Mapari, Rajesh B. and Kharat, Govind},
  booktitle = {Proc. of the Second International Conference on Information and Communication Technology for Competitive Strategies},
  title     = {{American Static Signs Recognition Using Leap Motion Sensor}},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {ICTCS '16},
  articleno = {67},
  doi       = {10.1145/2905055.2905125},
  isbn      = {9781450339629},
  keywords  = {Leap Motion Sensor, MLP, ASL},
  venue  = {Udaipur, India},
  numpages  = {5},
  url       = {https://doi.org/10.1145/2905055.2905125},
}


@article{Marin:2014,
    author = {Marin, Giulio and Dominio, Fabio and Zanuttigh, Pietro},
    title = {{Hand gesture recognition with Leap Motion and Kinect devices}},
    year = {2014},
    pages = {1565-1569},
    journal = {2014 IEEE International Conference on Image Processing, ICIP 2014},
    doi = {10.1109/ICIP.2014.7025313},
}


@article{Marin:2016,
    author     = {Marin, Giulio and Dominio, Fabio and Zanuttigh, Pietro},
    journal    = {Multimedia Tools Appl.},
    title      = {{Hand Gesture Recognition with Jointly Calibrated Leap Motion and Depth Sensor}},
    year       = {2016},
    issn       = {1380-7501},
    month      = nov,
    number     = {22},
    pages      = {14991–15015},
    volume     = {75},
    address    = {USA},
    doi        = {10.1007/s11042-015-2451-6},
    issue_date = {November 2016},
    keywords   = {Calibration, Kinect, Gesture recognition, SVM, Depth, Leap Motion},
    numpages   = {25},
    publisher  = {Kluwer Academic Publishers},
    url        = {https://doi.org/10.1007/s11042-015-2451-6},
}


@article{Marquardt:1963,
    author = {Marquardt, Donald W.},
    title = {{An Algorithm for Least-Squares Estimation of Nonlinear Parameters}},
    journal = {Journal of the Society for Industrial and Applied Mathematics},
    volume = {11},
    number = {2},
    pages = {431-441},
    year = {1963},
    doi = {10.1137/0111030},
    URL = {https://doi.org/10.1137/0111030},
    eprint = {https://doi.org/10.1137/0111030},
}


@inproceedings{Mcintosh:2017,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '17},
	title = {{DeskWave}: {Desktop} {Interactions} using {Low}-cost {Microwave} {Doppler} {Arrays}},
	isbn = {978-1-4503-4656-6},
	shorttitle = {{DeskWave}},
	url = {https://doi.org/10.1145/3027063.3053152},
	doi = {10.1145/3027063.3053152},
	abstract = {Microwaves are a type of electromagnetic radiation that can pass through a variety of commonly found materials but partially reflect off human bodies. Microwaves are non-ionizing and at controlled levels do not pose a danger. A wave that is capable of passing through materials and image humans could have useful applications in human-computer-interaction. However, only recently the full potential of microwaves for interactive devices has begun to be explored. Here, we present a scalable, low-cost system using an array of off-the-shelf microwave Doppler sensors and explore its potential for tabletop interactions. The arrays are installed beneath a desk, making it an ubiquitous device that enables a wide range of interactions such as 3D hand tracking, gesture recognition and different forms of tangible interaction. Given the low cost and availability of these sensors, we expect that this work will stimulate future interactive devices that employ microwave sensors.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2017 {CHI} {Conference} {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {McIntosh, Jess and Fraser, Mike and Worgan, Paul and Marzo, Asier},
	month = may,
	year = {2017},
	keywords = {3D tracking, gesture recognition, microwave sensors, tabletops, wireless power transfer},
	pages = {1885--1892},
}


@article{McNeill:1995,
    author = {McNeill, David},
    year = {1995},
    title = {{Hand and Mind: What Gestures Reveal About Thought}},
    volume = {27},
    journal = {The University of Chicago Press},
    doi = {10.2307/1576015},
}


@article{Mendez:2018,
    author     = {M\'{e}ndez, Roi and Flores, Juli\'{a}n and Castell\'{o}, Enrique and Viqueira, Jos\'{e} Ram\'{o}n},
    journal    = {Multimedia Tools Appl.},
    title      = {{New Distributed Virtual TV Set Architecture for a Synergistic Operation of Sensors and Improved Interaction between Real and Virtual Worlds}},
    year       = {2018},
    issn       = {1380-7501},
    month      = aug,
    number     = {15},
    pages      = {18999–19025},
    volume     = {77},
    address    = {USA},
    doi        = {10.1007/s11042-017-5353-y},
    issue_date = {August 2018},
    keywords   = {Human computer interaction, Virtual reality, TV broadcasting, Virtual TV sets, TV},
    numpages   = {27},
    publisher  = {Kluwer Academic Publishers},
    url        = {https://doi.org/10.1007/s11042-017-5353-y},
}


@inproceedings{Mezari:2017,
    author = {Mezari, Antigoni and Maglogiannis, Ilias},
    title = {{Gesture Recognition Using Symbolic Aggregate Approximation and Dynamic Time Warping on Motion Data}},
    year = {2017},
    isbn = {9781450363631},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3154862.3154927},
    doi = {10.1145/3154862.3154927},
    booktitle = {Proceedings of the 11th EAI International Conference on Pervasive Computing Technologies for Healthcare},
    pages = {342–347},
    numpages = {6},
    keywords = {smart watch, Android, DTW, gesture recognition, accelerometry motion sensor, SAX, pebble},
    venue = {Barcelona, Spain},
    series = {PervasiveHealth ’17},
}


@article{Michalski:1990,
   Author = {Michalski, Krzysztof A. and Zheng, Dalian},
   Title = {{Electromagnetic scattering and radiation by subsurfaces of arbitrary shape in layered media: Parts I and II}},
   Journal = {IEEE Transactions on Antennas and Propagation},
   Volume = {38},
   Pages = {335-352},
   Year = {1990},
}


@article{Miller:2020,
	title = {{RadSense}: {Enabling} one hand and no hands interaction for sterile manipulation of medical images using {Doppler} radar},
	volume = {15},
	issn = {2352-6483},
	url = {http://www.sciencedirect.com/science/article/pii/S2352648319300534},
	doi = {https://doi.org/10.1016/j.smhl.2019.100089},
	abstract = {In this paper, we show how surgeons can interact with medical images using finger and hand gestures in two situations: one hand-free and no hands-free interaction. We explain how interaction with only one hand or a couple of fingers is beneficial and can help surgeons have continuous interaction, without the need to release their tools and leave the operating table, saving valuable patient time. To this end, we present RadSense, an end-to-end and unobtrusive system that uses Doppler radar-sensing to recognize hand and finger gestures when either one or both hands are busy. Our system permits the following important capabilities: (1) touch-less input for sterile interaction with connected health applications, (2) hand and finger gesture recognition when either one or both hands are busy holding tools, extending multitasking capabilities for health professionals, and (3) mobile and networked, allowing for custom wearable and non-wearable configurations. We evaluated our system in a simulated operating room to manipulate preoperative images using four gestures: circle, double tap, swipe, and finger click. We collected data from five subjects and trained a K-Nearest-Neighbor multi-class classifier using 15-fold cross validation, achieving a 94.5\% precision for gesture classification. We conclude that our system performs with high accuracy and is useful in cases where only one hand or a few fingers are free to interact when the hands are busy.},
	journal = {Smart Health},
	author = {Miller, Elishiah and Li, Zheng and Mentis, Helena and Park, Adrian and Zhu, Ting and Banerjee, Nilanjan},
	year = {2020},
	keywords = {Busy hand interaction, Gesture recognition, Healthcare, Human centered computing, Wearable devices},
	pages = {100089},
}


@article{Minet:2010,  
    author={Minet, Julien and Lambot, S{\'{e}}bastien and Slob, Evert C. and Vanclooster, Marnik},  
    journal={IEEE Transactions on Geoscience and Remote Sensing},   
    title={{Soil Surface Water Content Estimation by Full-Waveform GPR Signal Inversion in the Presence of Thin Layers}},   
    year={2010},  
    volume={48},  
    number={3},  
    pages={1138-1150},  
    doi={10.1109/TGRS.2009.2031907},
}


@phdthesis{Moinnet:2022,
    title = {{Composition of strokes gestures into system instructions}},
    author = {Moinnet, Adrien and Gosselin, Benoît},
    abstract = {For a long time, the main technique for executing a command has been point and click. However, due to the increasing need to execute commands quickly, shortcuts are becoming more and more popular, as is the case with point and click. Many studies have improved the shortcut system by developing 2D shortcuts. Unfortunately, keyboard shortcuts and 2D shortcuts involve a one-to-one gesture-command mapping. This leads to many memorability problems which are mainly because it is necessary to assign a new gesture to each new command and also, after a certain number of shortcuts, some gestures become more and more abstract. To address this issue, we decided to explore the concept of combining 2D gestures when interacting in smart environments. To achieve this, we have created an application allowing to realize shortcuts using macro-commands or a combination of 2D gestures which are congruent and hierarchic. This application will aim to visualize interactions with connected objects in a smart environment named EMERITI. To better understand the EMERITI environment, we will detail its grammatical structure, its functioning, and its advantages.},
    Keywords = {EMERITI , Instructions , Strokes Gestures , QuantumLeap , Recognizer , $P+ , Combination of gestures , 2D shortcut , Smart environment},
    language = {Anglais},
    year = {2022},
    url = {http://hdl.handle.net/2078.1/thesis:37872},
    school = {UCL - Ecole polytechnique de Louvain},
}


@inproceedings{Molchanov:2015,
	title = {{Multi-sensor system for driver's hand-gesture recognition}},
	volume = {1},
	doi = {10.1109/FG.2015.7163132},
	abstract = {We propose a novel multi-sensor system for accurate and power-efficient dynamic car-driver hand-gesture recognition, using a short-range radar, a color camera, and a depth camera, which together make the system robust against variable lighting conditions. We present a procedure to jointly calibrate the radar and depth sensors. We employ convolutional deep neural networks to fuse data from multiple sensors and to classify the gestures. Our algorithm accurately recognizes 10 different gestures acquired indoors and outdoors in a car during the day and at night. It consumes significantly less power than purely vision-based systems.},
	booktitle = {2015 11th {IEEE} {International} {Conference} and {Workshops} on {Automatic} {Face} and {Gesture} {Recognition} ({FG})},
	author = {Molchanov, Pavlo and Gupta, Shalini and Kim, Kihwan and Pulli, Kari},
	month = may,
	year = {2015},
	keywords = {Cameras, car-driver hand-gesture recognition, color camera, convolutional deep neural networks, data fusion, depth camera, depth sensors, gesture classification, gesture recognition, Gesture recognition, image classification, Image color analysis, multisensor system, neural nets, radar, Radar imaging, sensor fusion, Sensor systems, short-range radar, traffic engineering computing},
	pages = {1--8},
}


@inproceedings{Morris:2010,
    author = {Morris, Meredith Ringel and Wobbrock, Jacob O. and Wilson, Andrew D.},
    title = {{Understanding Users' Preferences for Surface Gestures}},
    year = {2010},
    isbn = {9781568817125},
    publisher = {Canadian Information Processing Society},
    address = {CAN},
    abstract = {We compare two gesture sets for interactive surfaces---a set of gestures created by an end-user elicitation method and a set of gestures authored by three HCI researchers. Twenty-two participants who were blind to the gestures' authorship evaluated 81 gestures presented and performed on a Microsoft Surface. Our findings indicate that participants preferred gestures authored by larger groups of people, such as those created by end-user elicitation methodologies or those proposed by more than one researcher. This preference pattern seems to arise in part because the HCI researchers proposed more physically and conceptually complex gestures than end-users. We discuss our findings in detail, including the implications for surface gesture design.},
    booktitle = {Proceedings of Graphics Interface 2010},
    pages = {261–268},
    numpages = {8},
    keywords = {interactive tabletops, surface computing, gestures},
    venue = {Ottawa, Ontario, Canada},
    series = {GI '10},
}


@inproceedings{Morris:2012,
    author = {Morris, Meredith Ringel},
    title = {{Web on the Wall: Insights from a Multimodal Interaction Elicitation Study}},
    year = {2012},
    isbn = {9781450312097},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2396636.2396651},
    doi = {10.1145/2396636.2396651},
    abstract = {New sensing technologies like Microsoft's Kinect provide a low-cost way to add interactivity to large display surfaces, such as TVs. In this paper, we interview 25 participants to learn about scenarios in which they would like to use a web browser on their living room TV. We then conduct an interaction-elicitation study in which users suggested speech and gesture interactions for fifteen common web browser functions. We present the most popular suggested interactions, and supplement these findings with observational analyses of common gesture and speech conventions adopted by our participants. We also reflect on the design of multimodal, multi-user interaction-elicitation studies, and introduce new metrics for interpreting user-elicitation study findings.},
    booktitle = {Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces},
    pages = {95–104},
    numpages = {10},
    keywords = {user-defined gestures, gestures, web browsers, participatory design, speech, interactive walls, multimodal input},
    venue = {Cambridge, Massachusetts, USA},
    series = {ITS '12},
}


@article{Morris:2014,
    author = {Morris, Meredith Ringel and Danielescu, Andreea and Drucker, Steven and Fisher, Danyel and Lee, Bongshin and schraefel, m. c. and Wobbrock, Jacob O.},
    title = {{Reducing Legacy Bias in Gesture Elicitation Studies}},
    year = {2014},
    issue_date = {May-June 2014},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {21},
    number = {3},
    issn = {1072-5520},
    url = {https://doi.org/10.1145/2591689},
    doi = {10.1145/2591689},
    journal = {Interactions},
    month = may,
    pages = {40–45},
    numpages = {6},
}


@INPROCEEDINGS{Munroe:2016,
    author={Munroe, Christopher and Meng, Yuanliang and Yanco, Holly and Begum, Momotaz},
    booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
    title={{Augmented reality eyeglasses for promoting home-based rehabilitation for children with cerebral palsy}}, 
    year={2016},
    volume={},
    number={},
    pages={565-565},
    keywords={Glass;Electromyography;Augmented reality;Electronic mail;Games;Accelerometers},
    doi={10.1109/HRI.2016.7451858},
}



%===========================================================
% References starting by N
%===========================================================
@inproceedings{Nacenta:2013,
    author = {Nacenta, Miguel A. and Kamber, Yemliha and Qiang, Yizhou and Kristensson, Per Ola},
    title = {{Memorability of Pre-Designed and User-Defined Gesture Sets}},
    year = {2013},
    isbn = {9781450318990},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2470654.2466142},
    doi = {10.1145/2470654.2466142},
    abstract = {We studied the memorability of free-form gesture sets for invoking actions. We compared three types of gesture sets: user-defined gesture sets, gesture sets designed by the authors, and random gesture sets in three studies with 33 participants in total. We found that user-defined gestures are easier to remember, both immediately after creation and on the next day (up to a 24\% difference in recall rate compared to pre-designed gestures). We also discovered that the differences between gesture sets are mostly due to association errors (rather than gesture form errors), that participants prefer user-defined sets, and that they think user-defined gestures take less time to learn. Finally, we contribute a qualitative analysis of the tradeoffs involved in gesture type selection and share our data and a video corpus of 66 gestures for replicability and further analysis.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {1099–1108},
    numpages = {10},
    keywords = {gesture memorability, gesture sets, user-defined gestures},
    venue = {Paris, France},
    series = {CHI '13},
}


@inproceedings{NagaKarthik:2013,
    author={NagaKarthik, Tumuganti and Ahn, Eun Hye and Bae, Yun Sik and Choi, Jun Rim},
    booktitle={2013 International SoC Design Conference (ISOCC)}, 
    title={{TCAM based pattern matching technique for hand gesture recognition}}, 
    year={2013},
    volume={},
    number={},
    pages={368-369},
    doi={10.1109/ISOCC.2013.6864052},
}


@inproceedings{Nancel:2011,
    author = {Nancel, Mathieu and Wagner, Julie and Pietriga, Emmanuel and Chapuis, Olivier and Mackay, Wendy},
    title = {{Mid-Air Pan-and-Zoom on Wall-Sized Displays}},
    year = {2011},
    isbn = {9781450302289},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1978942.1978969},
    doi = {10.1145/1978942.1978969},
    abstract = {Very-high-resolution wall-sized displays offer new opportunities for interacting with large data sets. While pointing on this type of display has been studied extensively, higher-level, more complex tasks such as pan-zoom navigation have received little attention. It thus remains unclear which techniques are best suited to perform multiscale navigation in these environments. Building upon empirical data gathered from studies of pan-and-zoom on desktop computers and studies of remote pointing, we identified three key factors for the design of mid-air pan-and-zoom techniques: uni- vs. bimanual interaction, linear vs. circular movements, and level of guidance to accomplish the gestures in mid-air. After an extensive phase of iterative design and pilot testing, we ran a controlled experiment aimed at better understanding the influence of these factors on task performance. Significant effects were obtained for all three factors: bimanual interaction, linear gestures and a high level of guidance resulted in significantly improved performance. Moreover, the interaction effects among some of the dimensions suggest possible combinations for more complex, real-world tasks.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {177–186},
    numpages = {10},
    keywords = {pan \& zoom, navigation, wall-sized displays, multi-scale interfaces, mid-air interaction techniques},
    venue = {Vancouver, BC, Canada},
    series = {CHI '11},
}


@phdthesis{Neuville:2021,
    title = {{Air+Touch gesture recognition : algorithms, software, and experiment}},
    author = {Neuville, Romain},
    abstract = {Human Computer Interaction became omnipresent in recent years, especially gesture recognition. Thanks to major advances in machine learning, gesture recognition has become an everyday tool. Currently, almost everybody has mastered 2D gestures with the use of smartphones. However, limits of these 2D gestures have emerged which has led researchers to focus on 3D gesture recognition. These two types of gesture enable to create Air+Touch recognition. It simply consists in an environment where gestures can be 2D or 3D. We have decided to deal with Air+Touch gesture recognition for this thesis. There is a clear lack of research on this subject. A problem related to this lack is that devices that can sense and retrieve data for the two types of gestures, are quite rare. The 3DTouchpad device can sense Air+Touch gestures. Therefore, we can build our own gestures set based on this device and on all the Air+Touch gestures gathered from the literature. Once it was defined, we integrated the 3DTouchpad inside the QuantumLeap framework where we tested all their different recognizers to see which one fits the best with Air+Touch gestures. Each test was performed according to two scenarios: User-Independent, User-Dependent. We also analyzed two gestures elicitation studies based on the 3DTouchpad to confirm that our Air+Touch assumptions were corresponding to 60 participants feelings. To conclude, we are going to discuss about the benefits and contributions of our work before suggesting some future promising works.},
    Keywords = {HCI , Gesture recognition , Air+Touch gesture recognition , 3DTouchpad , Gesture elicitation study , QuantumLeap},
    language = {Anglais},
    year = {2021},
    url = {http://hdl.handle.net/2078.1/thesis:33024},
    school = {UCL - Ecole polytechnique de Louvain},
}


@article{Ngan:2004,
    author = {Ngan, K.  and Straub, D.N.  and Bartello, P.},
    title = {{Three-dimensionalization of freely-decaying two-dimensional turbulence}},
    journal = {Physics of Fluids},
    volume = {16},
    number = {8},
    pages = {2918-2932},
    year = {2004},
    doi = {10.1063/1.1763191},
    URL = {https://doi.org/10.1063/1.1763191},
    eprint = {https://doi.org/10.1063/1.1763191},
}


@inproceedings{Nguyen:2018a,
	title = {{Range-gating technology for millimeter-wave radar remote gesture control in {IoT} applications}},
	doi = {10.1109/IEEE-IWS.2018.8400811},
	abstract = {Touchless hand gesture using portable millimeter-wave radar sensors an enabling technology for Internet of Things (IoT) applications. In this paper, we investigate the feasibility of using a frequency modulated continuous wave (FMCW) radar with noise removal and range gating method to recognize human hand gesture for a user moving in the radar's field of view. These detected hand gestures will be applied to remote control of computers or smart TVs at a distance from 0.3 m to 1.2 m.},
	booktitle = {2018 {IEEE} {MTT}-{S} {International} {Wireless} {Symposium} ({IWS})},
	author = {Nguyen, Minh Q. and Flores-Nigaglioni, Anthony and Li, Changzhi},
	month = may,
	year = {2018},
	keywords = {CW radar, Doppler radar, Estimation, FM radar, FMCW radar, frequency modulated continuous wave radar, gesture recognition, Gesture recognition, human hand gesture, Internet of Things, Internet of Things applications, IoT applications, millimeter-wave radar remote gesture control, millimetre wave radar, noise removal, noise removal method, portable millimeter-wave radar sensors, Radar detection, range gating method, range-gating technology, Sensors, size 0.3 m to 1.2 m, touchless hand gesture, Touchless hand gesture},
	pages = {1--4},
}


@inproceedings{Nguyen:2018b,
	title = {{Radar and ultrasound hybrid system for human computer interaction}},
	doi = {10.1109/RADAR.2018.8378783},
	abstract = {Touchless hand gesture is one emerging technology for human computer interaction. In this paper, we investigate the feasibility of a hybrid system using frequency modulated continuous wave (FMCW) radar and ultrasound sensors with one transmitter and one receiver to detect hand movement for controlling a computer. Ultrasound will be used for near range application (1 cm to 30 cm) based on range estimation. FMCW radar, on the other hand, will be used for far range application (30 cm to 120 cm) based on range, velocity, and angle of arrival estimation. Leveraging these advantages of combining both ultrasound and FMCW radar will facilitate human hand for interacting computer with better performance.},
	booktitle = {2018 {IEEE} {Radar} {Conference} ({RadarConf18})},
	author = {Nguyen, Minh Q. and Li, Changzhi},
	month = apr,
	year = {2018},
	note = {ISSN: 2375-5318},
	keywords = {Angle of Arrival, angle of arrival estimation, Chirp, CW radar, direction-of-arrival estimation, Doppler effect, Estimation, FM radar, FMCW radar, FMCW Radar, frequency modulated continuous wave radar, gesture recognition, hand movement, human computer interaction, human hand, Radar, radar computing, Range Estimation, Receivers, Sensors, touchless hand gesture, Touchless hand gesture, Ultrasonic imaging, Ultrasound, ultrasound hybrid system, ultrasound sensors, Velocity Estimation},
	pages = {1476--1480},
}


@book{Nielsen:1994,
    title={{Usability Engineering}},
    author={Nielsen, Jakob},
    isbn={9780125184069},
    lccn={lc93000488},
    series={Interactive Technologies},
    url={https://books.google.be/books?id=95As2OF67f0C},
    year={1994},
    publisher={Elsevier Science},
}


@article{Nimmagadda:2019,
	title = {{Signal Analysis based Radar Imaging Using Rdar Sensor}},
	volume = {9},
	issn = {2249-2976},
	url = {https://www.pramanaresearch.org/gallery/prj-s319_1.pdf},
	doi = {16.10089.PRJ.2019.V9I6.19.3724},
	number = {6},
	journal = {Pramana Research Journal},
	author = {Nimmagadda, Prathyusha and Inteti, Hemalatha},
	month = jul,
	year = {2019},
	keywords = {gestures, interaction, radar, RF, sensors},
	pages = {1919--1924},
}


@article{Norman:1999,
    author = {Norman, Donald A.},
    title = {Affordance, conventions, and design},
    year = {1999},
    issue_date = {May/June 1999},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {6},
    number = {3},
    issn = {1072-5520},
    url = {https://doi.org/10.1145/301153.301168},
    doi = {10.1145/301153.301168},
    journal = {Interactions},
    month = {may},
    pages = {38–43},
    numpages = {6},
}


@article{Norman:2008,
    author = {Norman, Donald A.},
    title = {{Signifiers, not affordances}},
    year = {2008},
    issue_date = {November + December 2008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {15},
    number = {6},
    issn = {1072-5520},
    url = {https://doi.org/10.1145/1409040.1409044},
    doi = {10.1145/1409040.1409044},
    journal = {Interactions},
    month = {nov},
    pages = {18–19},
    numpages = {2},
}


@article{Norman:2010,
    author = {Norman, Donald A.},
    title = {{Natural user interfaces are not natural}},
    year = {2010},
    issue_date = {May + June 2010},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {17},
    number = {3},
    issn = {1072-5520},
    url = {https://doi.org/10.1145/1744161.1744163},
    doi = {10.1145/1744161.1744163},
    journal = {Interactions},
    month = {may},
    pages = {6–10},
    numpages = {5},
}


@phdthesis{Nothomb:2020,
    title = {{Opportunity study for the implementation of gesture recognition technology in the hospital environment - Commercialization through a university spin-off}},
    author = {Usuwiel, Harry and Wilberz, Jean and d'Aspremont Lynden, Tancrède and Nothomb, Bastien},
    abstract = {This master's thesis ambition is to study the opportunity to launch a UCLouvain spin-off specialized in the interaction between surgeons and medical imaging during operations. Guinevere will propose a software that will allow surgeons to control medical imaging in real time with simple hand gestures. This innovation will save surgeons valuable time while ensuring a sterile environment.},
    Keywords = {Touchless interaction , Surgery , Healthcare sector , Spin-off},
    language = {Anglais},
    year = {2020},
    url = {http://hdl.handle.net/2078.1/thesis:24615},
    school = {UCL UCL UCL UCL - Louvain School of Management Louvain School of Management Faculté des sciences économiques, sociales, politiques et de communication Ecole polytechnique de Louvain},
    url = {http://hdl.handle.net/2078.1/thesis:24615},
}



%===========================================================
% References starting by O
%===========================================================
@incollection{Oberhammer:2013,
    title = {{RF MEMS for automotive radar}},
    editor = {Deepak Uttamchandani},
    booktitle = {Handbook of Mems for Wireless and Mobile Applications},
    publisher = {Woodhead Publishing},
    pages = {518-549},
    year = {2013},
    series = {Woodhead Publishing Series in Electronic and Optical Materials},
    isbn = {978-0-85709-271-7},
    doi = {https://doi.org/10.1533/9780857098610.2.518},
    url = {https://www.sciencedirect.com/science/article/pii/B9780857092717500165},
    author = {Oberhammer, Joachim and Somjit, Nutapong and Shah, Umer and Baghchehsaraei, Zargham},
    keywords = {RF, MEMS, car radar, automotive radar, phase shifter},
    abstract = {Abstract:
    Radio-frequency microelectromechanical systems (RF MEMS) devices and circuits have attracted interest in applications such as car radar systems, particularly in the 76–81GHz frequency band, due to their near ideal signal performance and compatibility with semiconductor fabrication technology. This chapter gives an introduction to state-of-the-art car radar sensors and architectures, describes the most commonly engaged RF MEMS components and circuits, and gives examples of RF MEMS-based automotive radar prototypes.},
}


@inproceedings{Oh:2013,
    author = {Oh, Uran and Findlater, Leah},
    title = {{The challenges and potential of end-user gesture customization}},
    year = {2013},
    isbn = {9781450318990},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2470654.2466145},
    doi = {10.1145/2470654.2466145},
    abstract = {The vast majority of work on understanding and supporting the gesture creation process has focused on professional designers. In contrast, gesture customization by end users' - which may offer better memorability, efficiency and accessibility than pre-defined gestures - has received little attention. To understand the end-user gesture creation process, we conducted a study where 20 participants were asked to: (1) exhaustively create new gestures for an open-ended use case; (2) exhaustively create new gestures for 12 specific use cases; (3) judge the saliency of different touchscreen gesture features. Our findings showed that even when asked to create novel gestures, participants tended to focus on the familiar. Misconceptions about the gesture recognizer's abilities were also evident, and in some cases constrained the range of gestures that participants created. Finally, as a calibration point for future research, we used a simple gesture recognizer ($N) to analyze recognition accuracy of the participants' custom gesture sets: accuracy was 68-88\% on average, depending on the amount of training and the customization scenario. We conclude with implications for the design of a mixed-initiative approach to support custom gesture creation.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {1129–1138},
    numpages = {10},
    keywords = {customization, gestures, personalization, touchscreen},
    location = {Paris, France},
    series = {CHI '13},
}


@ARTICLE{Ovur:2021,
    author={Ovur, Salih Ertug and Su, Hang and Qi, Wen and De Momi, Elena and Ferrigno, Giancarlo},
    journal={IEEE Transactions on Instrumentation and Measurement}, 
    title={{Novel Adaptive Sensor Fusion Methodology for Hand Pose Estimation With Multileap Motion}}, 
    year={2021},
    volume={70},
    number={},
    pages={1-8},
    keywords={Sensor fusion;Kalman filters;Calibration;Position measurement;Human computer interaction;Real-time systems;Adaptive sensor fusion;human–computer interaction (HCI);Kalman filter;leap motion;mutisensor fusion},
    doi={10.1109/TIM.2021.3063752},
}



@inproceedings{Ortega:2017,
    author={Ortega, Francisco R. and Galvan, Alain and Tarre, Katherine and Barreto, Armando and Rishe, Naphtali and Bernal, Jonathan and Balcazar, Ruben and Thomas, Jason-Lee},
    booktitle={2017 IEEE Symposium on 3D User Interfaces (3DUI)}, 
    title={{Gesture elicitation for 3D travel via multi-touch and mid-Air systems for procedurally generated pseudo-universe}}, 
    year={2017},
    volume={},
    number={},
    pages={144-153},
    doi={10.1109/3DUI.2017.7893331},
}


@article{Ousmer:2020,
    author = {Ousmer, Mehdi and Slu\"{y}ters, Arthur and Magrofuoco, Nathan and Roselli, Paolo and Vanderdonckt, Jean},
    title = {{Recognizing 3D Trajectories as 2D Multi-Stroke Gestures}},
    year = {2020},
    issue_date = {November 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {4},
    number = {ISS},
    url = {https://doi.org/10.1145/3427326},
    doi = {10.1145/3427326},
    abstract = {While end users can acquire full 3D gestures with many input devices, they often capture only 3D trajectories, which are 3D uni-path, uni-stroke single-point gestures performed in thin air. Such trajectories with their $(x,y,z)$ coordinates could be interpreted as three 2D stroke gestures projected on three planes,ie, $XY$, $YZ$, and $ZX$, thus making them admissible for established 2D stroke gesture recognizers. To investigate whether 3D trajectories could be effectively and efficiently recognized, four 2D stroke gesture recognizers, ie,  Rubine3D, another extension of Rubine for 3D which projects the 3D gesture on three orthogonal planes, is also included. These seven recognizers are compared against three challenging datasets containing 3D trajectories, ie, SHREC2019 and 3DTCGS, in a user-independent scenario, and 3DMadLabSD with its four domains, in both user-dependent and user-independent scenarios, with varying number of templates and sampling. Individual recognition rates and execution times per dataset and aggregated ones on all datasets show a highly significant difference of $P+^3$ over its competitors. The potential effects of the dataset, the number of templates, and the sampling are also studied.},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = nov,
    articleno = {198},
    numpages = {21},
    keywords = {gesture recognition, large display interfaces and multi-display environments, gesture-based interfaces, mid-air gestural interaction, stroke gestures, surface computing, 3d trajectory},
}


@inproceedings{Ouyang:2012,
    author = {Ouyang, Tom and Li, Yang},
    title = {{Bootstrapping personal gesture shortcuts with the wisdom of the crowd and handwriting recognition}},
    year = {2012},
    isbn = {9781450310154},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2207676.2208695},
    doi = {10.1145/2207676.2208695},
    abstract = {Personal user-defined gesture shortcuts have shown great potential for accessing the ever-growing amount of data and computing power on touchscreen mobile devices. However, their lack of scalability is a major challenge for their wide adoption. In this paper, we present Gesture Marks, a novel approach to touch-gesture interaction that allows a user to access applications and websites using gestures without having to define them first. It offers two distinctive solutions to address the problem of scalability. First, it leverages the "wisdom of the crowd", a continually evolving library of gesture shortcuts that are collected from the user population, to infer the meaning of gestures that a user never defined himself. Second, it combines an extensible template-based gesture recognizer with a specialized handwriting recognizer to even better address handwriting-based gestures, which are a common form of gesture shortcut. These approaches effectively bootstrap a user's personal gesture library, alleviating the need to define most gestures manually. Our work was motivated and validated via a series of user studies, and the findings from these studies add to the body of knowledge on gesture-based interaction.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2895–2904},
    numpages = {10},
    keywords = {search, mobile computing, gesture-based interaction},
    location = {Austin, Texas, USA},
    series = {CHI '12},
}


%===========================================================
% References starting by P
%===========================================================
@article{Palipana:2021,
	title = {Pantomime: {Mid}-{Air} {Gesture} {Recognition} with {Sparse} {Millimeter}-{Wave} {Radar} {Point} {Clouds}},
	volume = {5},
	shorttitle = {Pantomime},
	url = {https://doi.org/10.1145/3448110},
	doi = {10.1145/3448110},
	abstract = {We introduce Pantomime, a novel mid-air gesture recognition system exploiting spatio-temporal properties of millimeter-wave radio frequency (RF) signals. Pantomime is positioned in a unique region of the RF landscape: mid-resolution mid-range high-frequency sensing, which makes it ideal for motion gesture interaction. We configure a commercial frequency-modulated continuous-wave radar device to promote spatial information over the temporal resolution by means of sparse 3D point clouds and contribute a deep learning architecture that directly consumes the point cloud, enabling real-time performance with low computational demands. Pantomime achieves 95\% accuracy and 99\% AUC in a challenging set of 21 gestures articulated by 41 participants in two indoor environments, outperforming four state-of-the-art 3D point cloud recognizers. We further analyze the effect of the environment in 5 different indoor environments, the effect of articulation speed, angle, and the distance of the person up to 5m. We have publicly made available the collected mmWave gesture dataset consisting of nearly 22,000 gesture instances along with our radar sensor configuration, trained models, and source code for reproducibility. We conclude that pantomime is resilient to various input conditions and that it may enable novel applications in industrial, vehicular, and smart home scenarios.},
	number = {1},
	urldate = {2021-05-25},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Palipana, Sameera and Salami, Dariush and Leiva, Luis A. and Sigg, Stephan},
	month = mar,
	year = {2021},
	keywords = {gesture recognition, deep learning, radar sensing},
	pages = {27:1--27:27},
}


@article {Page:2021,
	author = {Page, Matthew J and Moher, David and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M and Hr{\'o}bjartsson, Asbj{\o}rn and Lalu, Manoj M and Li, Tianjing and Loder, Elizabeth W and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A and Stewart, Lesley A and Thomas, James and Tricco, Andrea C and Welch, Vivian A and Whiting, Penny and McKenzie, Joanne E},
	title = {{PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews}},
	volume = {372},
	elocation-id = {n160},
	year = {2021},
	doi = {10.1136/bmj.n160},
	publisher = {BMJ Publishing Group Ltd},
	URL = {https://www.bmj.com/content/372/bmj.n160},
	eprint = {https://www.bmj.com/content/372/bmj.n160.full.pdf},
	journal = {BMJ},
    pages = {1--36},
}


@misc{Pandian:2020,
    title={{NASA-TLX Web App: An Online Tool to Analyse Subjective Workload}}, 
    author={Sermuga Pandian, Vinoth Pandian and Suleri, Sarah},
    year={2020},
    eprint={2001.09963},
    archivePrefix={arXiv},
    primaryClass={cs.HC},
}


@inproceedings{Paradiso:1997,
    author = {Paradiso, Joseph and Abler, Craig and Hsiao, Kai-yuh and Reynolds, Matthew},
    title = {{The Magic Carpet: Physical Sensing for Immersive Environments}},
    year = {1997},
    isbn = {0897919262},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1120212.1120391},
    doi = {10.1145/1120212.1120391},
    abstract = {An interactive environment has been developed that uses a pair of Doppler radars to
    measure upper-body kinematics (velocity, direction of motion, amount of motion) and
    a grid of piezoelectric wires hidden under a 6 x 10 foot carpet to monitor dynamic
    foot position and pressure. This system has been used in an audio installation, where
    users launch and modify complex musical sounds and sequences as they wander about
    the carpet. This paper describes the floor and radar systems, quantifies their performance,
    and outlines the musical application.},
    booktitle = {CHI '97 Extended Abstracts on Human Factors in Computing Systems},
    pages = {277–278},
    numpages = {2},
    keywords = {doppler radar, immersive environment, foot sensing, musical interfaces, PVDF, piezoelectrics},
    venue = {Atlanta, Georgia},
    series = {CHI EA '97},
}


@inproceedings{Park:2016,
	title = {{IR}-{UWB} {Radar} {Sensor} for {Human} {Gesture} {Recognition} by {Using} {Machine} {Learning}},
	doi = {10.1109/HPCC-SmartCity-DSS.2016.0176},
	abstract = {In this paper, we propose a human gesture recognition algorithm using impulse radio ultra-wideband (IR-UWB) radar. The radar signal is transmitted into a three dimensional space, however, the received signal is only expressed in one dimensional. Therefore, it is difficult to classify 3-D gestures by analyzing specific features, such as power, peak value, index of peak value, and other values of received signal. To resolve this problem, a new human gesture recognition algorithm using machine learning is proposed. Two machine learning technics are used in this paper. One is unsupervised learning technic which is used for extracting features from received radar signal is principal component analysis, and the other one is supervised learning which is used for classifying gestures. The features are extracted by using the principal component analysis (PCA) method, then neural network method is used for training and classifying gestures using the extracted features. In training and classifying step, other method can be used, such as supporting vector machine (SVM), however, this method is hard to recognize noise gesture which means untrained gesture. To resolve this problem, we use neural network method in this paper, then in order to classy noise gestures and trained gestures, a noise determining algorithm is used.},
	booktitle = {2016 {IEEE} 18th {International} {Conference} on {High} {Performance} {Computing} and {Communications}; {IEEE} 14th {International} {Conference} on {Smart} {City}; {IEEE} 2nd {International} {Conference} on {Data} {Science} and {Systems} ({HPCC}/{SmartCity}/{DSS})},
	author={Park, Junbum and Cho, Sung Ho},
	month = dec,
	year = {2016},
	keywords = {3D gestures classification, feature extraction, Feature extraction, gesture recognition, Gesture recognition, hand gesture recognition, human gesture recognition, impulse radio ultra-wideband (IR-UWB), impulse radio ultra-wideband radar, IR-UWB radar sensor, machine learning, Machine learning algorithms, motion recognition, neural nets, neural network, Neural networks, noise determining algorithm, PCA, principal component analysis, Principal component analysis, Radar, radar receivers, radar sensor, radar signal processing, received radar signal, Training, ultra wideband radar, unsupervised learning},
	pages = {1246--1249},
}


@inproceedings{Park:2019,
    author    = {Park, Chanho and Cho, Hyunwoo and Park, Sangheon and Yoon, Young-Suk and Jung, Sung-Uk},
    booktitle = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
    title     = {{HandPoseMenu: Hand Posture-Based Virtual Menus for Changing Interaction Mode in 3D Space}},
    year      = {2019},
    address   = {New York, NY, USA},
    pages     = {361–366},
    publisher = {Association for Computing Machinery},
    series    = {ISS '19},
    doi       = {10.1145/3343055.3360752},
    isbn      = {9781450368919},
    keywords  = {gesture recognition, graphical menu, hand posture, virtual reality, head-mounted display., mixed reality},
    venue  = {Daejeon, Republic of Korea},
    numpages  = {6},
    url       = {https://doi.org/10.1145/3343055.3360752},
}


@article{Park:2020,
	title = {A {Time} {Domain} {Artificial} {Intelligence} {Radar} {System} {Using} 33-{GHz} {Direct} {Sampling} for {Hand} {Gesture} {Recognition}},
	volume = {55},
	issn = {1558-173X},
	doi = {10.1109/JSSC.2020.2967547},
	abstract = {This article introduces a time-domain-based artificial intelligence (AI) radar system for gesture recognition using 33-GS/s direct sampling technique. High-speed sampling using a time-extension method allows AI learning to be applied to a time-domain radar signal reflecting information on both dynamic and static gestures, and thus can recognize not only dynamic but also static gestures. The Vernier clock generators and high-speed active samplers applied with the time-extension technique makes sampling at 33 GS/s possible. A 1-D convolutional neural network and long short-term memory are employed for both static and dynamic gestures and recognition rates of 93.2\% and 90.5\% are obtained, respectively. The radar system is implemented using a 65-nm CMOS process with a power consumption of 95 mW.},
	number = {4},
	journal = {IEEE Journal of Solid-State Circuits},
	author={Park, Jungwoon and Jang, Junyoung and Lee, Geunhaeng and Koh, Hyunmin and Kim, Changhwan and Kim, Tae Wook},
	month = apr,
	year = {2020},
	keywords = {1-D convolutional neural network, AI learning, Artificial intelligence, Artificial intelligence (AI) radar, Capacitors, clocks, Clocks, convolutional neural nets, direct sampling technique, dynamic gestures, Generators, gesture recognition, hand gesture recognition, high-speed sampling, impulse radar ultra-wideband (IR-UWB), learning (artificial intelligence), long short-term memory, power 95.0 mW, radar, Radar, radar computing, radar signal processing, recognition rates, recurrent neural nets, sampler, signal sampling, static gestures, time domain artificial intelligence radar system, time-domain analysis, Time-domain analysis, time-domain radar signal, time-domain-based artificial intelligence radar system, time-extension, time-extension method, time-extension technique, time-to-digital converter (TDC), Timing, transceiver, Vernier clock generators, wireless sensing},
	pages = {879--888},
}


@inproceedings{Parsons:1999,  
    author={Parsons, David and Rashid, Awais and Speck, Andreas and Telea, Alexandru C.},  
    booktitle={Proceedings Technology of Object-Oriented Languages and Systems. TOOLS 29 (Cat. No.PR00275)},   
    title={{A "framework" for object oriented frameworks design}},   
    year={1999},  
    volume={},  
    number={},  
    pages={141-151},  
    doi={10.1109/TOOLS.1999.779007}, 
    url={https://ieeexplore.ieee.org/author/37268563800},
}


@inproceedings{Parthiban:2019,
    author = {Parthiban, Vik and Lee, Ashley Jieun},
    title = {{LUI: A Multimodal, Intelligent Interface for Large Displays}},
    year = {2019},
    isbn = {9781450370028},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3359997.3365743},
    doi = {10.1145/3359997.3365743},
    booktitle = {The 17th International Conference on Virtual-Reality Continuum and Its Applications in Industry},
    articleno = {article 48},
    numpages = {2},
    keywords = {large displays, gestures, human-computer interaction, augmented reality, voice},
    venue = {Brisbane, QLD, Australia},
    series = {VRCAI ’19}
}


@inproceedings{Patra:2018,
	address = {New York, NY, USA},
	series = {{mmNets} '18},
	title = {mm-{Wave} {Radar} {Based} {Gesture} {Recognition}: {Development} and {Evaluation} of a {Low}-{Power}, {Low}-{Complexity} {System}},
	isbn = {978-1-4503-5928-3},
	shorttitle = {mm-{Wave} {Radar} {Based} {Gesture} {Recognition}},
	url = {https://doi.org/10.1145/3264492.3264501},
	doi = {10.1145/3264492.3264501},
	abstract = {Gesture recognition is gaining attention as an attractive feature for the development of ubiquitous, context-aware, IoT applications. Use of radars as a primary or secondary system is tempting, as they can operate in darkness, high light intensity environments, and longer distances than many competitor systems. Starting from this observation, we present a generic, low-cost, mm-wave radar-based gesture recognition system. Among potential benefits of mm-wave radars are a high spatial resolution due to small wavelength, the availability of multiple antennas in a small area and the low interference due to the natural attenuation of mm-wave radiation. We experimentally evaluate our COTS solution considering eight different gestures and using two low-complexity classification algorithms: the unsupervised Self Organized Map (SOM) and the supervised Learning Vector Quantization (LVQ). To test robustness, we consider gestures performed by a human hand and a human body, at short and long distance. From our preliminary evaluations, we observe that LVQ and SOM correctly detect 75\% and 60\% of all gestures, respectively, from the raw, unprocessed data. The detection rate is significantly higher ({\textgreater}90\%) for selected gesture groups. We argue that performance suffers due to inaccurate AoA estimation. Accordingly, we evaluate our system employing a two-radar setup that increases the estimation accuracy by 8-9\%.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2nd {ACM} {Workshop} on {Millimeter} {Wave} {Networks} and {Sensing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Patra, Avishek and Geuer, Philipp and Munari, Andrea and Mähönen, Petri},
	month = oct,
	year = {2018},
	keywords = {gesture recognition, machine learning, mm-wave, radar},
	pages = {51--56},
}


@article{Patriarca:2011,
    title = {{Reconstruction of sub-wavelength fractures and physical properties of masonry media using full-waveform inversion of proximal penetrating radar}},
    journal = {Journal of Applied Geophysics},
    volume = {74},
    number = {1},
    pages = {26-37},
    year = {2011},
    issn = {0926-9851},
    doi = {https://doi.org/10.1016/j.jappgeo.2011.03.001},
    url = {https://www.sciencedirect.com/science/article/pii/S0926985111000577},
    author = {Patriarca, Claudio and Lambot, S{\'{e}}bastien and Mahmoudzadeh, Mohammad Reza and Minet, Julien and Slob, Evert},
    keywords = {Ground penetrating radar, Electromagnetic inverse problem, Electric properties of concrete, Material characterization, Non-destructive testing},
    abstract = {High-frequency, ultra-wideband penetrating radar has the potential to be used as a non-invasive inspection technique for buildings, providing high-resolution images of structures and possible fractures affecting constructions. To test this possibility, numerical and laboratory experiments have been conducted using a proximal, stepped-frequency continuous-wave radar system operating in zero-offset mode, spanning the 3–8GHz frequency range. The reconstruction of the material electrical properties is achieved by resorting to full-waveform inverse modeling. Numerical experiments showed that for typical electric permittivity and electrical conductivity values of concrete and plaster, it is possible to retrieve the physical properties of the material and to detect fractures less than 1mm thick. Laboratory experiments were conducted on non-reinforced concrete and plaster test slabs in different configurations. The results showed the good potential of this method: (1) to provide a thorough fracture response model in buildings or artworks and (2) to non-invasively characterize the samples in terms of their electromagnetic properties.},
}


@article{Pauchot:2015,
    author = {Pauchot, Julien and Di Tommaso, Laetitia and Lounis, Ahmed and Benassarou, Mourad and Mathieu, Pierre and Bernot, Dominique and Aubry, Sébastien},
    title = {{Leap Motion Gesture Control With Carestream Software in the Operating Room to Control Imaging: Installation Guide and Discussion}},
    journal = {Surgical Innovation},
    volume = {22},
    number = {6},
    pages = {615-620},
    year = {2015},
    doi = {10.1177/1553350615587992},
    note = {PMID: 26002115},
    url = {https://doi.org/10.1177/1553350615587992},
}


@inproceedings{Pavlovych:2009,
	address = {New York, NY, USA},
	series = {{EICS} '09},
	title = {{The tradeoff between spatial jitter and latency in pointing tasks}},
	isbn = {978-1-60558-600-7},
	url = {https://doi.org/10.1145/1570433.1570469},
	doi = {10.1145/1570433.1570469},
	abstract = {Interactive computing systems frequently use pointing as an input modality, while also supporting other forms of input such as alphanumeric, voice, gesture, and force. We focus on pointing and investigate the effects of input device latency and spatial jitter on 2D pointing speed and accuracy. First, we characterize the latency and jitter of several common input devices. Then we present an experiment, based on ISO 9241-9, where we systematically explore combinations of latency and jitter on a desktop mouse to measure how these factors affect human performance. The results indicate that, while latency has a stronger effect on human performance compared to low amounts of spatial jitter, jitter dramatically increases the error rate, roughly inversely proportional to the target size. The findings can be used in the design of pointing devices for interactive systems, by providing a guideline for choosing parameters of spatial filtering to compensate for jitter, since stronger filtering typically also increases lag. We also describe target sizes at which error rates start to increase notably, as this is relevant for user interfaces where hand tremor or similar factors play a major role.},
	urldate = {2021-01-07},
	booktitle = {Proceedings of the 1st {ACM} {SIGCHI} symposium on {Engineering} interactive computing systems},
	publisher = {Association for Computing Machinery},
	author = {Pavlovych, Andriy and Stuerzlinger, Wolfgang},
	month = jul,
	year = {2009},
	keywords = {fitts' law, jitter, latency, pointing},
	pages = {187--196},
}


@inproceedings{Perez:2019,  
    author={Pérez-Medina, Jorge-Luis and Vanderdonckt, Jean and De Coster, Albéric and Lambot, S{\'{e}}bastien},  
    booktitle={2019 Sixth International Conference on eDemocracy \& eGovernment (ICEDEG)},   
    title={{Mobile Direct Visualization of Pipes, Apexes, and Water Leaks in Water Distribution Networks}},   
    year={2019},  
    volume={},  
    number={},  
    pages={172-179},  
    doi={10.1109/ICEDEG.2019.8734376},
}


@article{Perez:2020,
    AUTHOR = {Pérez-Medina, Jorge-Luis and Villarreal, Santiago and Vanderdonckt, Jean},
    TITLE = {{A Gesture Elicitation Study of Nose-Based Gestures}},
    JOURNAL = {Sensors},
    VOLUME = {20},
    YEAR = {2020},
    NUMBER = {24},
    article-NUMBER = {7118},
    URL = {https://www.mdpi.com/1424-8220/20/24/7118},
    ISSN = {1424-8220},
    ABSTRACT = {Presently, miniaturized sensors can be embedded in any small-size wearable to recognize movements on some parts of the human body. For example, an electrooculography-based sensor in smart glasses recognizes finger movements on the nose. To explore the interaction capabilities, this paper conducts a gesture elicitation study as a between-subjects experiment involving one group of 12 females and one group of 12 males, expressing their preferred nose-based gestures on 19 Internet-of-Things tasks. Based on classification criteria, the 912 elicited gestures are clustered into 53 unique gestures resulting in 23 categories, to form a taxonomy and a consensus set of 38 final gestures, providing researchers and practitioners with a larger base with six design guidelines. To test whether the measurement method impacts these results, the agreement scores and rates, computed for determining the most agreed gestures upon participants, are compared with the Condorcet and the de Borda count methods to observe that the results remain consistent, sometimes with a slightly different order. To test whether the results are sensitive to gender, inferential statistics suggest that no significant difference exists between males and females for agreement scores and rates.},
    DOI = {10.3390/s20247118},
}


@inproceedings{Piumsomboon:2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {User-{Defined} {Gestures} for {Augmented} {Reality}},
	isbn = {978-3-642-40480-1},
	doi = {10.1007/978-3-642-40480-1_18},
	abstract = {Recently there has been an increase in research towards using hand gestures for interaction in the field of Augmented Reality (AR). These works have primarily focused on researcher designed gestures, while little is known about user preference and behavior for gestures in AR. In this paper, we present our guessability study for hand gestures in AR in which 800 gestures were elicited for 40 selected tasks from 20 participants. Using the agreement found among gestures, a user-defined gesture set was created to guide designers to achieve consistent user-centered gestures in AR. Wobbrock’s surface taxonomy has been extended to cover dimensionalities in AR and with it, characteristics of collected gestures have been derived. Common motifs which arose from the empirical findings were applied to obtain a better understanding of users’ thought and behavior. This work aims to lead to consistent user-centered designed gestures in AR.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2013},
	publisher = {Springer},
	author = {Piumsomboon, Thammathip and Clark, Adrian and Billinghurst, Mark and Cockburn, Andy},
	editor = {Kotzé, Paula and Marsden, Gary and Lindgaard, Gitte and Wesson, Janet and Winckler, Marco},
	year = {2013},
	keywords = {Augmented reality, gestures, guessability},
	pages = {282--299},
}


@inproceedings{Pham:2018,
    author = {Pham, Tran and Vermeulen, Jo and Tang, Anthony and MacDonald Vermeulen, Lindsay},
    title = {{Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design}},
    year = {2018},
    isbn = {9781450351980},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3196709.3196719},
    doi = {10.1145/3196709.3196719},
    abstract = {Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.},
    booktitle = {Proceedings of the 2018 Designing Interactive Systems Conference},
    pages = {227–240},
    numpages = {14},
    keywords = {gestures, hololens, augmented reality, gesture elicitation},
    venue = {Hong Kong, China},
    series = {DIS '18},
}


@ARTICLE{Ponraj:2018,
    author={Ponraj, Godwin and Ren, Hongliang},
    journal={IEEE Sensors Journal}, 
    title={{Sensor Fusion of Leap Motion Controller and Flex Sensors Using Kalman Filter for Human Finger Tracking}}, 
    year={2018},
    volume={18},
    number={5},
    pages={2042-2049},
    keywords={Sensor fusion;Thumb;Tracking;Kalman filters;Joints;Leap motion controller;flex sensors;sensor fusion;Kalman filter},
    doi={10.1109/JSEN.2018.2790801},
}



@inproceedings{Pree:1994,
    author = {Pree, Wolfgang},
    title = {{Meta Patterns - A Means For Capturing the Essentials of Reusable Object-Oriented Design}},
    year = {1994},
    isbn = {3540582029},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    booktitle = {Proceedings of the 8th European Conference on Object-Oriented Programming},
    pages = {150–162},
    numpages = {13},
    series = {ECOOP ’94}
}


@inproceedings{Preventis:2014,
  author    = {Preventis, Alexandros and Stravoskoufos, Kostas and Sotiriadis, Stelios and Petrakis, Euripides G. M.},
  booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
  title     = {{Interact: Gesture Recognition in the Cloud}},
  year      = {2014},
  address   = {USA},
  pages     = {501–502},
  publisher = {IEEE Computer Society},
  series    = {UCC '14},
  doi       = {10.1109/UCC.2014.71},
  isbn      = {9781479978816},
  keywords  = {Cloud computing, gesture recognition, FI-WARE},
  numpages  = {2},
  url       = {https://doi.org/10.1109/UCC.2014.71},
}


@article{Pucihar:2022,
    author = {\v{C}opi\v{c} Pucihar, Klen and Attygalle, Nuwan T. and Kljun, Matjaz and Sandor, Christian and Leiva, Luis A.},
    title = {{Solids on Soli: Millimetre-Wave Radar Sensing through Materials}},
    year = {2022},
    issue_date = {June 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {6},
    number = {EICS},
    url = {https://doi.org/10.1145/3532212},
    doi = {10.1145/3532212},
    abstract = {Gesture recognition with miniaturised radar sensors has received increasing attention as a novel interaction medium. The practical use of radar technology, however, often requires sensing through materials. Yet, it is still not well understood how the internal structure of materials impacts recognition performance. To tackle this challenge, we collected a large dataset of 14,090 radar recordings for 6 paradigmatic gesture classes sensed through a variety of everyday materials, performed by humans (6 materials) and a robot system (75 materials). Next, we developed a hybrid CNN+LSTM deep learning model and derived a robust indirect method to measure signal distortions, which we used to compile a comprehensive catalogue of materials for radar-based interaction. Among other findings, our experiments show that it is possible to estimate how different materials would affect gesture recognition performance of arbitrary classifiers by selecting just 3 reference materials. Our catalogue, software, models, data collection platform, and labeled datasets are publicly available.},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = jun,
    articleno = {156},
    numpages = {19},
    keywords = {radar interaction, deep learning, gestures, materials, soli},
}


%===========================================================
% References starting by Q
%===========================================================


%===========================================================
% References starting by R
%===========================================================
@inproceedings{Rakthanmanon:2012,
    author = {Rakthanmanon, Thanawin and Campana, Bilson and Mueen, Abdullah and Batista, Gustavo and Westover, Brandon and Zhu, Qiang and Zakaria, Jesin and Keogh, Eamonn},
    title = {{Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping}},
    year = {2012},
    isbn = {9781450314626},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2339530.2339576},
    doi = {10.1145/2339530.2339576},
    booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {262–270},
    numpages = {9},
    keywords = {lower bounds, time series, similarity search},
    venue = {Beijing, China},
    series = {KDD ’12},
}


@inproceedings{Rajabiyazdi:2015,
    author = {Rajabiyazdi, Fateme and Walny, Jagoda and Mah, Carrie and Brosz, John and Carpendale, Sheelagh},
    title = {{Understanding Researchers' Use of a Large, High-Resolution Display Across Disciplines}},
    year = {2015},
    isbn = {9781450338998},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2817721.2817735},
    doi = {10.1145/2817721.2817735},
    abstract = {A driving force behind the design of increasingly large and high resolution displays (LHRDs) has been the need to support the explosion of data in the natural sciences such as physics, chemistry, and biology. However, our experience with an LHRD accessible to researchers across multiple disciplines has shown that they are useful for a wide range of research activities involving large images and data.  We conducted in-context, semi-structured interviews with researchers from a variety of disciplines about their experiences using the LHRD with their own data. Notably, it became apparent that the size and resolution of the LHRD supported a multitude of activities related to observation, for which zooming or other enlargement methods on standard resolution screens were not sufficient. The interview findings lead to implications for further research into supporting a broader range of disciplines in using large, high-resolution displays.},
    booktitle = {Proceedings of the 2015 International Conference on Interactive Tabletops \& Surfaces},
    pages = {107–116},
    numpages = {10},
    keywords = {multi-disciplinary, high-resolution displays, research support, education, large, qualitative study, observation},
    venue = {Madeira, Portugal},
    series = {ITS '15},
}
  

@article{Rautaray:2015,
    author    = {Rautaray, Siddharth S. and Agrawal, Anupam},
    title     = {{Vision based hand gesture recognition for human computer interaction: a survey}},
    journal   = {Artif. Intell. Rev.},
    volume    = {43},
    number    = {1},
    pages     = {1--54},
    year      = {2015},
    url       = {https://doi.org/10.1007/s10462-012-9356-9},
    doi       = {10.1007/s10462-012-9356-9},
    timestamp = {Wed, 17 May 2017 14:25:43 +0200},
    biburl    = {https://dblp.org/rec/journals/air/RautarayA15.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@misc{Rawart:2023,
    author = {Rawart, Isabelle},
    title = {{Citizens of Wallonia 2023. 9 projets innovants récompensés}},
    year = {2023},
    note = {[Online; accessed 20-November-2023]},
    howpublished = {\url{https://www.digitalwallonia.be/fr/publications/citizens-of-wallonia-2023-9-projets-innovants/}},
}


@inproceedings{Reaver:2011,
    author = {Reaver, J. and Stahovich, Thomas F. and Herold, James},
    title = {{How to Make a Quick\$: Using Hierarchical Clustering to Improve the Efficiency of the Dollar Recognizer}},
    booktitle = {Proceedings of the Eighth Eurographics Symposium on Sketch-Based Interfaces and Modeling},
    series = {SBIM '11},
    year = {2011},
    isbn = {978-1-4503-0906-6},
    venue = {Vancouver, British Columbia, Canada},
    pages = {103--108},
    numpages = {6},
    url = {http://doi.acm.org/10.1145/2021164.2021183},
    doi = {10.1145/2021164.2021183},
    acmid = {2021183},
    publisher = {ACM},
    address = {New York, NY, USA},
} 


@inproceedings{Reisman:2009,
    author = {Reisman, Jason L. and Davidson, Philip L. and Han, Jefferson Y.},
    title = {{A Screen-Space Formulation for 2D and 3D Direct Manipulation}},
    year = {2009},
    isbn = {9781605587455},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1622176.1622190},
    doi = {10.1145/1622176.1622190},
    booktitle = {Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology},
    pages = {69–78},
    numpages = {10},
    keywords = {optimization, multi-touch, direct manipulation, constraints, pressure},
    venue = {Victoria, BC, Canada},
    series = {UIST ’09},
}


@inproceedings{Ren:2012,  
    author={Ren, Gang and O'Neill, Eamonn},  
    booktitle={2012 IEEE Symposium on 3D User Interfaces (3DUI)},   
    title={{3D Marking menu selection with freehand gestures}},   
    year={2012},  
    volume={},  
    number={},  
    pages={61-68},
    doi={10.1109/3DUI.2012.6184185},
}


@article{Ren:2017,  
    author={Donghao Ren and Saleema Amershi and Bongshin Lee and Jina Suh and Jason D. Williams},  
    journal={IEEE Transactions on Visualization and Computer Graphics},   
    title={{Squares: Supporting Interactive Performance Analysis for Multiclass Classifiers}},   
    year={2017},  
    volume={23},  
    number={1},  
    pages={61-70},
    url={https://ieeexplore.ieee.org/document/7539404},
    doi={10.1109/TVCG.2016.2598828},
}


@inproceedings{Ren:2020,
	title = {A {Dynamic} {Continuous} {Hand} {Gesture} {Detection} and {Recognition} {Method} with {FMCW} {Radar}},
	doi = {10.1109/ICCC49849.2020.9238935},
	abstract = {In this paper, a continuous dynamic hand gesture detection and recognition method is proposed using a frequency modulated continuous wave (FMCW) radar. Specifically, we collect the raw radar data to estimate the radar intermediate frequency (IF) signal, and construct the range-time map (RTM) and Doppler-time map (DTM) with 2-Dimensional Fast Fourier Transform (2D-FFT). Then, we propose a hand gesture detection method, which obtains the amplitude of the normalized hand gesture and uses a threshold to effectively segment the continuous hand gesture. Finally, the hand gesture is recognized by the proposed Fusion Dynamic Time Warping (FDTW) algorithm based on the central time-frequency trajectory. Experiments with radar data show that the accuracy of the proposed hand gesture detection method can reach 96.17\%, and compared with the traditional recognition algorithm, the proposed recognition algorithm can significantly improve the recognition accuracy rate (hand gesture average recognition accuracy rate can reach 94.50\%) with the time complexity reduced by more than 50\%.},
	booktitle = {2020 {IEEE}/{CIC} {International} {Conference} on {Communications} in {China} ({ICCC})},
	author = {Ren, Aihu and Wang, Yong and Yang, Xiaobo and Zhou, Mu},
	month = aug,
	year = {2020},
	note = {ISSN: 2377-8644},
	keywords = {continuous hand gesture recognition, detection, Doppler, FDTW, FMCW radar, real time process},
	pages = {1208--1213},
}


@inproceedings{Ren:2021, 
    author={Ren, Yuwei and Lu, Jiuyuan and Beletchi, Andrian and Huang, Yin and Karmanov, Ilia and Fontijne, Daniel and Patel, Chirag and Xu, Hao},  
    booktitle={Proc. of IEEE Wireless Communications and Networking Conference Workshops},
    series = {WCNCW '21},
    title={{Hand gesture recognition using 802.11ad mmWave sensor in the mobile device}},   
    year={2021},  
    volume={},  
    number={},  
    pages={1-6},  
    doi={10.1109/WCNCW49093.2021.9419978},
}


@inproceedings{Ritchie:2017,
	title = {{Hand gesture classification using 24 {GHz} {FMCW} dual polarised radar}},
	doi = {10.1049/cp.2017.0482},
	abstract = {This paper evaluates the classification performance of a dual polarised on receive, 24 GHz Frequency Modulated Continuous Wave (FMCW) radar system to autonomously identify micro-Doppler signatures of unique hand gestures. We employ an Eigen subspace feature selection technique on the calculated signal subspace in order to classify each gesture. Measurements using the dual polarised radar, permitting simultaneous recording of both the co-pol and cross-pol returns, are evaluated with this processing technique and results are reported herein. Our analysis displays the challenges presented by the high variance in individuals gestures and the limited additional information the cross polarised returns have provided to the classifier. Classification performance comparisons are presented when co, cross and dual polarised data are provided to the classifier. With this technique we achieve autonomous classification performance of up to 84.6\% when Eigenvalue derived features are used for classification.},
	booktitle = {International {Conference} on {Radar} {Systems} ({Radar} 2017)},
	author = {Ritchie, Matthew and Jones, A. and Brown, J. and Griffiths, Hugh D.},
	month = oct,
	year = {2017},
	keywords = {autonomous classification performance, Autonomy, Classification, cross-pol returns, CW radar, Doppler radar, dual polarised data, Eigen subspace feature selection technique, eigenvalues and eigenfunctions, electromagnetic wave polarisation, FM radar, FMCW dual polarised radar, FMCW Radar, frequency 24.0 GHz, Frequency Modulated Continuous Wave radar system, frequency modulation, gesture recognition, hand gesture classification, Machine Learning, Micro-Doppler, microDoppler signatures, unique hand gestures},
	pages = {1--6},
}


@article{Ritchie:2020,
	title = {Dop-{NET}: a micro-{Doppler} radar data challenge},
	volume = {56},
	issn = {0013-5194},
	doi = {10.1049/el.2019.4153},
	abstract = {Radar sensors have a new growing application area of dynamic hand gesture recognition. Traditionally radar systems are considered to be very large, complex and focused on detecting targets at long ranges. With modern electronics and signal processing it is now possible to create small compact RF sensors that can sense subtle movements over short ranges. For such applications, access to comprehensive databases of signatures is critical to enable the effective training of classification algorithms and to provide a common baseline for benchmarking purposes. This Letter introduces the Dop-NET radar micro-Doppler database and data challenge to the radar and machine learning communities. Dop-NET is a database of radar micro-Doppler signatures that are shareable and distributed with the purpose of improving micro-Doppler classification techniques. A continuous wave 24 GHz radar module is used to capture the first contributions to the Dop-NET database and classification results based on discriminating these hand gestures as shown.},
	number = {11},
	journal = {Electronics Letters},
	author = {Ritchie, Matthew and Capraru, Richard and Fioranelli, Francesco },
	year = {2020},
	keywords = {compact RF sensors, continuous wave radar module, CW radar, Dop-NET database, Dop-NET radar microDoppler database, Doppler radar, dynamic hand gesture recognition, frequency 24.0 GHz, learning (artificial intelligence), machine learning communities, microDoppler classification techniques, microDoppler radar data challenge, radar microDoppler signatures, radar sensors, radar signal processing, signal classification, signal processing},
	pages = {568--570},
}


@article{Rose:2012,
    author = {Rose, Susan and Clark, Moira and Samouel, Phillip and Hair, Neil},
    year = {2012},
    month = {06},
    pages = {308–322},
    title = {{Online Customer Experience in e-Retailing: An empirical model of Antecedents and Outcomes}},
    volume = {88},
    journal = {Journal of Retailing},
    doi = {10.1016/j.jretai.2012.03.001},
}


@InProceedings{Rovelo:2015,
    author="Rovelo, Gustavo
    and Degraen, Donald
    and Vanacken, Davy
    and Luyten, Kris
    and Coninx, Karin",
    editor="Abascal, Julio
    and Barbosa, Simone
    and Fetter, Mirko
    and Gross, Tom
    and Palanque, Philippe
    and Winckler, Marco",
    title={{Gestu-Wan - An Intelligible Mid-Air Gesture Guidance System for Walk-up-and-Use Displays}},
    booktitle="Human-Computer Interaction -- INTERACT 2015",
    year="2015",
    publisher="Springer International Publishing",
    address="Cham",
    pages="368--386",
    abstract="We present Gestu-Wan, an intelligible gesture guidance system designed to support mid-air gesture-based interaction for walk-up-and-use displays. Although gesture-based interfaces have become more prevalent, there is currently very little uniformity with regard to gesture sets and the way gestures can be executed. This leads to confusion, bad user experiences and users who rather avoid than engage in interaction using mid-air gesturing. Our approach improves the visibility of gesture-based interfaces and facilitates execution of mid-air gestures without prior training. We compare Gestu-Wan with a static gesture guide, which shows that it can help users with both performing complex gestures as well as understanding how the gesture recognizer works.",
    isbn="978-3-319-22668-2",
}




@inbook{Roy:2013,
    author = {Roy, Quentin and Malacria, Sylvain and Guiard, Yves and Lecolinet, Eric and Eagan, James},
    title = {{Augmented Letters: Mnemonic Gesture-Based Shortcuts}},
    year = {2013},
    isbn = {9781450318990},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2470654.2481321},
    abstract = {We propose Augmented Letters, a new technique aimed at augmenting gesture-based techniques such as Marking Menus [9] by giving them natural, mnemonic associations. Augmented Letters gestures consist of the initial of command names, sketched by hand in the Unistroke style, and affixed with a straight tail. We designed a tentative touch device interaction technique that supports fast interactions with large sets of commands, is easily discoverable, improves user's recall at no speed cost, and supports fluid transition from novice to expert mode. An experiment suggests that Augmented Letters outperform Marking Menu in terms of user recall.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2325–2328},
    numpages = {4},
}


@inproceedings{Rozman:2020,
	title = {Privacy-{Preserving} {Gesture} {Recognition} with {Explainable} {Type}-2 {Fuzzy} {Logic} {Based} {Systems}},
	doi = {10.1109/FUZZ48607.2020.9177768},
	abstract = {Smart homes are a growing market in need of privacy preserving sensors paired with explainable, interpretable and reliable control systems. The recent boom in Artificial Intelligence (AI) has seen an ever-growing persistence to incorporate it in all spheres of human life including the household. This growth in AI has been met with reciprocal concern for the privacy impacts and reluctance to introduce sensors, such as cameras, into homes. This concern has led to research of sensors not traditionally found in households, mainly short range radar. There has been also increasing awareness of AI transparency and explainability. Traditional AI black box models are not trusted, despite boasting high accuracy scores, due to the inability to understand what the decisions were based on. Interval Type-2 Fuzzy Logic offers a powerful alternative, achieving close to black box levels of performance while remaining completely interpretable. This paper presents a privacy preserving short range radar sensor coupled with an Explainable AI system employing a Big Bang Big Crunch (BB-BC) Interval Type-2 Fuzzy Logic System (FLS) to classify gestures performed in an indoor environment.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Fuzzy} {Systems} ({FUZZ}-{IEEE})},
	author={Rožman, Josip and Hagras, Hani and Perez, Javier Andreu and Clarke, Damien and Müller, Beate and Data, Steve Fitz},
	month = jul,
	year = {2020},
	note = {ISSN: 1558-4739},
	keywords = {artificial intelligence, Artificial intelligence, Big Bang - Big Crunch, big bang big crunch interval type-2 fuzzy logic system, cameras, data privacy, explainability, explainable AI system, Explainable Artificial Intelligence (XAI), explainable control systems, explainable type-2 fuzzy logic based systems, fuzzy logic, Fuzzy logic, fuzzy set theory, gesture recognition, growing market, high accuracy scores, human life, image classification, interpretable control systems, Privacy, privacy impacts, Privacy Preserving Sensing, privacy preserving sensors, privacy-preserving gesture recognition, Radar, reciprocal concern, reliable control systems, Sensors, short range radar sensor, smart homes, Three-dimensional displays, traditional AI black box models, Type-2 Fuzzy Logic, Uncertainty},
	pages = {1--8},
}


@inproceedings{Rubine:1991,
    author = {Rubine, Dean},
    title = {{Specifying Gestures by Example}},
    booktitle = {Proceedings of the 18th Annual Conference on Computer Graphics and Interactive Techniques},
    series = {SIGGRAPH '91},
    year = {1991},
    isbn = {0-89791-436-8},
    pages = {329--337},
    numpages = {9},
    url = {http://doi.acm.org/10.1145/122718.122753},
    doi = {10.1145/122718.122753},
    acmid = {122753},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {gesture, interaction techniques, statistical pattern recognition, user interface toolkits},
} 


@inproceedings{Ruiz:2011,
    author = {Ruiz, Jaime and Li, Yang and Lank, Edward},
    title = {{User-Defined Motion Gestures for Mobile Interaction}},
    year = {2011},
    isbn = {9781450302289},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1978942.1978971},
    doi = {10.1145/1978942.1978971},
    abstract = {Modern smartphones contain sophisticated sensors to monitor three-dimensional movement of the device. These sensors permit devices to recognize motion gestures - deliberate movements of the device by end-users to invoke commands. However, little is known about best-practices in motion gesture design for the mobile computing paradigm. To address this issue, we present the results of a guessability study that elicits end-user motion gestures to invoke commands on a smartphone device. We demonstrate that consensus exists among our participants on parameters of movement and on mappings of motion gestures onto commands. We use this consensus to develop a taxonomy for motion gestures and to specify an end-user inspired motion gesture set. We highlight the implications of this work to the design of smartphone applications and hardware. Finally, we argue that our results influence best practices in design for all gestural interfaces.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {197–206},
    numpages = {10},
    keywords = {mobile interaction, motion gestures, sensors},
    venue = {Vancouver, BC, Canada},
    series = {CHI '11},
}


@article{Ryu:2018,
	title = {Feature-{Based} {Hand} {Gesture} {Recognition} {Using} an {FMCW} {Radar} and its {Temporal} {Feature} {Analysis}},
	volume = {18},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2018.2859815},
	abstract = {In this paper, feature-based gesture recognition in a frequency modulated continuous wave (FMCW) radar system is introduced. We obtain a range-Doppler map (RDM) from raw signals of FMCW radar and generate a variety of features from the RDM. The features are broadly defined to reflect radar-specific characteristics as well as statistical values commonly used in machine learning. Among these radar features, those that are highly correlated with gesture recognition are selected by the proposed feature selection algorithm, which is a wrapper-based feature selection algorithm incorporated with a quantum-inspired evolutionary algorithm (QEA). Furthermore, the information factor based on the minimum redundancy maximum relevance criterion is applied to QEA in order to find feature subsets effectively. The proposed algorithm is able to extract from all feature sets feature subsets related to gesture recognition, and improves the gesture recognition accuracy of the FMCW radar system. In addition, we analyze which features of the radar are helpful for gesture recognition and perform effective gesture recognition using the features determined through feature analysis.},
	number = {18},
	journal = {IEEE Sensors Journal},
	author={Ryu, Si-Jung and Suh, Jun-Seuk and Baek, Seung-Hwan and Hong, Songcheol and Kim, Jong-Hwan},
	month = sep,
	year = {2018},
	keywords = {continuous wave radar system, CW radar, evolutionary algorithm, evolutionary computation, feature analysis, feature extraction, Feature extraction, feature selection, feature subsets, feature-based hand gesture recognition, FM radar, FMCW radar, FMCW radar system, gesture recognition, Gesture recognition, gesture recognition accuracy, learning (artificial intelligence), Machine learning, military computing, minimum redundancy maximum relevance criterion, quantum-inspired evolutionary algorithm, Radar, Radar antennas, radar features, radar-specific characteristics, range-Doppler map, RDM, Sensors, temporal feature, Two dimensional displays, wrapper-based feature selection algorithm},
	pages = {7593--7602},
}


%===========================================================
% References starting by S
%===========================================================
@inproceedings{Sadik:2017,
    author    = {Sadik, Ahmed R. and Urban, Bodo and Adel, Omar},
    booktitle = {Proceedings of the 3rd International Conference on Mechatronics and Robotics Engineering},
    title     = {{Using Hand Gestures to Interact with an Industrial Robot in a Cooperative Flexible Manufacturing Scenario}},
    year      = {2017},
    address   = {New York, NY, USA},
    pages     = {11–16},
    publisher = {Association for Computing Machinery},
    series    = {ICMRE 2017},
    doi       = {10.1145/3068796.3068801},
    isbn      = {9781450352802},
    keywords  = {Flexible Manufacturing Paradigm, Worker-Industrial Robot Cooperation, Semi-Autonomous Systems},
    venue  = {Paris, France},
    numpages  = {6},
    url       = {https://doi.org/10.1145/3068796.3068801},
}


@article{Saito:2015,
    author = {Saito, Priscila T. M. and Nakamura, Rodrigo Y. M. and Amorim, Willian P. and Papa, João P. and de Rezende, Pedro J. and Falcão, Alexandre X.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {{Choosing the Most Effective Pattern Classification Model under Learning-Time Constraint}},
    year = {2015},
    month = {06},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0129947},
    pages = {1-23},
    abstract = {Nowadays, large datasets are common and demand faster and more effective pattern analysis techniques. However, methodologies to compare classifiers usually do not take into account the learning-time constraints required by applications. This work presents a methodology to compare classifiers with respect to their ability to learn from classification errors on a large learning set, within a given time limit. Faster techniques may acquire more training samples, but only when they are more effective will they achieve higher performance on unseen testing sets. We demonstrate this result using several techniques, multiple datasets, and typical learning-time limits required by applications.},
    number = {6},
    doi = {10.1371/journal.pone.0129947},
}


@inproceedings{Sakamoto:2017,
	title = {{Radar-based hand gesture recognition using {I}-{Q} echo plot and convolutional neural network}},
	doi = {10.1109/CAMA.2017.8273461},
	abstract = {The present paper proposes a technique for the automatic recognition of hand gestures using a 2.4-GHz continuous radar and a convolutional neural network. The proposed technique applies the neural network to the time-domain I-Q plot of radar echoes. The accurate recognition capability of the technique was established with a set of radar data for three types of hand gesture performed by a participant. The radar echo trajectories were converted to low-resolution images to achieve fast signal processing, and the neural network was trained using the images. Another set of I-Q plot images were used for the evaluation of the recognition accuracy. The results indicate that the proposed technique is able to recognize hand gestures with accuracy exceeding 90\%.},
	booktitle = {2017 {IEEE} {Conference} on {Antenna} {Measurements} {Applications} ({CAMA})},
	author={Sakamoto, Takuya and Gao, Xiaomeng and Yavari, Ehsan and Rahman, Ashikur and Boric-Lubecke, Olga and Lubecke, Victor M.},
	month = dec,
	year = {2017},
	keywords = {Atmospheric measurements, automatic recognition, classification, continuous radar, convolutional neural network, echo, gesture recognition, Gesture recognition, hand gesture, hand gesture recognition, image resolution, low-resolution images, neural nets, Particle measurements, radar, radar computing, radar data, radar echo trajectories, radar echoes, radar imaging, Radar imaging, recognition accuracy, recognition capability, Sensors, signal processing, Time-domain analysis, time-domain I-Q plot},
	pages = {393--395},
}

@article{Sakamoto:2018,
	title = {Hand {Gesture} {Recognition} {Using} a {Radar} {Echo} {I}–{Q} {Plot} and a {Convolutional} {Neural} {Network}},
	volume = {2},
	issn = {2475-1472},
	doi = {10.1109/LSENS.2018.2866371},
	abstract = {We propose a hand gesture recognition technique using a convolutional neural network applied to radar echo inphase/quadrature (I/Q) plot trajectories. The proposed technique is demonstrated to accurately recognize six types of hand gestures for ten participants. The system consists of a low-cost 2.4-GHz continuous-wave monostatic radar with a single antenna. The radar echo trajectories are converted to low-resolution images and are used for the training and evaluation of the proposed technique. Results indicate that the proposed technique can recognize hand gestures with average accuracy exceeding 90\%.},
	number = {3},
	journal = {IEEE Sensors Letters},
	author={Sakamoto, Takuya and Gao, Xiaomeng and Yavari, Ehsan and Rahman, Ashikur and Boric-Lubecke, Olga and Lubecke, Victor M.},
	month = sep,
	year = {2018},
	keywords = {Atmospheric measurements, continuous-wave monostatic radar, convolutional neural network, CW radar, echo, feedforward neural nets, frequency 2.4 GHz, gesture recognition, Gesture recognition, hand gesture recognition technique, low-resolution images, machine learning, neural network, Particle measurements, radar, radar antenna, radar antennas, Radar antennas, radar echo I-Q plot, radar echo inphase-quadrature plot trajectories, radar echo trajectories, Radar imaging, radar resolution, Sensor signals processing, Training, UHF antennas},
	pages = {1--4},
}


@inproceedings{Sangiorgi:2012,
    author    = {Sangiorgi, Ugo Braga and Beuvens, Fran{\c{c}}ois and Vanderdonckt, Jean},
    title     = {{User interface design by collaborative sketching}},
    booktitle = {Proc. of ACM Int. COnf. on Designing Interactive Systems},
    series    = {{DIS} '12},
    venue  = {Newcastle  Upon Tyne, United Kingdom},
    dates     = {June 11-15, 2012},
    pages     = {378--387},
    publisher = {{ACM}},
    year      = {2012},
    url       = {https://doi.org/10.1145/2317956.2318013},
    doi       = {10.1145/2317956.2318013},
    timestamp = {Tue, 06 Nov 2018 16:58:03 +0100},
    biburl    = {https://dblp.org/rec/conf/ACMdis/SangiorgiBV12.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Santhalingam:2020a,
	title = {Expressive {ASL} {Recognition} using {Millimeter}-wave {Wireless} {Signals}},
	doi = {10.1109/SECON48991.2020.9158441},
	abstract = {Over half a million people in the United States use American Sign Language (ASL) as their primary mode of communication. Automatic ASL recognition would enable Deaf and Hard of Hearing (DHH) users to interact with others who are not familiar with ASL as well as voice-controlled digital assistants (e.g., Alexa, Siri, etc.). While ASL recognition has been extensively studied, there is a little attention given to recognition of ASL non-manual body markers. The non-manual markers are typically expressed through head, torso and shoulder movements, and add essential meaning and context to the signed sentences. In this work, we present ExASL, a sentence-level ASL recognition system using millimeter-wave radars. ExASL can recognize manual markers (hand gestures) and non-manual markers (head and torso movements). It utilizes multi-distance clustering to recognize body parts and cluster mmWave point clouds. We then present a multi-view deep learning algorithm that can learn from clustered body part representation for an expressive sentence-level recognition. Our evaluation shows that ExASL can recognize ASL sentences with a word error rate of 0.79\%, sentence error rate of 1.25\%, and non-manual markers with an accuracy of 83.5\%.},
	booktitle = {2020 17th {Annual} {IEEE} {International} {Conference} on {Sensing}, {Communication}, and {Networking} ({SECON})},
	author={Santhalingam, Panneer Selvam and Du, Yuanqi and Wilkerson, Riley and Hosain, Al Amin and Zhang, Ding and Pathak, Parth and Rangwala, Huzefa and Kushalnagar, Raja},
	month = jun,
	year = {2020},
	note = {ISSN: 2155-5494},
	keywords = {American Sign Language, ASL nonmanual body markers, ASL sentences, automatic ASL recognition, body parts, Chirp, clustered body part representation, ExASL, expressive ASL recognition, expressive sentence-level recognition, feature extraction, gesture recognition, half a million people, handicapped aids, knowledge based systems, learning (artificial intelligence), Machine learning, manual markers, Manuals, millimeter-wave radars, millimeter-wave wireless signals, nonmanual markers, Radar, Sensors, sentence-level ASL recognition, shoulder movements, signed sentences, Three-dimensional displays, Torso, torso movements, United States, voice-controlled digital assistants},
	pages = {1--9},
}


@article{Santurkar:2018,
    title={{How does batch normalization help optimization?}},
    author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
    journal={Advances in neural information processing systems},
    volume={31},
    year={2018},
}


@book{Sauro:2010,
  title={{A practical guide to measuring usability: 72 answers to the most common questions about quantifying the usability of websites and software}},
  author={Sauro, Jeff},
  year={2010},
  publisher={Measuring Usability LCC},
}  


@article{Sawilowsky:2003, 
    author={Sawilowsky, Shlomo S.}, 
    journal={Journal of Modern Applied Statistical Method},  
    title={{A Different Future For Social And Behavioral Science Research}},   
    year={2003},  
    volume={2},
    number={1},
    url={https://digitalcommons.wayne.edu/jmasm/vol2/iss1/11/},
    doi={10.22237/jmasm/1051747860},
    pages={128--132},
}


@article{Santhalingam:2020b,
	title = {{mmASL}: {Environment}-{Independent} {ASL} {Gesture} {Recognition} {Using} 60 {GHz} {Millimeter}-wave {Signals}},
	volume = {4},
	shorttitle = {{mmASL}},
	url = {https://doi.org/10.1145/3381010},
	doi = {10.1145/3381010},
	abstract = {Home assistant devices such as Amazon Echo and Google Home have become tremendously popular in the last couple of years. However, due to their voice-controlled functionality, these devices are not accessible to Deaf and Hard-of-Hearing (DHH) people. Given that over half a million people in the United States communicate using American Sign Language (ASL), there is a need of a home assistant system that can recognize ASL. The objective of this work is to design a home assistant system for DHH users (referred to as mmASL) that can perform ASL recognition using 60 GHz millimeter-wave wireless signals. mmASL has two important components. First, it can perform reliable wake-word detection using spatial spectrograms. Second, using a scalable and extensible multi-task deep learning model, mmASL can learn the phonological properties of ASL signs and use them to accurately recognize the ASL signs. We implement mmASL on 60 GHz software radio platform with phased array, and evaluate it using a large-scale data collection from 15 signers, 50 ASL signs and over 12K sign instances. We show that mmASL is tolerant to the presence of other interfering users and their activities, change of environment and different user positions. We compare mmASL with a well-studied Kinect and RGB camera based ASL recognition systems, and find that it can achieve a comparable performance (87\% average accuracy of sign recognition), validating the feasibility of using 60 GHz mmWave system for ASL sign recognition.},
	number = {1},
	urldate = {2020-12-21},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Santhalingam, Panneer Selvam and Hosain, Al Amin and Zhang, Ding and Pathak, Parth and Rangwala, Huzefa and Kushalnagar, Raja},
	month = mar,
	year = {2020},
	keywords = {60 GHz milli-meter wave wireless, accessible computing, gesture recognition, personal digital assistants, sign language recognition},
	pages = {26:1--26:30},
}


@InProceedings{Schak:2019,
    author="Schak, Monika and Gepperth, Alexander",
    editor="Tetko, Igor V.
    and K{\r{u}}rkov{\'a}, V{\v{e}}ra
    and Karpov, Pavel
    and Theis, Fabian",
    title="A Study on Catastrophic Forgetting in Deep LSTM Networks",
    booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2019: Deep Learning",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="714--728",
    abstract="We present a systematic study of Catastrophic Forgetting (CF), i.e., the abrupt loss of previously acquired knowledge, when retraining deep recurrent LSTM networks with new samples. CF has recently received renewed attention in the case of feed-forward DNNs, and this article is the first work that aims to rigorously establish whether deep LSTM networks are afflicted by CF as well, and to what degree. In order to test this fully, training is conducted using a wide variety of high-dimensional image-based sequence classification tasks derived from established visual classification benchmarks (MNIST, Devanagari, FashionMNIST and EMNIST). We find that the CF effect occurs universally, without exception, for deep LSTM-based sequence classifiers, regardless of the construction and provenance of sequences. This leads us to conclude that LSTMs, just like DNNs, are fully affected by CF, and that further research work needs to be conducted in order to determine how to avoid this effect (which is not a goal of this study).",
    isbn="978-3-030-30484-3",
}


@inproceedings{Schiettecatte:2008,
    author    = {Schiettecatte, Bert and Vanderdonckt, Jean},
    editor    = {Schmidt, Albrecht and Gellersen, Hans and van den Hoven, Elise and Mazalek, Ali and Holleis, Paul and Villar, Nicolas},
    title     = {{AudioCubes: a distributed cube tangible interface based on interaction range for sound design}},
    booktitle = {Proceedings of the 2nd International Conference on Tangible and Embedded
               Interaction},
    series    = {TEI 2008},
    venue  = {Bonn, Germany},
    dates     = {February 18-20, 2008},
    pages     = {3--10},
    publisher = {{ACM}},
    year      = {2008},
    url       = {https://doi.org/10.1145/1347390.1347394},
    doi       = {10.1145/1347390.1347394},
    timestamp = {Tue, 06 Nov 2018 11:07:48 +0100},
    biburl    = {https://dblp.org/rec/conf/tei/SchiettecatteV08.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Schioppo:2019,
    author    = {Schioppo, Jacob and Meyer, Zachary and Fabiano, Diego and Canavan, Shaun},
    booktitle = {Extended Abstracts of the ACM Conference on Human Factors in Computing Systems},
    title     = {{Sign Language Recognition: Learning American Sign Language in a Virtual Environment}},
    year      = {2019},
    address   = {New York, NY, USA},
    pages     = {1–6},
    publisher = {Association for Computing Machinery},
    series    = {CHI EA '19},
    doi       = {10.1145/3290607.3313025},
    isbn      = {9781450359719},
    keywords  = {classification, sign language, gesture, virtual reality},
    venue  = {Glasgow, Scotland Uk},
    numpages  = {6},
    url       = {https://doi.org/10.1145/3290607.3313025},
}


@article{Schrepp:2017,
    author = {Schrepp, Martin and Hinderks, Andreas and Thomaschewski, Jörg},
    year = {2017},
    month = {06},
    pages = {40-44},
    title = {{Construction of a Benchmark for the User Experience Questionnaire (UEQ)}},
    volume = {4},
    journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
    doi = {10.9781/ijimai.2017.445},
}


@article{Schrepp:2019,
    author = {Schrepp, Martin and Thomaschewski, Jörg},
    year = {2019},
    month = {12},
    pages = {1},
    title = {{Design and Validation of a Framework for the Creation of User Experience Questionnaires}},
    volume = {InPress},
    journal = {International Journal of Interactive Multimedia and Artificial Intelligence},
    doi = {10.9781/ijimai.2019.06.006},
}


@inproceedings{Sethu:2013,  
    author={Sethu Janaki, V. M. and Babu, Satish and Sreekanth, Sarath}, 
    booktitle={2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)},  
    title={{Real time recognition of 3D gestures in mobile devices}},   
    year={2013},  
    volume={},  
    number={},  
    pages={149-152},
    doi={10.1109/RAICS.2013.6745463},
}


@techreport{Sheng:2004,
    author = {Sheng, Jia},
    title = {{A Study of AdaBoost in 3D Gesture Recognition}},
    institution = {Department of Computer Science, University of Toronto},
    year = {2004},
    type= {technical report},
    number= {CSC2515},
    url={http://www.dgp.toronto.edu/~jsheng/doc/CSC2515/Report.pdf},
}


@article{Shneiderman:1992,
    author = {Shneiderman, Ben},
    title = {{Tree Visualization with Tree-Maps: 2-d Space-Filling Approach}},
    year = {1992},
    issue_date = {Jan. 1992},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {11},
    number = {1},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/102377.115768},
    doi = {10.1145/102377.115768},
    journal = {ACM Trans. Graph.},
    month = jan,
    pages = {92–99},
    numpages = {8},
}


@article{Shorten:2019,
    title={{A survey on image data augmentation for deep learning}},
    author={Shorten, Connor and Khoshgoftaar, Taghi M},
    journal={Journal of big data},
    volume={6},
    number={1},
    pages={1--48},
    year={2019},
    publisher={SpringerOpen},
}


@inproceedings{Siddhpuria:2017,
    author = {Siddhpuria, Shaishav and Katsuragawa, Keiko and Wallace, James R. and Lank, Edward},
    title = {{Exploring At-Your-Side Gestural Interaction for Ubiquitous Environments}},
    year = {2017},
    isbn = {9781450349222},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3064663.3064695},
    doi = {10.1145/3064663.3064695},
    abstract = {Free-space gestural systems are faced with two major issues: a lack of subtlety due to explicit mid-air arm movements, and the highly effortful nature of such interactions. With an ever-growing ubiquity of interactive devices, displays, and appliances with non-standard interfaces, lower-effort and more socially acceptable interaction paradigms are essential. To address these issues, we explore at-one's-side gestural input. Within this space, we present the results of two studies that investigate the use of side-gesture input for interaction. First, we investigate end-user preference through a gesture elicitation study, present a gesture set, and validate the need for dynamic, diverse, and variable-length gestures. We then explore the feasibility of designing such a gesture recognition system, dubbed WatchTrace, which supports alphanumeric gestures of up to length three with an average accuracy of up to 82\%, providing a rich, dynamic, and feasible gestural vocabulary.},
    booktitle = {Proceedings of the 2017 Conference on Designing Interactive Systems},
    pages = {1111–1122},
    numpages = {12},
    keywords = {ubiquitous computing, gestures, large displays, smartwatch},
    venue = {Edinburgh, United Kingdom},
    series = {DIS '17},
}


@inproceedings{Siean:2022,
    author = {Siean, Alexandru-Ionut and Pampar\u{a}u, Cristian and Vatavu, Radu-Daniel},
    title = {{Scenario-Based Exploration of Integrating Radar Sensing into Everyday Objects for Free-Hand Television Control}},
    year = {2022},
    isbn = {9781450392129},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3505284.3532982},
    doi = {10.1145/3505284.3532982},
    abstract = {We address gesture input for TV control, for which we examine mid-air free-hand interactions that can be detected via radar sensing. We adopt a scenario-based design approach to explore possible locations from the living room where to integrate radar sensors, e.g., in the TV set, the couch armrest, or the user’s smartphone, and we contribute a four-level taxonomy of locations relative to the TV set, the user, personal robot assistants, and the living room environment, respectively. We also present preliminary results about an interactive system using a 15-antenna ultra-wideband 3D radar, for which we implemented a dictionary of six directional swipe gestures for the control of dichotomous TV system functions.},
    booktitle = {Proceedings of ACM International Conference on Interactive Media Experiences},
    pages = {357–362},
    numpages = {6},
    keywords = {mid-air gestures, free-hand input, Gesture input, remote control, TV},
    venue = {Aveiro, JB, Portugal},
    series = {IMX '22},
}


@article{Siean:2023,
    author = {Siean, Alexandru-Ionut and Pampar\u{a}u, Cristian and Slu\"{y}ters, Arthur and Vatavu, Radu-Daniel and Vanderdonckt, Jean},
    title = {{Flexible gesture input with radars: systematic literature review and taxonomy of radar sensing integration in ambient intelligence environments}},
    journal = {Journal of Ambient Intelligence and Humanized Computing},
    year  = {2023},
    publisher = {Springer},
    doi = {10.1007/s12652-023-04606-9},
    URL = {https://link.springer.com/article/10.1007/s12652-023-04606-9},
}


@inproceedings{Signer:2007,
    author    = {Signer, Beat and Kurmann, Ueli and Norrie, Moira C.},
    title     = {{iGesture: A General Gesture Recognition Framework}},
    booktitle = {9th International Conference on Document Analysis and Recognition
               {(ICDAR} 2007), 23-26 September, Curitiba, Paran{\'{a}}, Brazil},
    pages     = {954--958},
    publisher = {{IEEE} Computer Society},
    year      = {2007},
    url       = {https://doi.org/10.1109/ICDAR.2007.4377056},
    doi       = {10.1109/ICDAR.2007.4377056},
    timestamp = {Wed, 16 Oct 2019 14:14:54 +0200},
    biburl    = {https://dblp.org/rec/conf/icdar/SignerKN07.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@TECHREPORT{Signer:2011,
	series = {Technical Report / ETH Zurich, Department of Computer Science},
	volume = {561},
	author = {Signer, Beat and Norrie, Moira C. and Kurmann, Ueli},
	publisher = {ETH, Department of Computer Science},
	year = {2011},
	language = {en},
	copyright = {In Copyright - Non-Commercial Use Permitted},
	keywords = {PROGRAMMING ENVIRONMENTS + COMPILERS (SOFTWARE PRODUCTS); PROGRAMMIERUMGEBUNGEN + COMPILER (SOFTWAREPRODUKTE); BEWEGUNGSWAHRNEHMUNG (COMPUTERVISION); MOTION PERCEPTION (COMPUTER VISION); JAVA (PROGRAMMING LANGUAGES); JAVA (PROGRAMMIERSPRACHEN)},
	size = {11 p.},
	address = {Zürich},
	DOI = {10.3929/ethz-a-006787267},
	title = {{iGesture. A Java framework for the development and deployment of stoke-based online Gesture recognition algorithms}},
	Note = {Technical Reports D-INFK.},
}


@inproceedings{Simos:2016,
  author    = {Simos, Merkourios and Nikolaidis, Nikolaos},
  booktitle = {Proceedings of the 9th Hellenic Conference on Artificial Intelligence},
  title     = {{Greek Sign Language Alphabet Recognition Using the Leap Motion Device}},
  year      = {2016},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SETN '16},
  articleno = {34},
  doi       = {10.1145/2903220.2903249},
  isbn      = {9781450337342},
  keywords  = {Greek Sign language Recognition, Support Vector Machines, Leap Motion},
  venue  = {Thessaloniki, Greece},
  numpages  = {4},
  url       = {https://doi.org/10.1145/2903220.2903249},
}


@incollection{Simske:2019,
    title = "Chapter 4 - Meta-analytic design patterns",
    editor = "Simske, Steven",
    booktitle = "Meta-Analytics",
    publisher = "Morgan Kaufmann",
    pages = "147 - 185",
    year = "2019",
    isbn = "978-0-12-814623-1",
    doi = "https://doi.org/10.1016/B978-0-12-814623-1.00004-6",
    url = "http://www.sciencedirect.com/science/article/pii/B9780128146231000046",
    author = "Steven Simske",
    keywords = "Classification, Clustering, Confusion matrix, Co-occurrence, Cumulative response, Decision tree, Design patterns, Entropy, Expectation-maximization, Independence, Model agreement, Natural language processing, NLP, Putative identity, Regression, Sensitivity analysis, Similarity",
    abstract = "Meta-analytics are, in short, patterns for approaching modern data tasks. In this chapter, a broad set of design patterns relevant to data analytics are overviewed. In later chapters, many of these will be described in more rigor and applied to specific problems; however, the intent of this chapter is to provide a sweeping overview of the depth and breadth of meta-analytic design patterns to drive home the cheering message that the large repertoire of patterns to choose from virtually guarantees that a better system can be built than any single existing analytic approach can provide. Armed with the ground truthing and experimental design techniques we have already learned, we end this chapter with a powerful panoply of approaches to extracting an optimal set of information from a given data set.",
}


@article{Skaria:2019,
	title = {Hand-{Gesture} {Recognition} {Using} {Two}-{Antenna} {Doppler} {Radar} {With} {Deep} {Convolutional} {Neural} {Networks}},
	volume = {19},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2019.2892073},
	abstract = {Low-cost consumer radar integrated circuits combined with recent advances in machine learning have opened up a range of new possibilities in smart sensing. In this paper, we use a miniature radar sensor to capture Doppler signatures of 14 different hand gestures and train a deep convolutional neural network (DCNN) to classify these captured gestures. We utilize two receiving antennas of a continuous-wave Doppler radar capable of producing the in-phase and quadrature components of the beat signals. We map these two beat signals into three input channels of a DCNN as two spectrograms and an angle of arrival matrix. The classification results of the proposed architecture show a gesture classification accuracy exceeding 95\% and a very low confusion between different gestures. This is almost 10\% improvement over the single-channel Doppler methods reported in the literature.},
	number = {8},
	journal = {IEEE Sensors Journal},
	author={Skaria, Sruthy and Al-Hourani, Akram and Lech, Margaret and Evans, Robin J.},
	month = apr,
	year = {2019},
	keywords = {beat signals, captured gestures, continuous-wave Doppler radar, convolutional neural nets, DCNN, deep convolutional neural network, deep convolutional neural networks, Doppler effect, Doppler radar, Doppler signatures, feature extraction, gesture classification accuracy, gesture recognition, Gesture recognition, hand-gesture recognition, image classification, learning (artificial intelligence), low-cost consumer radar integrated circuits, machine learning, millimeter-wave radar, miniature radar sensor, multi-antenna radar, quadrature components, radar antennas, radar imaging, Radar sensors, radar signal processing, receiving antennas, Receiving antennas, Sensors, single-channel Doppler methods, smart sensing, Spectrogram, two-antenna doppler radar},
	pages = {3041--3048},
}


@article{Skaria:2020a,
	title = {Deep-{Learning} {Methods} for {Hand}-{Gesture} {Recognition} {Using} {Ultra}-{Wideband} {Radar}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3037062},
	abstract = {Using deep-learning techniques for analyzing radar signatures has opened new possibilities in the field of smart-sensing, especially in the applications of hand-gesture recognition. In this paper, we present a framework, using deep-learning techniques, to classify hand-gesture signatures generated from an ultra-wideband (UWB) impulse radar. We extract the signals of 14 different hand-gestures and represent each signature as a 3-dimensional tensor consisting of range-Doppler frame sequence. These signatures are passed to a convolutional neural network (CNN) to extract the unique features of each gesture, and are then fed to a classifier. We compare 4 different classification architectures to predict the gesture class, namely; (i) fully connected neural network (FCNN), (ii) \$k\$ -Nearest Neighbours ( \$k\$ -NN), (iii) support vector machine (SVM), (iv) long short term memory (LSTM) network. The shape of the range-Doppler-frame tensor and the parameters of the classifiers are optimized in order to maximize the classification accuracy. The classification results of the proposed architectures show a high level of accuracy above 96 \% and a very low confusion probability even between similar gestures.},
	journal = {IEEE Access},
	author={Skaria, Sruthy and Al-Hourani, Akram and Evans, Robin J.},
	year = {2020},
	keywords = {deep-learning, Doppler radar, Feature extraction, Hand-gesture recognition, Radar, Radar antennas, radar sensors, radar signal processing, Sensors, Tensors, Ultra wideband radar, UWB impulse radar},
	pages = {203580--203590},
}


@inproceedings{Skaria:2020b,
	title = {Deep-{Learning} for {Hand}-{Gesture} {Recognition} with {Simultaneous} {Thermal} and {Radar} {Sensors}},
	doi = {10.1109/SENSORS47125.2020.9278683},
	abstract = {In this paper, we present a framework for integrating two different types of sensors for hand-gesture recognition using deep-learning. The two sensors utilize completely different approaches for detecting the signal, namely; an ultra-wideband (UWB) impulse radar sensor and a thermal sensor. For robust gesture classification two parallel paths are utilized, each employs a combination of a convolutional neural network (CNN) and a long short-term memory (LSTM) network on both the radar signal and the thermal signal. The classification results from the two paths are then fused to improve the overall detection probability. The two sensors compliment the capability of each other; while the UWB radar is accurate for radial movement and less accurate for lateral movement, the thermal sensor is vice-versa. Thus, we find that combining both sensors produces near perfect classification accuracy of 99 \% for 14 different hand-gestures.},
	booktitle = {2020 {IEEE} {Sensors}},
	author={Skaria, Sruthy and Huang, Da and Al-Hourani, Akram and Evans, Robin J. and Lech, Margaret},
	month = oct,
	year = {2020},
	note = {ISSN: 2168-9229},
	keywords = {Conferences, Doppler radar, Radar, Radar detection, Sensors, Thermal sensors, Ultra wideband radar},
	pages = {1--4},
}


@article{Slob:2002,
    Author = {Slob, Evert and Fokkema, Jacob},
    Title = {{Coupling effects of two electric dipoles on an interface}},
    Journal = {Radio Science},
    Volume = {37},
    Number = {5},
    Pages = {1--10},
    doi = {10.1029/2001RS2529},
    Year = {2002}, 
}


@article{Slob:2010,
    author = {Slob, Evert and Sato, Motoyuki and Olhoeft, Gary},
    title = {{Surface and borehole ground-penetrating-radar developments}},
    journal = {Geophysics},
    volume = {75},
    number = {5},
    pages = {75A103-75A120},
    year = {2010},
    doi = {10.1190/1.3480619},
    URL = {https://doi.org/10.1190/1.3480619},
    eprint = {https://doi.org/10.1190/1.3480619},
}


@article{Sluyters:2022:EICS,
    author    = {Slu\"{y}ters, Arthur and Ousmer, Mehdi and Roselli, Paolo and Vanderdonckt, Jean},
    title     = {{QuantumLeap, a Framework for Engineering Gestural User Interfaces based on the Leap Motion Controller}},
    journal   = {Proc. {ACM} Human-Computer Interaction},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume    = {6},
    number    = {{EICS}},
    pages     = {1--47},
    year      = {2022},
    articleno = {161},
    numpages = {47},
    month = jun,
    url       = {https://doi.org/10.1145/3532211},
    doi       = {10.1145/3532211},
}


@inproceedings{Sluyters:2022:IUI,
    author = {Slu\"{y}ters, Arthur and Lambot, S\'{e}bastien and Vanderdonckt, Jean},
    title = {{Hand Gesture Recognition for an Off-the-Shelf Radar by Electromagnetic Modeling and Inversion}},
    year = {2022},
    isbn = {9781450391443},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3490099.3511107},
    doi = {10.1145/3490099.3511107},
    abstract = {Microwave radar sensors in human-computer interactions have several advantages compared to wearable and image-based sensors, such as privacy preservation, high reliability regardless of the ambient and lighting conditions, and larger field of view. However, the raw signals produced by such radars are high-dimension and relatively complex to interpret. Advanced data processing, including machine learning techniques, is therefore necessary for gesture recognition. While these approaches can reach high gesture recognition accuracy, using artificial neural networks requires a significant amount of gesture templates for training and calibration is radar-specific. To address these challenges, we present a novel data processing pipeline for hand gesture recognition that combines advanced full-wave electromagnetic modelling and inversion with machine learning. In particular, the physical model accounts for the radar source, radar antennas, radar-target interactions and target itself, i.e.,, the hand in our case. To make this processing feasible, the hand is emulated by an equivalent infinite planar reflector, for which analytical Green’s functions exist. The apparent dielectric permittivity, which depends on the hand size, electric properties, and orientation, determines the wave reflection amplitude based on the distance from the hand to the radar. Through full-wave inversion of the radar data, the physical distance as well as this apparent permittivity are retrieved, thereby reducing by several orders of magnitude the dimension of the radar dataset, while keeping the essential information. Finally, the estimated distance and apparent permittivity as a function of gesture time are used to train the machine learning algorithm for gesture recognition. This physically-based dimension reduction enables the use of simple gesture recognition algorithms, such as template-matching recognizers, that can be trained in real time and provide competitive accuracy with only a few samples. We evaluate significant stages of our pipeline on a dataset of 16 gesture classes, with 5 templates per class, recorded with the Walabot, a lightweight, off-the-shelf array radar. We also compare these results with an ultra wideband radar made of a single horn antenna and lightweight vector network analyzer, and a Leap Motion Controller.},
    booktitle = {Proceedings of 27th International ACM Conference on Intelligent User Interfaces},
    pages = {506–522},
    numpages = {17},
    keywords = {Gesture-based interfaces, Radar-based interaction., Mid-air gestural interaction, Dimension reduction, New datasets, Hand gesture recognition},
    venue = {Helsinki, Finland},
    dates = {22-25 March 2022},
    series = {IUI '22},
}


@article{Sluyters:2022:LUI,
    author = {Slu\"{y}ters, Arthur and Sellier, Quentin and Vanderdonckt, Jean and Parthiban, Vik and Maes, Pattie},
    title = {{Consistent, Continuous, and Customizable Mid-Air Gesture Interaction for Browsing Multimedia Objects on Large Displays}},
    journal = {International Journal of Human–Computer Interaction},
    pages = {1-32},
    year  = {2022},
    publisher = {Taylor \& Francis},
    doi = {10.1080/10447318.2022.2078464},
    URL = {https://doi.org/10.1080/10447318.2022.2078464},
    eprint = {https://doi.org/10.1080/10447318.2022.2078464},
}


@article{Sluyters:2023,
    author = {Slu\"{y}ters, Arthur and Lambot, S\'{e}bastien and Vanderdonckt, Jean and Vatavu, Radu-Daniel},
    title = {{RadarSense: Accurate Recognition of Mid-Air Hand Gestures with Radar Sensing and Few Training Examples}},
    year = {2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    issn = {2160-6455},
    url = {https://doi.org/10.1145/3589645},
    doi = {10.1145/3589645},
    abstract = {Microwave radars bring many benefits to mid-air gesture sensing due to their large field of view and independence from environmental conditions, such as ambient light and occlusion. However, radar signals are highly dimensional and usually require complex deep learning approaches. To understand this landscape, we report results from a systematic literature review of (N = 118) scientific papers on radar sensing, unveiling a large variety of radar technology of different operating frequencies and bandwidths, antenna configurations, but also various gesture recognition techniques. Although highly accurate, these techniques require a large amount of training data that depend on the type of radar. Therefore, the training results cannot be easily transferred to other radars. To address this aspect, we introduce a new gesture recognition pipeline that implements advanced full-wave electromagnetic modeling and inversion to retrieve physical characteristics of gestures that are radar independent, i.e., independent of the source, antennas, and radar-hand interactions. Inversion of radar signals further reduces the size of the dataset by several orders of magnitude, while preserving the essential information. This approach is compatible with conventional gesture recognizers, such as those based on template matching, which only need a few training examples to deliver high recognition accuracy rates. To evaluate our gesture recognition pipeline, we conducted user-dependent and user-independent evaluations on a dataset of 16 gesture types collected with the Walabot, a low-cost off-the-shelf array radar. We contrast these results with those obtained for the same gesture types collected with an ultra-wideband radar made of a vector network analyzer with a single horn antenna and with a computer vision sensor, respectively. Based on our findings, we suggest some design implications to support future development in radar-based gesture recognition.},
    journal = {ACM Transactions on Interactive Intelligent Systems},
    month = mar,
    keywords = {Hand gesture recognition, Dimension reduction, Radar-based interaction.},
}


@article{Smith:2018,
	title = {Gesture {Recognition} {Using} mm-{Wave} {Sensor} for {Human}-{Car} {Interface}},
	volume = {2},
	issn = {2475-1472},
	doi = {10.1109/LSENS.2018.2810093},
	abstract = {This article details the development of a gesture recognition technique using a mm-wave radar sensor for in-car infotainment control. Gesture recognition is becoming a more prominent form of human-computer interaction and can be used in the automotive industry to provide a safe and intuitive control interface that will limit driver distraction. We use a 60 GHz mm-wave radar sensor to detect precise features of fine motion. Specific gesture features are extracted and used to build a machine learning engine that can perform real-time gesture recognition. This article discusses the user requirements and in-car environmental constraints that influenced design decisions. Accuracy results of the technique are presented, and recommendations for further research and improvements are made.},
	number = {2},
	journal = {IEEE Sensors Letters},
	author={Smith, Karly A. and Csech, Clément and Murdoch, David and Shaker, George},
	month = jun,
	year = {2018},
	keywords = {60 GHz mm-wave radar, 60 GHz mm-wave radar sensor, Automobiles, automotive industry, Cameras, feature extraction, gesture feature extraction, gesture recognition, gesture sensing, human-car interface, human-computer interaction, in-car environmental constraints, in-car infotainment control, intuitive control interface, learning (artificial intelligence), machine learning, machine learning engine, Microwave/millimeter wave sensors, mm-wave sensor, Radar detection, random forest classifier, real-time gesture recognition, safe control interface, Sensor systems, traffic engineering computing, user interfaces},
	pages = {1--4},
}


@article{Soldovieri:2011,
    author    = {Soldovieri, Francesco and Lopera, Olga and Lambot, S{\'{e}}bastien},
    title     = {{Combination of Advanced Inversion Techniques for an Accurate Target Localization via GPR for Demining Applications}},
    journal   = {{IEEE} Trans. Geosci. Remote. Sens.},
    volume    = {49},
    number    = {1-2},
    pages     = {451--461},
    year      = {2011},
    url       = {https://doi.org/10.1109/TGRS.2010.2051675},
    doi       = {10.1109/TGRS.2010.2051675},
    timestamp = {Tue, 12 May 2020 16:46:55 +0200},
    biburl    = {https://dblp.org/rec/journals/tgrs/SoldovieriLL11.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@book{Spiegelmock:2013,
    title = {{Leap Motion Development Essentials}},
    author = {Spiegelmock, Mischa},
    publisher = {Packt Publishing},
    isbn = {978-1849697729},
    year = {2013},
    month = oct,
    series = {},
    edition = {1},
    volume = {},
    url = {https://www.packtpub.com/eu/hardware-and-creative/leap-motion-development-essentials},
}


@article{Srivastava:2014,
    title={{Dropout: a simple way to prevent neural networks from overfitting}},
    author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    journal={The journal of machine learning research},
    volume={15},
    number={1},
    pages={1929--1958},
    year={2014},
    publisher={JMLR. org},
}


@phdthesis{Steeman:2022,
    title = {{Uni/Bi-manual wrist gesture interaction for one/two users : application to Kinemic}},
    author = {Steeman, Sarah and Chauvaux, Chloé},
    abstract = {Wrist-based gesture interaction enables end-users to interact by mid-air gestures while keeping hands and fingers free for other manual tasks. This thesis aims to investigate how wrist-based gesture interaction could be made feasible with one wrist (unimanual) or two wrists (bimanual) for one end-user (in isolation) or two (in collaboration) and how to integrate it into an interactive application. For this purpose, we adopted a development life cycle made up of the following stages: design of a set of gestures from different sources (elicitation and literature survey), acquire a dataset of these gestures to train a recognizer, compare several recognizers on this dataset with various conditions, and mapping gestures to commands into an interactive application so that they can be handled in real time. This life cycle is applied specifically to the Kinemic device and for a computer-assisted presentation application.},
    Keywords = {Wrist-based gesture , Gesture interaction , Kinemic , Human computer interaction},
    language = {Anglais},
    year = {2022},
    url = {http://hdl.handle.net/2078.1/thesis:35604},
    school = {UCL - Ecole polytechnique de Louvain},
}


@article{Stern:1929,
    author="Stern, Walter",
    title="Versuch einer elektrodynamischen Dickenmessung von Gletschereis",
    journal="Gerlands Beitrage zur Geophysik",
    year="1929",
    volume="27",
    pages="292-333",
    URL="https://cir.nii.ac.jp/crid/1570572700956541952",
}


@book{Steinmetz:2004,
    author = {Steinmetz, Ralf and Nahrstedt, Klara},
    title = {{Multimedia Application}},
    pages = {197--214},
    publisher = {Springer},
    year = {2004},
    doi = {10.1007/978-3-662-08876-0_9},
}


@inproceedings{Stinghen:2018,
    author = {Stinghen, Ivo and Gatto, Bernardo},
    year = {2018},
    month = {11},
    pages = {},
    title = {{Gesture Recognition Using Leap Motion: A Machine Learning-based Controller Interface}},
    booktitle = {17th Brazilian Symposium on Computer Games and Digital Entertainment},
    series = {SBGames '18},
    dates = {October 29-November 1, 2018},
    venue = {Foz do Iguaçu, Brazil},
    issn={2179-2259},
    publisher = {IEEE Computer Society},
    isbn = {978-1-5386-9605-7},
}


@inproceedings{Suh:2018,
	title = {24 {GHz} {FMCW} {Radar} {System} for {Real}-{Time} {Hand} {Gesture} {Recognition} {Using} {LSTM}},
	doi = {10.23919/APMC.2018.8617375},
	abstract = {A 24 GHz frequency modulated continuous wave radar system to recognize human's hand gestures is implemented, which uses commercial off-the-shelf RF front-end IC with one transmitter and four receivers. Planar patch array antennas, signal conditioning circuits and interconnections to a PC are designed for the system. Range-Doppler maps for four receiver channels are obtained with saw-tooth chirping signals transmitted to detect hand gestures. The radar system shows real-time highly accurate gesture recognition. Long-short term memory recurrent neural network as a supervised machine learning technique is used. Seven kinds of hand gestures are recognized within 0.4 m and ±30° from the center of the transmitted antenna with above 91 \% accuracy.},
	booktitle = {2018 {Asia}-{Pacific} {Microwave} {Conference} ({APMC})},
	author={Suh, Jun Seuk and Ryu, Siiung and Han, Bvunghun and Choi, Jaewoo and Kim, Jong-Hwan and Hong, Songcheol},
	month = nov,
	year = {2018},
	keywords = {continuous wave radar system, CW radar, FM radar, FMCW Radar, FMCW radar system, frequency 24.0 GHz, gesture recognition, Gesture recognition, Hand Gesture Recognition, hand gestures, learning (artificial intelligence), Linear antenna arrays, long-short term memory recurrent neural network, LSTM, microstrip antenna arrays, off-the-shelf RF front-end IC, planar antenna arrays, planar patch array antennas, Radar, radar antennas, Radar antennas, radar computing, Range-Doppler maps, real-time hand gesture recognition, Real-time systems, receiver channels, Receivers, recurrent neural nets, saw-tooth chirping signals, signal conditioning circuits},
	pages = {860--862},
}


@inproceedings{Sun:2018,
	title = {Gesture {Classification} with {Handcrafted} {Micro}-{Doppler} {Features} using a {FMCW} {Radar}},
	doi = {10.1109/ICMIM.2018.8443507},
	abstract = {This paper deals with gesture recognition using a 77 GHz FMCW radar system based on the micro-Doppler (μ D) signatures. In addition to the Doppler information, the range information is also available in the FMCW radar. Therefore, it is utilized to filter out the irrelevant targets. We have proposed five micro-Doppler based handcrafted features for gesture recognition. Finally, a simple k-nearest neighbor (k-NN) classifier is applied to evaluate the importance of the five features. The classification results demonstrate that the proposed features can guarantee a promising recognition accuracy.},
	booktitle = {2018 {IEEE} {MTT}-{S} {International} {Conference} on {Microwaves} for {Intelligent} {Mobility} ({ICMIM})},
	author={Sun, Yuliang and Fei, Tai and Schliep, Frank and Pohl, Nils},
	month = apr,
	year = {2018},
	keywords = {Chirp, CW radar, Doppler effect, Doppler information, Doppler radar, feature extraction, FM radar, FMCW automotive radar, FMCW radar system, frequency 77 GHz, Frequency modulation, gesture classification, gesture recognition, handcrafted feature, handcrafted microDoppler features, image classification, k-nearest neighbor classifier, k-NN classifier, microDoppler signatures, nearest neighbour methods, range information, Sensors, Spectrogram, supervised learning, μD signature},
	pages = {1--4},
}


@inproceedings{Sun:2019,
	title = {Automatic {Radar}-based {Gesture} {Detection} and {Classification} via a {Region}-based {Deep} {Convolutional} {Neural} {Network}},
	doi = {10.1109/ICASSP.2019.8682277},
	abstract = {In this paper, a region-based deep convolutional neural network (R-DCNN) is proposed to detect and classify gestures measured by a frequency-modulated continuous wave radar system. Micro-Doppler (μD) signatures of gestures are exploited, and the resulting spectrograms are fed into a neural network. We are the first to use the R-DCNN for radar-based gesture recognition, such that multiple gestures could be automatically detected and classified without manually clipping the data streams according to each hand movement in advance. Further, along with the μD signatures, we incorporate phase-difference information of received signals from an L-shaped antenna array to enhance the classification accuracy. Finally, the classification results show that the proposed network trained with spectrogram and phase-difference information can guarantee a promising performance for nine gestures.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author={Sun, Yuliang and Fei, Tai and Gao, Shangyin and Pohl, Nils},
	month = may,
	year = {2019},
	note = {ISSN: 2379-190X},
	keywords = {antenna arrays, Antenna measurements, automatic radar-based gesture detection, convolutional neural nets, CW radar, Doppler radar, Faster-RCNN, Feature extraction, FM radar, FMCW radar, Frequency measurement, frequency-modulated continuous wave radar system, gesture recognition, Gesture recognition, hand movement, L-shaped antenna array, Micro-Doppler signature, MicroDoppler signatures, Phase difference, phase-difference information, R-DCNN, Radar, radar antennas, Radar antennas, radar detection, Receiving antennas, region-based deep convolutional neural network, signal classification, Spectrogram},
	pages = {4300--4304},
}


@article{Sun:2020a,
	title = {Real-{Time} {Radar}-{Based} {Gesture} {Detection} and {Recognition} {Built} in an {Edge}-{Computing} {Platform}},
	volume = {20},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2020.2994292},
	abstract = {In this paper, a real-time signal processing framework based on a 60 GHz frequency-modulated continuous wave (FMCW) radar system to recognize gestures is proposed. In order to improve the robustness of the radar-based gesture recognition system, the proposed framework extracts a comprehensive hand profile, including range, Doppler, azimuth and elevation, over multiple measurement-cycles and encodes them into a feature cube. Rather than feeding the range-Doppler spectrum sequence into a deep convolutional neural network (CNN) connected with recurrent neural networks, the proposed framework takes the aforementioned feature cube as input of a shallow CNN for gesture recognition to reduce the computational complexity. In addition, we develop a hand activity detection (HAD) algorithm to automatize the detection of gestures in real-time case. The proposed HAD can capture the time-stamp at which a gesture finishes and feeds the hand profile of all the relevant measurement-cycles before this time-stamp into the CNN with low latency. Since the proposed framework is able to detect and classify gestures at limited computational cost, it could be deployed in an edge-computing platform for real-time applications, whose performance is notedly inferior to a state-of-the-art personal computer. The experimental results show that the proposed framework has the capability of classifying 12 gestures in real-time with a high F1-score.},
	number = {18},
	journal = {IEEE Sensors Journal},
	author={Sun, Yuliang and Fei, Tai and Li, Xibo and Warnecke, Alexander and Warsitz, Ernst and Pohl, Nils},
	month = sep,
	year = {2020},
	keywords = {AoA information, Azimuth, computational complexity, computational cost, CW radar, deep convolutional neural network, Doppler effect, Doppler radar, edge-computing platform, Feature extraction, FM radar, FMCW radar, FMCW radar system, frequency 60.0 GHz, frequency-modulated continuous wave radar system, gesture classification, gesture recognition, Gesture recognition, HAD algorithm, hand activity detection, hand activity detection algorithm, hand profile, personal computer, Radar, radar computing, radar detection, radar-based gesture recognition system, range-Doppler spectrum sequence, real time radar-based gesture detection, real-time, real-time signal, Real-time systems, recurrent neural nets, recurrent neural networks, Sensors, shallow CNN, signal classification, time-stamp},
	pages = {10706--10716},
}


@inproceedings{Sun:2020b,
	title = {Multi-{Feature} {Encoder} for {Radar}-{Based} {Gesture} {Recognition}},
	doi = {10.1109/RADAR42522.2020.9114664},
	abstract = {In this paper, a multi-feature encoder for gesture recognition based on a 60 GHz frequency-modulated continuous wave (FMCW) radar system is proposed to extract the gesture characteristics, i.e., range, Doppler, azimuth and elevation, from the low-level raw data. The radar system updates the hand information for every measurement-cycle on all the scattering centers in its field of view, and our proposed encoder is devised to only focus on those essential scattering centers. After observing the hand over several measurement-cycles, we encode the gesture characteristics sequentially into a 2-D feature matrix, which is successively fed into a shallow convolutional neural network (CNN) for classification. For the purpose of distinguishing relevant gestures, the proposed multi-feature encoder is able to efficiently extract adequate information from a multi-dimensional feature space. Thus, the proposed approach is practical for industrial applications where the available dataset is mostly small-scale. The experimental results show that the proposed multi-feature encoder could guarantee a promising performance for a gesture dataset with 12 gestures.},
	booktitle = {2020 {IEEE} {International} {Radar} {Conference} ({RADAR})},
	author={Sun, Yuliang and Fei, Tai and Li, Xibo and Warnecke, Alexander and Warsitz, Ernst and Pohl, Nils},
	month = apr,
	year = {2020},
	note = {ISSN: 2640-7736},
	keywords = {2D feature matrix, CNN, convolutional neural nets, convolutional neural network, CW radar, feature extraction, FM radar, FMCW radar system, frequency 60.0 GHz, frequency-modulated continuous wave radar system, gesture dataset characteristics, gesture recognition, image classification, image coding, matrix algebra, measurement-cycle, millimetre wave radar, multidimensional feature space, multifeature encoder, radar computing, radar imaging, radar-based gesture recognition},
	pages = {351--356},
}


%===========================================================
% References starting by T
%===========================================================
@inproceedings{Taele:2017,
    author="Taele, Paul and Hammond, Tracy",
    editor="Chen, Yaxi and Christie, Marc and Tan, Wenrong",
    title="InvisiShapes: A Recognition System for Sketched 3D Primitives in Continuous Interaction Spaces",
    booktitle="Smart Graphics",
    year="2017",
    publisher="Springer International Publishing",
    address="Cham",
    pages="63--74",
    abstract="Continued improvements and rising ubiquity in-touchscreen and motion-sensing technologies enable users to leverage mid-air input modalities for intelligent surface sketching into the third dimension. However, existing approaches largely either focus on constrained 3D gesture sets, require specialized hardware setups, or do not deviate beyond surface sketching assumptions. We present InvisiShapes, a recognition system for users to sketch 3D geometric primitives in continuous interaction spaces that explore surfaces and mid-air environments. Our system leverages a collection of sketch and gesture recognition techniques and heuristics and takes advantage of easily accessible computing hardware for users to incorporate depth to their sketches. From our interaction study and user evaluations, we observed that our system successfully accomplishes strong recognition and intuitive interaction capabilities on collected sketch+motion data and interactive sketching scenarios, respectively.",
    isbn="978-3-319-53838-9",
}


@article{Tahir:2014,
    author    = {Tahir, Muhammad and Madni, Tahir and Ziauddin, Sheikh and Awan, Muhammad and Waqar, Rana and Khalid, Saher},
    title     = {{Wiimote squash: comparing DTW and WFM techniques for 3D gesture  recognition}},
    journal   = {Int. Arab J. Inf. Technol.},
    volume    = {11},
    number    = {4},
    pages     = {362--369},
    year      = {2014},
    url       = {http://iajit.org/index.php?option=com\_content\&task=blogcategory\&id=93\&Itemid=356},
    timestamp = {Wed, 27 Mar 2019 13:18:45 +0100},
    biburl    = {https://dblp.org/rec/journals/iajit/TahirMZAR14.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Tang:2018,
    title = {{Structured dynamic time warping for continuous hand trajectory gesture recognition}},
    journal = {Pattern Recognition},
    volume = {80},
    pages = {21-31},
    year = {2018},
    issn = {0031-3203},
    doi = {https://doi.org/10.1016/j.patcog.2018.02.011},
    url = {https://www.sciencedirect.com/science/article/pii/S0031320318300621},
    author = {Tang, Jingren and Cheng, Hong and Zhao, Yang and Guo, Hongliang},
    keywords = {Continuous hand gesture recognition, Dynamic time warping, Continuous trajectory segment, Human computer interaction},
    abstract = {Continuous hand gesture recognition is an important area of HCI and challenged by various writing habits and unconstrained hand movement. In this paper, we propose a Structured Dynamic Time Warping (SDTW) approach for continuous hand trajectory recognition. We first propose an automatic continuous trajectory segmentation approach which combines templates and velocity information to spot the beginning and ending points in hand gesture trajectories. Then we assign different weights to feature sequences based on the structured information, from the positions of corner points in the arbitrary trajectories. Finally, we evaluate the SDTW on the Continuous Letter Trajectory (CLT) database. Experimental results show that the proposed approach is robust to the diversity of same handwritten letter, and significantly outperforms state-of-the-art approaches.},
}


@article{Tang:2020,
    author={Tang, Adrian and Carey, Robert and Virbila, Gabriel and Zhang, Yan and Huang, Rulin and Frank Chang, Mau-Chung},
    journal={IEEE Transactions on Terahertz Science and Technology}, 
    title={{A Delay-Correlating Direct-Sequence Spread-Spectrum (DS/SS) Radar System-on-Chip Operating at 183–205 GHz in 28 nm CMOS}}, 
    year={2020},
    volume={10},
    number={2},
    pages={212-220},
    doi={10.1109/TTHZ.2020.2969105},
}


@inproceedings{Taranta:2015,
    author = {Taranta,II, Eugene M. and LaViola,Jr., Joseph J.},
    title = {{Penny Pincher: A Blazing Fast, Highly Accurate \$-family Recognizer}},
    booktitle = {Proceedings of the 41st Graphics Interface Conference},
    series = {GI '15},
    year = {2015},
    isbn = {978-0-9947868-0-7},
    venue = {Halifax, Nova Scotia, Canada},
    pages = {195--202},
    numpages = {8},
    url = {http://dl.acm.org/citation.cfm?id=2788890.2788925},
    acmid = {2788925},
    publisher = {Canadian Information Processing Society},
    address = {Toronto, Ont., Canada, Canada},
    keywords = {\$-family, gesture recognition, template matching},
} 


@inproceedings{Taranta:2016,
	address = {New York, NY, USA},
	series = {{UIST} '16},
	title = {A {Rapid} {Prototyping} {Approach} to {Synthetic} {Data} {Generation} for {Improved} {2D} {Gesture} {Recognition}},
	isbn = {978-1-4503-4189-9},
	url = {https://doi.org/10.1145/2984511.2984525},
	doi = {10.1145/2984511.2984525},
	abstract = {Training gesture recognizers with synthetic data generated from real gestures is a well known and powerful technique that can significantly improve recognition accuracy. In this paper we introduce a novel technique called gesture path stochastic resampling (GPSR) that is computationally efficient, has minimal coding overhead, and yet despite its simplicity is able to achieve higher accuracy than competitive, state-of-the-art approaches. GPSR generates synthetic samples by lengthening and shortening gesture subpaths within a given sample to produce realistic variations of the input via a process of nonuniform resampling. As such, GPSR is an appropriate rapid prototyping technique where ease of use, understandability, and efficiency are key. Further, through an extensive evaluation, we show that accuracy significantly improves when gesture recognizers are trained with GPSR synthetic samples. In some cases, mean recognition errors are reduced by more than 70\%, and in most cases, GPSR outperforms two other evaluated state-of-the-art methods.},
	urldate = {2020-12-23},
	booktitle = {Proceedings of the 29th {Annual} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Taranta, Eugene M. and Maghoumi, Mehran and Pittman, Corey R. and LaViola, Joseph J.},
	month = oct,
	year = {2016},
	keywords = {gesture recognition, rapid prototyping, gesture path, stochastic resampling, synthetic gestures},
	pages = {873--885},
}
  
  
@inproceedings{Taranta:2017,
    author = {Taranta II, Eugene M. and Samiei, Amirreza and Maghoumi, Mehran and Khaloo, Pooya and Pittman, Corey R. and LaViola Jr., Joseph J.},
    title = {{Jackknife: A Reliable Recognizer with Few Samples and Many Modalities}},
    booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
    series = {CHI '17},
    year = {2017},
    isbn = {978-1-4503-4655-9},
    venue = {Denver, Colorado, USA},
    pages = {5850--5861},
    numpages = {12},
    url = {http://doi.acm.org/10.1145/3025453.3026002},
    doi = {10.1145/3025453.3026002},
    acmid = {3026002},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {dynamic time warping, gesture customization, gesture recognition, rapid prototyping, user evaluation},
}


@article{Taranta:2021,
    author = {Taranta II, Eugene M. and Pittman, Corey R. and Maghoumi, Mehran and Maslych, Mykola and Moolenaar, Yasmine M. and Laviola Jr, Joseph J.},
    title = {{Machete: Easy, Efficient, and Precise Continuous Custom Gesture Segmentation}},
    year = {2021},
    issue_date = {January 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {28},
    number = {1},
    issn = {1073-0516},
    url = {https://doi.org/10.1145/3428068},
    doi = {10.1145/3428068},
    abstract = {We present Machete, a straightforward segmenter one can use to isolate custom gestures in continuous input. Machete uses traditional continuous dynamic programming with a novel dissimilarity measure to align incoming data with gesture class templates in real time. Advantages of Machete over alternative techniques is that our segmenter is computationally efficient, accurate, device-agnostic, and works with a single training sample. We demonstrate Machete’s effectiveness through an extensive evaluation using four new high-activity datasets that combine puppeteering, direct manipulation, and gestures. We find that Machete outperforms three alternative techniques in segmentation accuracy and latency, making Machete the most performant segmenter. We further show that when combined with a custom gesture recognizer, Machete is the only option that achieves both high recognition accuracy and low latency in a video game application.},
    journal = {ACM Trans. Comput.-Hum. Interact.},
    month = jan,
    articleno = {5},
    numpages = {46},
    keywords = {DTW, Continuous, segmentation, custom, CDP, gesture},
}


@inproceedings{Tayag:2021,
    author = {Tayag, Marlon and Ronie, Bituin and Marvin, Reyes},
    title = {{Leap Motion Controller Enabled Simulations of Personal Computer Assembly Programs}},
    year = {2021},
    isbn = {9781450388955},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3451471.3451474},
    doi = {10.1145/3451471.3451474},
    abstract = {The merging of both physical and virtual worlds into one seamless reality is an idea
    that has been tackled by many disciplines such as computer science, psychology, sociology
    and even literary genres like science fiction. One such amalgamation is Mixed Reality,
    which allows for physical and digital objects to co-exist and interact in real time,
    and it has been used in different areas such as entertainment to military training.
    This research explores the possibilities for creating collaborative mixed reality
    in an educational context. In this paper, we developed two (2) simulated realities.
    The one uses Conventional Screen typically found or see in games and where there are
    perspective cameras which is First Person Perspective and Third Person Perspective.
    While the other one is inside Mixed Reality paradigm, where reality and virtual objects
    co-exist, and used the Leap Motion Controller to support mid-air gestures (controls).
    We performed comparison between the two simulations and also measure Mixed Reality
    application. Also, we present how mixed reality applications can be used as an alternative
    method in exhibiting topics.},
    booktitle = {Proceedings of the 4th International Conference on Software Engineering and Information Management},
    pages = {17–21},
    numpages = {5},
    keywords = {Leap Motion, AR (Augmented Reality), Dual Reality, MR (Mixed Reality) and VR (Virtual Reality), Gyroscope},
    venue = {Yokohama, Japan},
    series = {ICSIM '21},
}


@inproceedings{Thaweesitthichat:2018,
    author    = {Thaweesitthichat, Pakpoom and Jiamsorn, Thuchchai and Punyabukkana, Proadpran and Chuangsuwanich, Ekapol and Suchato, Atiwong},
    booktitle = {Proceedings of the 12th International Convention on Rehabilitation Engineering and Assistive Technology},
    title     = {{Thai Sign Language Recognition with Leap Motion}},
    year      = {2018},
    address   = {Midview City, SGP},
    pages     = {47–50},
    publisher = {Singapore Therapeutic, Assistive \& Rehabilitative Technologies (START) Centre},
    series    = {i-CREATe 2018},
    keywords  = {Thai Sign Language, Deep Learning, Translation Device, Gesture Recognition, Machine Learning},
    venue  = {Shanghai, China},
    numpages  = {4},
}


@inproceedings{Theissler:2020,
    author="Theissler, Andreas and Vollert, Simon and Benz, Patrick and Meerhoff, Laurentius A. and Fernandes, Marc",
    editor="Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar",
    title="ML-ModelExplorer: An Explorative Model-Agnostic Approach to Evaluate and Compare Multi-class Classifiers",
    booktitle="Machine Learning and Knowledge Extraction",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="281--300",
    isbn="978-3-030-57321-8",
}


@inproceedings{Theofanidis:2017,
    author    = {Theofanidis, Michail and Sayed, Saif Iftekar and Lioulemes, Alexandros and Makedon, Fillia},
    booktitle = {Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments},
    title     = {{VARM: Using Virtual Reality to Program Robotic Manipulators}},
    year      = {2017},
    address   = {New York, NY, USA},
    pages     = {215–221},
    publisher = {Association for Computing Machinery},
    series    = {PETRA '17},
    doi       = {10.1145/3056540.3056541},
    isbn      = {9781450352277},
    keywords  = {Robot Kinematics, Human Robot Interaction, Leap Motion Controller, Virtual Reality},
    venue  = {Island of Rhodes, Greece},
    numpages  = {7},
    url       = {https://doi.org/10.1145/3056540.3056541},
}


@article{Togootogtokh:2018,
    author     = {Togootogtokh, Enkhtogtokh and Shih, Timothy K. and Kumara, W. G. and Wu, Shih-Jung and Sun, Shih-Wei and Chang, Hon-Hang},
    journal    = {Multimedia Tools Appl.},
    title      = {{3D Finger Tracking and Recognition Image Processing for Real-Time Music Playing with Depth Sensors}},
    year       = {2018},
    issn       = {1380-7501},
    month      = apr,
    number     = {8},
    pages      = {9233–9248},
    volume     = {77},
    address    = {USA},
    doi        = {10.1007/s11042-017-4784-9},
    issue_date = {April 2018},
    keywords   = {Neural network, Senz3D, 3D finger tracking, Leap motion, 3D finger recognition, 3D finger gesture, Virtual musical instruments, Gesture recognition},
    numpages   = {16},
    publisher  = {Kluwer Academic Publishers},
    url        = {https://doi.org/10.1007/s11042-017-4784-9},
}


@article{Tran:2013,
    author = {Tran, Anh Phuong and Andr\'e, Frederic and Craeye, Christophe and Lambot, S{\'{e}}bastien},
    title = {{Near-field or far-field full-wave ground penetrating radar modeling as a function of the antenna height above a planar layered medium}},
    journal = {Progress in Electromagnetics Research-Pier},
    volume = {141},
    pages = {415-430},
    ISSN = {1559-8985},
    DOI = {10.2528/pier13053106},
    year = {2013},
    type = {Journal Article},
}


@article{Tran:2016,
    author     = {Tran, Van Thanh and Lee, Jaewoon and Kim, Dongho and Jeong, Young-Sik},
    journal    = {J. Supercomput.},
    title      = {{Easy-to-Use Virtual Brick Manipulation Techniques Using Hand Gestures}},
    year       = {2016},
    issn       = {0920-8542},
    month      = jul,
    number     = {7},
    pages      = {2752–2766},
    volume     = {72},
    address    = {USA},
    doi        = {10.1007/s11227-015-1588-4},
    issue_date = {July 2016},
    keywords   = {NUI (Natural User Interface), Gesture recognition, Leap Motion, Virtual reality},
    numpages   = {15},
    publisher  = {Kluwer Academic Publishers},
    url        = {https://doi.org/10.1007/s11227-015-1588-4},
}


@inproceedings{Tzadok:2020,
	title = {{AI}-driven {Event} {Recognition} with a {Real}-{Time} {3D} 60-{GHz} {Radar} {System}},
	doi = {10.1109/IMS30576.2020.9224112},
	abstract = {A vertically integrated antennas-to-AI system is presented. 60-GHz 16-element phased array transmitter and receiver modules, previously developed for Gb/s NLOS communications, are used to implement a 3D radar system that extracts volumetric information from a scene at a high frame rate. The system employs an FMCW signal with 1-GHz bandwidth and can process 1250 radar readouts per second. An efficient timing control scheme between the radar electronics and the phased array module control enables obtaining each of the radar readouts from a separate beam direction. The system can scan a frame of 5×5 directions 50 times per second. All the radar system components including signal generation and ADC are assembled in a single portable chassis. A camera is also included in the system to enable the simultaneous capture of radar and video streams. A DNN was developed to extract temporal and volumetric features from the 3D radar information stream and enable the automatic recognition of fast evolving events. As an application example, the DNN was trained to perform automatic hand gesture recognition. The overall radar system and the associated DNN achieved a recognition accuracy of 93\% on a set of 9 different gestures involving two hands.},
	booktitle = {2020 {IEEE}/{MTT}-{S} {International} {Microwave} {Symposium} ({IMS})},
	author={Tzadok, Asaf and Valdes-Garcia, Alberto and Pepeljugoski, Petar and Plouchart, J.-O. and Yeck, Mark and Liu, Huijuan},
	month = aug,
	year = {2020},
	note = {ISSN: 2576-7216},
	keywords = {3D radar information stream, 3D sensing, ADC, antenna phased arrays, antennas-to-AI system, array transmitter, automatic hand gesture recognition, beam direction, CW radar, DNN, event recognition, feature extraction, FM radar, FMCW signal, gesture recognition, hand gestures recognition, Millimeter-wave radar, millimetre wave antenna arrays, millimetre wave radar, neural nets, NLOS communications, phased array module control, radar antennas, radar electronics, radar imaging, radar readouts, radar system components, receiver modules, signal generation, stereo image processing, telecommunication control, timing control scheme, video cameras, video signal processing, video streams, volumetric information},
	pages = {795--798},
}


%===========================================================
% References starting by U
%===========================================================
@misc{Ultraleap:2021,
    author = {Ultraleap},
    year= {2021},
    title = {{Ultraleap API Overview}},
    journal={Leap Motion Blog}, 
    howpublished = {\url{https://developer-archive.leapmotion.com/documentation/csharp/devguide/Leap_Overview.html}},
    note = {[Online; accessed 02-October-2021]},
}


%===========================================================
% References starting by V
%===========================================================
@inproceedings{Valdez:2014, 
    author={Valdez, Nicanor and Besas, Ronnie and Yu, China and Dumalaon, Donna and Atienza, Rowel},  
    booktitle={2014 IEEE Asia Pacific Conference on Wireless and Mobile},   
    title={{3D gestures on 2D screens for mobile games}},   
    year={2014},  
    volume={},  
    number={},  
    pages={232-237},
    doi={10.1109/APWiMob.2014.6920274},
}


@inproceedings{Vanderdonckt:2018,
    author    = {Vanderdonckt, Jean and Roselli, Paolo and P{\'{e}}rez{-}Medina, Jorge Luis},
    editor    = {D'Mello, Sidney K. and Georgiou, Panayiotis G. and Scherer, Stefan and Provost, Emily Mower and Soleymani, Mohammad and Worsley, Marcelo},
    title     = {{!FTL}, an Articulation-Invariant Stroke Gesture Recognizer with Controllable  Position, Scale, and Rotation Invariances},
    booktitle = {Proceedings of the ACM International Conference on Multimodal
               Interaction},
    series    = {{ICMI} '18},
    venue  = {Boulder, CO, USA},
    dates     = {October 16-20, 2018},
    pages     = {125--134},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    year      = {2018},
    url       = {https://doi.org/10.1145/3242969.3243032},
    doi       = {10.1145/3242969.3243032},
    timestamp = {Tue, 15 Oct 2019 08:40:00 +0200},
    biburl    = {https://dblp.org/rec/conf/icmi/VanderdoncktRP18.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Vanderdonckt:2019a,
    author = {Vanderdonckt, Jean and Nguyen, Thanh-Diane},
    title = {{MoCaDiX: Designing Cross-Device User Interfaces of an Information System Based on Its Class Diagram}},
    year = {2019},
    issue_date = {June 2019},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {3},
    number = {EICS},
    url = {https://doi.org/10.1145/3331159},
    doi = {10.1145/3331159},
    abstract = {This paper presents MoCaDiX, a method for designing cross-device graphical user interfaces of an information system based on its UML class diagram, structured as a four-step process: (1) a UML class diagram of the information system is created in a model editor, (2) how the classes, attributes, methods, and relationships of this class diagram are presented across devices is then decided based on user interface patterns with their own parametrization, (3) based on these parameters, a Concrete User Interface model is generated in QuiXML, a lightweight fit-to-purpose User Interface Description Language, and (4) based on this model, HTML5 cross-device user interfaces are semi-automatically generated for four configurations: single/multipledevice single/multiple-display on a smartphone, a tablet, and a desktop. From the practitioners' viewpoint, a first experiment investigates effectiveness, efficiency, and subjective satisfaction of three intermediate and three expert designers, using MoCaDiX on a representative class diagram. From the end user's viewpoint, a second experiment compares subjective satisfaction and preference of twenty end users assessing layout strategies for interfaces generated on two devices.},
    journal = {Proc. ACM Hum.-Comput. Interact.},
    month = jun,
    articleno = {17},
    numpages = {40},
    keywords = {class diagram., cross-device interaction, mapping rule, unified modeling language, crossdevice, user interfaces, automated generation of user interfaces, design option, design exploration},
}


@inproceedings{Vanderdonckt:2019b,
    author="Vanderdonckt, Jean and Magrofuoco, Nathan and Kieffer, Suzanne and P{\'e}rez, Jorge and Rase, Ysabelle and Roselli, Paolo and Villarreal, Santiago",
    editor="Marcus, Aaron and Wang, Wentao",
    title="Head and Shoulders Gestures: Exploring User-Defined Gestures with Upper Body",
    booktitle="Design, User Experience, and Usability. User Experience in Advanced Technological Environments",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="192--213",
    abstract="This paper presents empirical results about user-defined gestures for head and shoulders by analyzing 308 gestures elicited from 22 participants for 14 referents materializing 14 different types of tasks in IoT context of use. We report an overall medium consensus but with medium variance (mean: .263, min: .138, max: .390 on the unit scale) between participants gesture proposals, while their thinking time were less similar (min: 2.45s, max: 22.50s), which suggests that head and shoulders gestures are not all equally easy to imagine and to produce. We point to the challenges of deciding which head and shoulders gestures will become the consensus set based on four criteria: the agreement rate, their individual frequency, their associative frequency, and their unicity.",
    isbn="978-3-030-23541-3",
}


@article{vanderMaaten:2009,
    author = {van der Maaten, Laurens and Postma, Eric and van den Herik, Jaap},
    year = {2009},
    month = {1},
    pages = {66-71},
    title = {{Dimensionality Reduction: A Comparative Review}},
    volume = {10},
    journal = {Journal of Machine Learning Research},
    url = {https://members.loria.fr/moberger/Enseignement/AVR/Exposes/TR_Dimensiereductie.pdf},
}


@article{Vandersmissen:2020,
	title = {{Indoor human activity recognition using high-dimensional sensors and deep neural networks}},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {http://link.springer.com/10.1007/s00521-019-04408-1},
	doi = {10.1007/s00521-019-04408-1},
	language = {en},
	number = {16},
	urldate = {2020-12-21},
	journal = {Neural Computing and Applications},
	author = {Vandersmissen, Baptist and Knudde, Nicolas and Jalalvand, Azarakhsh and Couckuyt, Ivo and Dhaene, Tom and De Neve, Wesley},
	month = aug,
	year = {2020},
	pages = {12295--12309},
}


@book{vanSomeren:1994,
  title = {{The Think Aloud Method - A Practical Guide to Modelling Cognitive Processes}},
  author = {van Someren, Maarten and Barnard, Yvonne and Sandberg, Jacobijn},
  year = {1994},
  publisher = {Academic Press},
  address = {London},
}


@inproceedings{Vatavu:2007,
    author    = {Vatavu, Radu{-}Daniel and Grisoni, Laurent and Pentiuc, Stefan Gheorghe},
    editor    = {Dias, Miguel Sales and Gibet, Sylvie and Wanderley, Marcelo M. and Bastos, Rafael},
    title     = {{Gesture Recognition Based on Elastic Deformation Energies}},
    booktitle = {Gesture-Based Human-Computer Interaction and Simulation, 7th International Gesture Workshop, {GW} 2007, Lisbon, Portugal, May 23-25, 2007, Revised Selected Papers},
    series    = {Lecture Notes in Computer Science},
    volume    = {5085},
    pages     = {1--12},
    publisher = {Springer},
    year      = {2007},
    url       = {https://doi.org/10.1007/978-3-540-92865-2\_1},
    doi       = {10.1007/978-3-540-92865-2\_1},
    timestamp = {Tue, 14 May 2019 10:00:35 +0200},
    biburl    = {https://dblp.org/rec/conf/gw/VatavuGP07.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Vatavu:2008,
    author = {Vatavu, Radu-Daniel and Pentiuc, Stefan},
    year = {2008},
    month = {01},
    pages = {837-851},
    title = {{Multi-Level Representation of Gesture as Command for Human Computer Interaction.}},
    volume = {27},
    journal = {Computing and Informatics},
}


@inproceedings{Vatavu:2011,
    author = {Vatavu, Radu-Daniel},
    title = {{The Effect of Sampling Rate on the Performance of Template-Based Gesture Recognizers}},
    year = {2011},
    isbn = {9781450306416},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2070481.2070531},
    doi = {10.1145/2070481.2070531},
    booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces},
    pages = {271–278},
    numpages = {8},
    keywords = {recognition rate, gesture recognition, execution time, sampling resolution, experimentation, comparing classifiers, classifier performance},
    venue = {Alicante, Spain},
    series = {ICMI ’11},
}


@inproceedings{Vatavu:2012,
    author = {Vatavu, Radu-Daniel and Anthony, Lisa and Wobbrock, Jacob O.},
    title = {{Gestures As Point Clouds: A \$P Recognizer for User Interface Prototypes}},
    booktitle = {Proceedings of the 14th ACM International Conference on Multimodal Interaction},
    series = {ICMI '12},
    year = {2012},
    isbn = {978-1-4503-1467-1},
    venue = {Santa Monica, California, USA},
    pages = {273--280},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/2388676.2388732},
    doi = {10.1145/2388676.2388732},
    acmid = {2388732},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {\$1, \$n, \$p, comparing classifiers, euclidean, gesture recognition, hausdorff, hungarian, multistrokes, point clouds},
} 


@inproceedings{Vatavu:2012:1F,
    author = {Vatavu, Radu-Daniel},
    title = {{1F: One Accessory Feature Design for Gesture Recognizers}},
    booktitle = {Proceedings of the 2012 ACM International Conference on Intelligent User Interfaces},
    series = {IUI '12},
    year = {2012},
    isbn = {978-1-4503-1048-2},
    venue = {Lisbon, Portugal},
    pages = {297--300},
    numpages = {4},
    url = {http://doi.acm.org/10.1145/2166966.2167022},
    doi = {10.1145/2166966.2167022},
    acmid = {2167022},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {classification, comparing classifiers, feature selection, gesture descriptors, gesture recognition, nearest neighbor, pruning, training set},
} 


@article{Vatavu:2013,
    title = "The impact of motion dimensionality and bit cardinality on the design of 3D gesture recognizers",
    journal = "International Journal of Human-Computer Studies",
    volume = "71",
    number = "4",
    pages = "387 - 409",
    year = "2013",
    issn = "1071-5819",
    doi = "https://doi.org/10.1016/j.ijhcs.2012.11.005",
    url = "http://www.sciencedirect.com/science/article/pii/S1071581912002108",
    author = "Vatavu, Radu-Daniel",
    keywords = "Gesture recognition, Gesture dimensionality, Sampling rate, 3D gestures, Classifiers, Bit cardinality, Bit depth, Euclidean distance, Angular cosine distance, Dynamic time warping, Hausdorff, Gesture toolkit",
    abstract = "The interactive demands of the upcoming ubiquitous computing era have set off researchers and practitioners toward prototyping new gesture-sensing devices and gadgets. At the same time, the practical needs of developing for such miniaturized prototypes with sometimes very low processing power and memory resources make practitioners in high demand of fast gesture recognizers employing little memory. However, the available work on motion gesture classifiers has mainly focused on delivering high recognition performance with less discussion on execution speed or required memory. This work investigates the performance of today's commonly used 3D motion gesture recognizers under the effect of different gesture dimensionality and bit cardinality representations. Specifically, we show that few sampling points and low bit depths are sufficient for most motion gesture metrics to attain their peak recognition performance in the context of the popular Nearest-Neighbor classification approach. As a practical consequence, 16x faster recognizers working with 32x less memory while delivering the same high levels of recognition performance are being reported. We present recognition results for a large gesture corpus consisting in nearly 20,000 gesture samples. In addition, a toolkit is provided to assist practitioners in optimizing their gesture recognizers in order to increase classification speed and reduce memory consumption for their designs. At a deeper level, our findings suggest that the precision of the human motor control system articulating 3D gestures is needlessly surpassed by the precision of today's motion sensing technology that unfortunately bares a direct connection with the sensors' cost. We hope this work will encourage practitioners to consider improving the performance of their prototypes by careful analysis of motion gesture representation rather than by throwing more processing power and more memory into the design.",
}


@inproceedings{Vatavu:2014a,
    author = {Vatavu, Radu-Daniel and Anthony, Lisa and Wobbrock, Jacob O.},
    title = {{Gesture Heatmaps: Understanding Gesture Performance with Colorful Visualizations}},
    year = {2014},
    isbn = {9781450328852},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2663204.2663256},
    doi = {10.1145/2663204.2663256},
    booktitle = {Proceedings of the 16th International Conference on Multimodal Interaction},
    pages = {172–179},
    numpages = {8},
    keywords = {gesture heatmaps, user study, toolkits, recognition, features},
    venue = {Istanbul, Turkey},
    series = {ICMI ’14},
}


@inproceedings{Vatavu:2014b,
    author = {Vatavu, Radu-Daniel and Zaiti, Ionut-Alexandru},
    title = {{Leap Gestures for TV: Insights from an Elicitation Study}},
    year = {2014},
    isbn = {9781450328388},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2602299.2602316},
    doi = {10.1145/2602299.2602316},
    abstract = {We present insights from a gesture elicitation study in the context of interacting with TV, during which 18 participants contributed and rated the execution difficulty and recall likeliness of free-hand gestures for 21 distinct TV tasks. Our study complements previous work on gesture interaction design for the TV set with the first exploration of fine-grained resolution 3-D finger movements and hand pose gestures. We report lower agreement rates (.20) than previous gesture studies and 72.8\% recall rate and 15.8\% false positive recall, results that are explained by the complexity and variability of unconstrained finger gestures. Nevertheless, we report a large 82\% preference for gesture commands versus TV remote controls. We also confirm previous findings, such as people's preferences for related gestures for dichotomous tasks, and we report low agreement rates for abstract tasks, such as "open browser" or "show channels list" in our specific TV scenario. In the end, we contribute a set of design guidelines for practitioners interested in free-hand finger and hand pose gestures for interactive TV scenarios, and we release a dataset of 378 Leap Motion gesture records consisting in finger position, direction, and velocity coordinates for further studies in the community. We see this exploration as a first step toward designing low-effort high-resolution finger gestures and hand poses for lean-back interaction with the TV set.},
    booktitle = {Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video},
    pages = {131–138},
    numpages = {8},
    keywords = {recall likeliness, interactive tv, motion gestures, gesture interfaces, hand pose, leap motion, elicitation study},
    venue = {Newcastle Upon Tyne, United Kingdom},
    series = {TVX '14},
}


@inproceedings{Vatavu:2015,
    author    = {Vatavu, Radu{-}Daniel and Wobbrock, Jacob O.},
    editor    = {Begole, Bo and Kim, Jinwoo and Inkpen, Kori and Woo, Woontack},
    title     = {{Formalizing Agreement Analysis for Elicitation Studies: New Measures, Significance Test, and Toolkit}},
    booktitle = {Proceedings of the 33rd Annual {ACM} Conference on Human Factors in Computing Systems},
    series = {CHI 2015},
    venue = {Seoul, Republic of Korea}, 
    dates = {April 18-23, 2015},
    pages     = {1325--1334},
    publisher = {{ACM}},
    year      = {2015},
    url       = {https://doi.org/10.1145/2702123.2702223},
    doi       = {10.1145/2702123.2702223},
    timestamp = {Fri, 12 Mar 2021 15:27:48 +0100},
    biburl    = {https://dblp.org/rec/conf/chi/VatavuW15.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@misc{Vatavu:2016,
    author = {Craciun, Elena-Gina and Rusu, Ionela  and Vatavu, Radu-Daniel},
    year= {2016},
    title = {{Free-Hand Gesture Recognizer Pseudocode}},
    howpublished = {\url{http://www.eed.usv.ro/mintviz/projects/GIVISIMP/data/Pseudocode2.pdf}},
    keywords = {Recognizer,FreeHand},
    note = {[Online; accessed 09-August-2020]},
}


@inproceedings{Vatavu:2017a,
    author = {Vatavu, Radu-Daniel},
    title = {{Improving Gesture Recognition Accuracy on Touch Screens for Users with Low Vision}},
    booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems},
    series = {CHI '17},
    year = {2017},
    isbn = {978-1-4503-4655-9},
    venue = {Denver, Colorado, USA},
    pages = {4667--4679},
    numpages = {13},
    url = {http://doi.acm.org/10.1145/3025453.3025941},
    doi = {10.1145/3025453.3025941},
    acmid = {3025941},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {1, P, P+, algorithms, evaluation, gesture recognition, low vision, point clouds, recognition, recognition accuracy, touch gestures, touch screens, visual impairments},
}


@article{Vatavu:2017b,
	author = {Vatavu, Radu-Daniel},
	year = {2017},
	title = {{Beyond Features for Recognition: Human-Readable Measures to Understand Users' Whole-Body Gesture Performance}},
	journal = {International Journal of Human-Computer Interaction},
	volume = {33},
	number = {9},
	numpages = {19},
	pages = {713-730},
}


@inproceedings{Vatavu:2018,
    author = {Vatavu, Radu-Daniel and Anthony, Lisa and Wobbrock, Jacob O.},
    title = {{\$Q: A Super-Quick, Articulation-Invariant Stroke-Gesture Recognizer for Low-Resource Devices}},
    year = {2018},
    isbn = {9781450358989},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3229434.3229465},
    doi = {10.1145/3229434.3229465},
    booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services},
    articleno = {23},
    numpages = {12},
    keywords = {point-cloud recognizer, mobile devices, \$Q, stroke recognition, \$1, \$-family, \$P, gesture recognition, low-resource devices},
    venue = {Barcelona, Spain},
    series = {MobileHCI '18},
}


@inproceedings{Vatavu:2019,
    author = {Vatavu, Radu-Daniel},
    title = {{The Dissimilarity-Consensus Approach to Agreement Analysis in Gesture Elicitation Studies}},
    year = {2019},
    isbn = {9781450359702},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3290605.3300454},
    doi = {10.1145/3290605.3300454},
    abstract = {We introduce the dissimilarity-consensus method, a new approach to computing objective measures of consensus between users' gesture preferences to support data analysis in end-user gesture elicitation studies. Our method models and quantifies the relationship between users' consensus over gesture articulation and numerical measures of gesture dissimilarity, e.g., Dynamic Time Warping or Hausdorff distances, by employing growth curves and logistic functions. We exemplify our method on 1,312 whole-body gestures elicited from 30 children, ages 3 to 6 years, and we report the first empirical results in the literature on the consensus between whole-body gestures produced by children this young. We provide C\# and R software implementations of our method and make our gesture dataset publicly available.},
    booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
    pages = {1–13},
    numpages = {13},
    keywords = {growth curves, consensus, dataset, gesture elicitation, logistic model, gesture input, children, whole-body gestures},
    venue = {Glasgow, Scotland Uk},
    series = {CHI '19},
}


@Incollection{Vatavu:2023a,
    author="Vatavu, Radu-Daniel",
    editor="Vanderdonckt, Jean and Palanque, Philippe and Winckler, Marco",
    title="Gesture-Based Interaction",
    bookTitle="Handbook of Human-Computer Interaction",
    year="2023",
    publisher="Springer International Publishing",
    address="Cham",
    pages="1--47",
    abstract="Many interactive devices and systems, from smartphones and tablets to smart wearables, video game consoles, ambient displays and interactive surfaces to systems rendering virtual and augmented reality environments, leverage users' capabilities to communicate and interact with gestures of many kinds: taps, touches, grasps, pinches, head nods, pointing, hand poses, signs and emblems, flicks and swipes, and mid-air movements of the fingers, wrists, arms, legs, and the whole body. These gestures are sensed, modeled, recognized, and interpreted by interactive computer systems to leverage the rich expressiveness of human communicative skills and, consequently, enable natural, intuitive, and fluent means of communication for users in relation to a computer interlocutor. This chapter presents multiple aspects relevant to the design and engineering of gesture-based interaction, including desirable quality properties of gesture input, gesture representation and recognition techniques, gesture analysis methods, and corresponding software tools to assist the design of gesture sets for interactive systems and accessibility aspects of gesture interaction for users with various sensory or motor abilities. Also, this chapter is meant to provide an overview of the large scientific literature available on the topic of gesture-based interaction, pointing the reader to both theoretical and practical aspects and highlighting design challenges, technical solutions, and opportunities for designing and engineering gesture-based interaction with computer systems.",
    isbn="978-3-319-27648-9",
    doi="10.1007/978-3-319-27648-9_20-1",
    url="https://doi.org/10.1007/978-3-319-27648-9_20-1",
}

@inproceedings{Vatavu:2023b,
    author = {Vatavu, Radu-Daniel},
    title = {{IFAD Gestures: Understanding Users’ Gesture Input Performance with Index-Finger Augmentation Devices}},
    year = {2023},
    isbn = {9781450394215},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3544548.3580928},
    doi = {10.1145/3544548.3580928},
    abstract = {We examine gestures performed with a class of input devices with distinctive quality properties in the wearables landscape, which we call “index-Finger Augmentation Devices” (iFADs). We introduce a four-level taxonomy to characterize the diversity of iFAD gestures, evaluate iFAD gesture articulation on a dataset of 6,369 gestures collected from 20 participants, and compute recognition accuracy rates. Our findings show that iFAD gestures are fast (1.84s on average), easy to articulate (1.52 average rating on a difficulty scale from 1 to 5), and socially acceptable (81\% willingness to use them in public places). We compare iFAD gestures with gestures performed using other devices (styli, touchscreens, game controllers) from several public datasets (39,263 gestures, 277 participants), and report that iFAD gestures are two times faster than whole-body gestures and as fast as stylus and finger strokes performed on touchscreens.},
    booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
    articleno = {576},
    numpages = {17},
    keywords = {finger augmentation devices, taxonomy, gesture analysis, Index finger, gesture recognition, gesture input},
    venue = {Hamburg, Germany},
    series = {CHI '23},
}


@inproceedings{Villarreal:2020,
    author = {Villarreal-Narvaez, Santiago and Vanderdonckt, Jean and Vatavu, Radu-Daniel and Wobbrock, Jacob O.},
    title = {{A Systematic Review of Gesture Elicitation Studies: What Can We Learn from 216 Studies?}},
    year = {2020},
    isbn = {9781450369749},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3357236.3395511},
    doi = {10.1145/3357236.3395511},
    abstract = {Gesture elicitation studies represent a popular and resourceful method in HCI to inform the design of intuitive gesture commands, reflective of end-users' behavior, for controlling all kinds of interactive devices, applications, and systems. In the last ten years, an impressive body of work has been published on this topic, disseminating useful design knowledge regarding users' preferences for finger, hand, wrist, arm, head, leg, foot, and whole-body gestures. In this paper, we deliver a systematic literature review of this large body of work by summarizing the characteristics and findings ofN=216gesture elicitation studies subsuming 5,458 participants, 3,625 referents, and 148,340 elicited gestures. We highlight the descriptive, comparative, and generative virtues of our examination to provide practitioners with an effective method to (i) understand how new gesture elicitation studies position in the literature; (ii) compare studies from different authors; and (iii) identify opportunities for new research. We make our large corpus of papers accessible online as a Zotero group library at https://www.zotero.org/groups/2132650/gesture_elicitation_studies.},
    booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
    pages = {855–872},
    numpages = {18},
    keywords = {systematic literature review, gesture elicitation, survey},
    venue = {Eindhoven, Netherlands},
    series = {DIS '20},
}


@inproceedings{Villarreal:2022,
    author = {Villarreal-Narvaez, Santiago and \c{S}iean, Alexandru-Ionu\c{t} and Slu\"{y}ters, Arthur and Vatavu, Radu-Daniel and Vanderdonckt, Jean},
    title = {{Informing Future Gesture Elicitation Studies for Interactive Applications That Use Radar Sensing}},
    year = {2022},
    isbn = {9781450397193},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3531073.3534475},
    doi = {10.1145/3531073.3534475},
    booktitle = {Proceedings of the 2022 International Conference on Advanced Visual Interfaces},
    articleno = {50},
    numpages = {3},
    venue = {Frascati, Rome, Italy},
    series = {AVI 2022},
}


@inproceedings{Viunytskyi:2017,
	title = {{Novel bispectrum-based wireless vision technique using disturbance of electromagnetic field by human gestures}},
	doi = {10.1109/SPS.2017.8053684},
	abstract = {Novel human gesture recognition and classification technique is suggested and experimentally studied. Suggested strategy is based on exploiting the interactions of human gestures with high-frequency electromagnetic field. Extracting of classification features contained in the wireless radio signal modulated by human gestures is proposed by utilizing bispectrum-based processing of the signal envelope. Novel two kinds of human gesture classification features are computed in the form of biphase values contained in the given slice on the bispectral plane or array of biphase samples contained within the limits of the main bispectral triangular area. It is shown that phase bispectrum contains information about the shape of wireless signal envelope and, consequently, about type of human gesture. Performance of the bispectrum-based human gesture classification technique is studied experimentally. Experimental results obtained by developed measuring hardware and designed software are represented and discussed. The results obtained for the set of test human gestures demonstrate that proposed technique is non-sensitive to random wireless signal delays and magnitude variations commonly observed in closed multi-path interference environment.},
	booktitle = {2017 {Signal} {Processing} {Symposium} ({SPSympo})},
	author={Viunytskyi, Oleh and Totsky, Alexander},
	month = sep,
	year = {2017},
	keywords = {biphase, bispectrum, bispectrum-based wireless vision technique, classification features, feature extraction, Feature extraction, gesture recognition, high-frequency electromagnetic field disturbance, human gesture classification, human gesture classification feature extraction, human gesture classification technique, human gesture recognition, Interference, random wireless signal delays, Robustness, Shape, signal classification, Signal processing, Wireless communication, wireless LAN, wireless radio signal, Wireless sensor networks, wireless signal, wireless signal envelope, wireless vision},
	pages = {1--4},
}


@INPROCEEDINGS{Vlaming:2008,
    author={Vlaming, Luc and Smit, Jasper and Isenberg, Tobias},
    booktitle={2008 3rd IEEE International Workshop on Horizontal Interactive Human Computer Systems}, 
    title={{Presenting using two-handed interaction in open space}}, 
    year={2008},
    volume={},
    number={},
    pages={29-32},
    doi={10.1109/TABLETOP.2008.4660180},
}


@inproceedings{Vogel:2005,
    author = {Vogel, Daniel and Balakrishnan, Ravin},
    title = {{Distant Freehand Pointing and Clicking on Very Large, High Resolution Displays}},
    year = {2005},
    isbn = {1595932712},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1095034.1095041},
    doi = {10.1145/1095034.1095041},
    abstract = {We explore the design space of freehand pointing and clicking interaction with very large high resolution displays from a distance. Three techniques for gestural pointing and two for clicking are developed and evaluated. In addition, we present subtle auditory and visual feedback techniques to compensate for the lack of kinesthetic feedback in freehand interaction, and to promote learning and use of appropriate postures.},
    booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
    pages = {33–42},
    numpages = {10},
    keywords = {freehand gestures, pointing, whole hand interaction, very large displays},
    venue = {Seattle, WA, USA},
    series = {UIST '05},
}


@mastersthesis{Vogels:2008,
    author = {Vogels, Arthur},
    title = {{iGesture Extension for 3D Input Devices}},
    school = {Vrije Universiteit Brussel},
    type = {Master thesis},
    address = {Brussels, Belgium},
    year = {2008},
    month = nov,
}



@Article{Vogiatzidakis:2019,
    AUTHOR = {Vogiatzidakis, Panagiotis and Koutsabasis, Panayiotis},
    TITLE = {{Frame-Based Elicitation of Mid-Air Gestures for a Smart Home Device Ecosystem}},
    JOURNAL = {Informatics},
    VOLUME = {6},
    YEAR = {2019},
    NUMBER = {2},
    ARTICLE-NUMBER = {23},
    URL = {https://www.mdpi.com/2227-9709/6/2/23},
    ISSN = {2227-9709},
    ABSTRACT = {If mid-air interaction is to be implemented in smart home environments, then the user would have to exercise in-air gestures to address and manipulate multiple devices. This paper investigates a user-defined gesture vocabulary for basic control of a smart home device ecosystem, consisting of 7 devices and a total of 55 referents (commands for device) that can be grouped to 14 commands (that refer to more than one device). The elicitation study was conducted in a frame (general scenario) of use of all devices to support contextual relevance; also, the referents were presented with minimal affordances to minimize widget-specific proposals. In addition to computing agreement rates for all referents, we also computed the internal consistency of user proposals (single-user agreement for multiple commands). In all, 1047 gestures from 18 participants were recorded, analyzed, and paired with think-aloud data. The study reached to a mid-air gesture vocabulary for a smart-device ecosystem, which includes several gestures with very high, high and medium agreement rates. Furthermore, there was high consistency within most of the single-user gesture proposals, which reveals that each user developed and applied her/his own mental model about the whole set of interactions with the device ecosystem. Thus, we suggest that mid-air interaction support for smart homes should not only offer a built-in gesture set but also provide for functions of identification and definition of personalized gesture assignments to basic user commands.},
    DOI = {10.3390/informatics6020023},
}


@Article{Vogiatzidakis:2020,
    AUTHOR = {Vogiatzidakis, Panagiotis and Koutsabasis, Panayiotis},
    TITLE = {{Mid-Air Gesture Control of Multiple Home Devices in Spatial Augmented Reality Prototype}},
    JOURNAL = {Multimodal Technologies and Interaction},
    VOLUME = {4},
    YEAR = {2020},
    NUMBER = {3},
    ARTICLE-NUMBER = {61},
    URL = {https://www.mdpi.com/2414-4088/4/3/61},
    ISSN = {2414-4088},
    ABSTRACT = {Touchless, mid-air gesture-based interactions with remote devices have been investigated as alternative or complementary to interactions based on remote controls and smartphones. Related studies focus on user elicitation of a gesture vocabulary for one or a few home devices and explore recommendations of respective gesture vocabularies without validating them by empirical testing with interactive prototypes. We have developed an interactive prototype based on spatial Augmented Reality (AR) of seven home devices. Each device responds to touchless gestures (identified from a previous elicitation study) via the MS Kinect sensor. Nineteen users participated in a two-phase test (with and without help provided by a virtual assistant) according to a scenario that required from each user to apply 41 gestural commands (19 unique). We report on main usability indicators: task success, task time, errors (false negative/positives), memorability, perceived usability, and user experience. The main conclusion is that mid-air interaction with multiple home devices is feasible, fairly easy to learn and apply, and enjoyable. The contributions of this paper are (a) validation of a previously elicited gesture set; (b) development of a spatial AR prototype for testing of mid-air gestures, and (c) extensive assessment of gestures and evidence in favor of mid-air interaction in smart environments.},
    DOI = {10.3390/mti4030061},
}


@article{Vogiatzidakis:2022,
    author = {Vogiatzidakis, Panagiotis and Koutsabasis, Panayiotis},
    title = {{‘Address and Command’: Two-Handed Mid-Air Interactions with Multiple Home Devices}},
    year = {2022},
    issue_date = {Mar 2022},
    publisher = {Academic Press, Inc.},
    address = {USA},
    volume = {159},
    number = {C},
    issn = {1071-5819},
    url = {https://doi.org/10.1016/j.ijhcs.2021.102755},
    doi = {10.1016/j.ijhcs.2021.102755},
    journal = {Int. J. Hum.-Comput. Stud.},
    month = mar,
    numpages = {14},
    keywords = {Spatial augmented reality, Multiple home devices, Mid-air interaction, Smart home, Prototyping, MS kinect, Projection-mapping, Elicitation study, Gestural interaction, Ubiquitous environment, Address and command, Usability study},
}


@article{Volioti:2018,
    author     = {Volioti, Christina and Manitsaris, Sotiris and Hemery, Edgar and Hadjidimitriou, Stelios and Charisis, Vasileios and Hadjileontiadis, Leontios and Katsouli, Eleni and Moutarde, Fabien and Manitsaris, Athanasios},
    journal    = {J. Comput. Cult. Herit.},
    title      = {{A Natural User Interface for Gestural Expression and Emotional Elicitation to Access the Musical Intangible Cultural Heritage}},
    year       = {2018},
    issn       = {1556-4673},
    month      = apr,
    number     = {2},
    volume     = {11},
    address    = {New York, NY, USA},
    articleno  = {10},
    doi        = {10.1145/3127324},
    issue_date = {June 2018},
    keywords   = {Gesture recognition, sonification, emotional status, evaluation},
    numpages   = {20},
    publisher  = {Association for Computing Machinery},
    url        = {https://doi.org/10.1145/3127324},
}


%===========================================================
% References starting by W
%===========================================================
@inproceedings{Walter:2013,
    author = {Walter, Robert and Bailly, Gilles and M\"{u}ller, J\"{o}rg},
    title = {{StrikeAPose: revealing mid-air gestures on public displays}},
    year = {2013},
    isbn = {9781450318990},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2470654.2470774},
    doi = {10.1145/2470654.2470774},
    abstract = {We investigate how to reveal an initial mid-air gesture on interactive public displays. This initial gesture can serve as gesture registration for advanced operations. We propose three strategies to reveal the initial gesture: spatial division, temporal division and integration. Spatial division permanently shows the gesture on a dedicated screen area. Temporal division interrupts the application to reveal the gesture. Integration embeds gesture hints directly in the application. We also propose a novel initial gesture called Teapot to illustrate our strategies. We report on a laboratory and field study. Our main findings are: A large percentage of all users execute the gesture, especially with spatial division (56\%). Users intuitively discover a gesture vocabulary by exploring variations of the Teapot gesture by themselves, as well as by imitating and extending other users' variations.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {841–850},
    numpages = {10},
    keywords = {revelation, public displays, initial gesture, field study},
    location = {Paris, France},
    series = {CHI '13},
}


@inproceedings{Wan:2014,
	title = {{Gesture recognition for smart home applications using portable radar sensors}},
	doi = {10.1109/EMBC.2014.6945096},
	abstract = {In this article, we consider the design of a human gesture recognition system based on pattern recognition of signatures from a portable smart radar sensor. Powered by AAA batteries, the smart radar sensor operates in the 2.4 GHz industrial, scientific and medical (ISM) band. We analyzed the feature space using principle components and application-specific time and frequency domain features extracted from radar signals for two different sets of gestures. We illustrate that a nearest neighbor based classifier can achieve greater than 95\% accuracy for multi class classification using 10 fold cross validation when features are extracted based on magnitude differences and Doppler shifts as compared to features extracted through orthogonal transformations. The reported results illustrate the potential of intelligent radars integrated with a pattern recognition system for high accuracy smart home and health monitoring purposes.},
	booktitle = {2014 36th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}},
	author={Wan, Qian and Li, Yiran and Li, Changzhi and Pal, Ranadip},
	month = aug,
	year = {2014},
	note = {ISSN: 1558-4615},
	keywords = {AAA batteries, Accuracy, application specific time, Automated, Doppler effect, Doppler radar, Doppler shifts, Feature extraction, feature space, frequency 2.4 GHz, frequency domain features, gesture recognition, Gesture recognition, Gestures, health monitoring, human gesture recognition, Humans, industrial, intelligent radars, ISM band, Monitoring, Movement, multiclass classification, pattern recognition, Pattern Recognition, Physiologic, portable smart radar sensor, Principal component analysis, Principal Component Analysis, principle components, Radar, radar signal processing, radar signals, scientific and medical band, Sensors, signal classification, smart home applications},
	pages = {6414--6417},
}


@inproceedings{Wang:2016,
    author = {Wang, Saiwen and Song, Jie and Lien, Jaime and Poupyrev, Ivan and Hilliges, Otmar},
    title = {{Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum}},
    year = {2016},
    isbn = {9781450341899},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2984511.2984565},
    doi = {10.1145/2984511.2984565},
    abstract = {This paper proposes a novel machine learning architecture, specifically designed for
    radio-frequency based gesture recognition. We focus on high-frequency (60]GHz), short-range
    radar based sensing, in particular Google's Soli sensor. The signal has unique properties
    such as resolving motion at a very fine level and allowing for segmentation in range
    and velocity spaces rather than image space. This enables recognition of new types
    of inputs but poses significant difficulties for the design of input recognition algorithms.
    The proposed algorithm is capable of detecting a rich set of dynamic gestures and
    can resolve small motions of fingers in fine detail. Our technique is based on an
    end-to-end trained combination of deep convolutional and recurrent neural networks.
    The algorithm achieves high recognition rates (avg 87\%) on a challenging set of 11
    dynamic gestures and generalizes well across 10 users. The proposed model runs on
    commodity hardware at 140 Hz (CPU only).},
    booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
    pages = {851–860},
    numpages = {10},
    keywords = {gesture recognition, deep learning, wearables, radar sensing},
    venue = {Tokyo, Japan},
    series = {UIST '16},
}


@inproceedings{Wang:2019a,
	title = {Two-{Stream} {Time} {Sequential} {Network} {Based} {Hand} {Gesture} {Recognition} {Method} {Using} {Radar} {Sensor}},
	doi = {10.1109/GCWkshps45667.2019.9024691},
	abstract = {This paper proposes a deep learning based twostream time series hand gesture recognition method using the frequency modulated continuous wave (FMCW) radar. Firstly, we collect the hand gesture data by the FMCW radar, and the range and Doppler of the hand gesture are estimated by the 2 dimensional Fast Fourier Transform (2D-FFT). Then, the angle of hand gesture is estimated by Multiple Signal classification (MUSIC) algorithm. Afterward, we construct the Range- Doppler Map (RDM), and generate the Angle-Time Map (ATM) via multiframe accumulation. The interference in RDM is filtered out by peak interference cancellation, and the hand gesture feature in RDM and ATM are enhanced by wavelet transform. A systematic of two-stream time series neural network is designed for gesture feature extraction and classification. The experimental results show that the recognition accuracy rate for each type hand gesture of the proposed method is higher than 95\%.},
	booktitle = {2019 {IEEE} {Globecom} {Workshops} ({GC} {Wkshps})},
	author={Wang, Yong and Wang, Shasha and Zhou, Mu and Nie, Wei and Yang, Xiaolong and Tian, Zengshan},
	month = dec,
	year = {2019},
	keywords = {2 dimensional fast Fourier transform, angle-time map, CW radar, deep learning, Doppler radar, fast Fourier transforms, feature extraction, Feature extraction, FM radar, FMCW radar, frequency modulated continuous wave radar, gesture feature extraction, gesture recognition, Gesture recognition, hand gesture data, hand gesture feature, image classification, Interference, learning (artificial intelligence), Multiple signal classification, multiple signal classification algorithm, MUSIC algorithm, neural nets, peak interference cancellation, Radar, radar sensor, radar signal processing, range- Doppler map, RDM, recognition accuracy rate, time series, Time series analysis, time series hand gesture recognition method, two-stream time sequential network, two-stream time series neural network, wavelet transform, wavelet transforms, Wavelet transforms},
	pages = {1--6},
}


@inproceedings{Wang:2019b,
	title = {Fine-{Grained} {Gesture} {Recognition} {Based} on {High} {Resolution} {Range} {Profiles} of {Terahertz} {Radar}},
	doi = {10.1109/IGARSS.2019.8900601},
	abstract = {As one of the most commonly used natural languages, gesture has become an important approach for people to interact with machines. However, the flexibility of fingers has posed a challenge for detecting fine-grained gestures. In this paper, a novel method of high precision fine-grained gesture recognition is proposed based on a terahertz radar, which is able to sense any gesture movement when its range of motion is greater than 5mm. First, High Resolution Range Profile (HRRP) sequences are extracted from radar echoes. Then, the HRRP features are fed to Random Forest classifier after dimensionality reduction by PCA. In order to verify the proposed method is effective to detect fine-grained gestures, four kinds of similar gestures with multi-fingers are designed for experiments. The results indicate the recognition rate exceeds 99.7\%, which demonstrates a great prospect in fine-grained gesture recognition using a terahertz radar.},
	booktitle = {{IGARSS} 2019 - 2019 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author={Wang, Liying and Cui, Zongyong and Cao, Zongjie and Xu, Shengping and Min, Rui},
	month = jul,
	year = {2019},
	note = {ISSN: 2153-7003},
	keywords = {commonly used natural languages, feature extraction, Feature extraction, fine-grained gestures, gesture movement, gesture recognition, Gesture recognition, high precision fine-grained gesture recognition, High Resolution Range Profile, High Resolution Range Profile sequences, High Resolution Range profiles, Magnetic sensors, principal component analysis, Principal component analysis, Radar, radar echoes, radar resolution, radar target recognition, Radio frequency, similar gestures, size 5.0 mm, terahertz radar},
	pages = {1470--1473},
}


@article{Wang:2019c,
	title = {A novel {F}-{RCNN} based hand gesture detection approach for {FMCW} systems},
	issn = {1022-0038, 1572-8196},
	url = {http://link.springer.com/10.1007/s11276-019-02096-2},
	doi = {10.1007/s11276-019-02096-2},
	language = {en},
	urldate = {2020-12-21},
	journal = {Wireless Networks},
	author = {Wang, Yong and Jia, Xiuqian and Zhou, Mu and Xie, Liangbo and Tian, Zengshan},
	month = jul,
	year = {2019},
    pages = {1--14}
}


@article{Wang:2019d,
	title = {{Enabling non-invasive and real-time human-machine interactions based on wireless sensing and fog computing}},
	volume = {23},
	issn = {1617-4909, 1617-4917},
	url = {http://link.springer.com/10.1007/s00779-018-1185-7},
	doi = {10.1007/s00779-018-1185-7},
	language = {en},
	number = {1},
	urldate = {2020-12-21},
	journal = {Personal and Ubiquitous Computing},
	author = {Wang, Zhu and Lou, Xinye and Yu, Zhiwen and Guo, Bin and Zhou, Xingshe},
	month = feb,
	year = {2019},
	pages = {29--41},
}


@incollection{Wang:2019e,
	address = {Cham},
	title = {Two {Dimensional} {Parameters} {Based} {Hand} {Gesture} {Recognition} {Algorithm} for {FMCW} {Radar} {Systems}},
	volume = {280},
	isbn = {978-3-030-19152-8 978-3-030-19153-5},
	url = {http://link.springer.com/10.1007/978-3-030-19153-5_23},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Wireless and {Satellite} {Systems}},
	publisher = {Springer International Publishing},
	author = {Wang, Yong and Zhao, Zedong and Zhou, Mu and Wu, Jinjun},
	editor = {Jia, Min and Guo, Qing and Meng, Weixiao},
	year = {2019},
	doi = {10.1007/978-3-030-19153-5_23},
	note = {Series Title: Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},
	pages = {226--234},
}


@article{Wang:2019f,
	title = {{TS}-{I3D} {Based} {Hand} {Gesture} {Recognition} {Method} {With} {Radar} {Sensor}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2897060},
	abstract = {Aiming at the problems of the noise impact on the parametric image of hand gestures, the difficulty of gesture feature extraction, and the inefficient utilization of continuous gesture time sequential information, we propose a time sequential inflated 3 dimensions (TS-I3D) convolutional neural network approach for hand gesture recognition based on frequency modulated continuous wave (FMCW) radar sensor. Specifically, the FMCW radar is used to acquire the hand gesture data, and the range and speed of the gesture in each frame signal are calculated by 2 dimensions fast Fourier transform. Then, the range-Doppler map (RDM) is generated based on the relationship between motion parameters and frequency. The interference in RDM caused by people and the external environment is filtered out and the peak of hand gesture in RDM is further enhanced by wavelet transform. Finally, TS-I3D network is designed to extract the range and speed change information of the continuous gestures. The experimental results show that the average recognition accuracy rate of the hand gestures of the proposed method is 96.17\%.},
	journal = {IEEE Access},
	author={Wang, Yong and Wang, Shasha and Zhou, Mu and Jiang, Qing and Tian, Zengshan},
	year = {2019},
	keywords = {continuous gesture time sequential information, continuous gestures, convolutional neural nets, Convolutional neural networks, CW radar, Data mining, deep learning, fast Fourier transforms, feature extraction, Feature extraction, FM radar, FMCW radar, frequency modulated continuous wave radar sensor, gesture feature extraction, gesture recognition, Gesture recognition, hand gesture data, hand gesture recognition, Interference, interference filtering, LSTM, radar computing, Radar imaging, RDM, TS-I3D based hand gesture recognition method, TS-I3D network, wavelet transforms},
	pages = {22902--22913},
}


@inproceedings{Wang:2019g,
	doi = {10.22260/ISARC2019/0157},
	year = 2019,
	month = may,
	author = {Wang, Ruoyu and Xiang, Siyuan and Feng, Chen and Wang, Pu and Ergan, Semiha and Fang, Yi},
	title = {{Through-Wall Object Recognition and Pose Estimation}},
	booktitle = {Proceedings of the 36th International Symposium on Automation and Robotics in Construction},
	series = {ISARC '19},
	url={https://www.iaarc.org/publications/2019_proceedings_of_the_36th_isarc/through_wall_object_recognition_and_pose_estimation.html},
	dates={21-24 May 2019},
	venue={Banff},
	isbn = {978-952-69524-0-6},
	publisher = {International Association for Automation and Robotics in Construction (IAARC)},
	editor = {Al-Hussein, Mohamed},
	pages = {1176-1183},
	address = {Banff, Canada},
}


@article{Wang:2020a,
	title={{A Gesture Air-Writing Tracking Method that Uses 24 GHz SIMO Radar SoC}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3017869},
	abstract = {Gesture air-writing is an advanced nontouching interaction method that replaces traditional typewriter keyboards, touch screens and other input devices. Due to its low power consumption, noncontact detection and independence from light conditions, millimeter wave radar has become a valuable gesture air-writing solution. In this paper, we proposed a prototype of a gesture air-writing tracking system that is based on the 24-GHz frequency-modulated continuous-wave (FMCW) radar system-on-chip (SoC). The transmitted chirp signal of this radar chip covers up to a 4-GHz bandwidth, which provides sufficient range resolution to track hand gestures. With the development of single input, multiple output (SIMO) antennas, the air-writing symbols can be reconstructed in an observation plane. A system design and signal processing algorithm for gesture air-writing applications is proposed for the prototype system in this paper. To test the performance of the proposed method, experiments with cases of five simple gestures, air-writing numbers and letter tracking are carried out. The experimental results verify the efficiency and accuracy of the proposed method.},
	journal = {IEEE Access},
	author={Wang, Pengcheng and Lin, Junyang and Wang, Fuyue and Xiu, Jianping and Lin, Yue and Yan, Na and Xu, Hongtao},
	year = {2020},
	keywords = {24-GHz frequency-modulated continuous-wave, advanced nontouching interaction method, air-writing numbers, air-writing symbols, angular measurement, Chirp, CW radar, distance measurement, FM radar, FMCW, frequency 24.0 GHz, Gesture air writing, gesture air-writing applications, gesture air-writing tracking method, gesture air-writing tracking system, gesture recognition, hand gestures, indoor communication, input devices, letter tracking, low power consumption, microwave antenna arrays, millimeter wave radar, Millimeter wave radar, multiple output antennas, noncontact detection, prototype system, Radar antennas, radar chip, radar system-on-chip, Radar tracking, Receiving antennas, signal processing algorithm, SIMO, SIMO radar SoC, simple gestures, single input, SoC, system design, touch screens, traditional typewriter keyboards, transmitted chirp signal, valuable gesture air-writing solution, wireless channels, Writing},
	pages = {152728--152741},
}


@article{Wang:2020b,
	title = {Negative {Latency} {Recognition} {Method} for {Fine}-{Grained} {Gestures} {Based} on {Terahertz} {Radar}},
	volume = {58},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2020.2985421},
	abstract = {Noncontact gesture recognition is gradually being applied to emerging applications, such as smart cars and smart phones. Negative latency gesture recognition (recognition before a gesture is finished) is desirable due to the instantaneous feedback. However, it is difficult for existing methods to achieve a high precision and negative latency gesture recognition. A fragment can provide too few features to directly identify all gestures well. By observing a large number of existing gesture sets and people's daily operating habits, we found that some high frequency used gestures are similar. To the best of our knowledge, it is the first time to redivide the gestures into two subsets according to their movement physical states. We divided the gestures with different shapes or motion states into a parent-class subset, and further divided each pair of parent-class gestures to obtain a child-class subset. In order to achieve a better tradeoff between the high-precision and negative latency, an approach of motion pattern and behavior intention (MPBI) is proposed. Taking full advantage of the characteristics of each subset, MPBI includes two models. First, pattern model coarsely classify the parent-class gestures by a convolutional network, and then intention model further classifies child-class gestures according to their opposite motion direction. MPBI is evaluated on a 340-GHz terahertz radar. With the advantage of its accurate ranging, intention model can recognize child-class gestures directly without training. MPBI is evaluated on 12 gestures and achieves a recognition accuracy of 94.13\%, which only needs a 0.033-s gesture fragment as an input sample.},
	number = {11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author={Wang, Liying and Cao, Zongjie and Cui, Zongyong and Cao, Changjie and Pi, Yiming},
	month = nov,
	year = {2020},
	keywords = {Acoustics, child-class gestures, child-class subset, convolutional network, convolutional neural nets, Doppler radar, fine-grained gestures, gesture fragment, gesture recognition, Gesture recognition, high resolution range profile (HRRP), Human computer interaction, image classification, Indexes, learning (artificial intelligence), motion pattern and behavior intention, MPBI, negative latency gesture recognition, noncontact gesture recognition, parent-class gestures, parent-class subset, radar imaging, Radar imaging, real-time, terahertz radar},
	pages = {7955--7968},
}


@article{Wang:2020c,
	title = {Dynamic {Hand} {Gesture} {Recognition} {Based} on {Micro}-{Doppler} {Radar} {Signatures} {Using} {Hidden} {Gauss}-{Markov} {Models}},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2020.2974821},
	abstract = {Dynamic hand gesture recognition using the microwave or millimeter-wave radar sensors has become a typical technology for many human-computer interaction (HCI) applications. In this letter, a novel method is proposed for dynamic hand gesture recognition based on micro-Doppler radar signatures. The short-time Fourier transform is carried out on the raw data to obtain the time-frequency spectrogram. The time-frequency spectrograms associated with the same dynamic hand gesture are modeled by a hidden Gauss-Markov model (HGMM), and the testing gesture is recognized by the maximum likelihood criterion. Experimental results with real radar data demonstrate that the proposed method has a strong generalization ability for radar gesture recognition in the cases of low signal-to-noise ratio (SNR) and unknown users.},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author={Wang, Zetao and Li, Gang and Yang, Le},
	year = {2020},
	keywords = {Dynamic hand gesture recognition, Feature extraction, Gesture recognition, hidden Markov model (HMM), Hidden Markov models, micro-Doppler, Radar, radar sensor., Sensors, Testing, Training},
	pages = {291--295},
    volume = {18},
    number = {2},
}


@article{Wang:2020d,
	title = {A {Novel} {Detection} and {Recognition} {Method} for {Continuous} {Hand} {Gesture} {Using} {FMCW} {Radar}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3023187},
	abstract = {In this article, a novel method for continuous hand gesture detection and recognition is proposed based on a frequency modulated continuous wave (FMCW) radar. Firstly, we adopt the 2-Dimensional Fast Fourier Transform (2D-FFT) to estimate the range and Doppler parameters of the hand gesture raw data, and construct the range-time map (RTM) and Doppler-time map (DTM). Meanwhile, we apply the Multiple Signal Classification (MUSIC) algorithm to calculate the angle and construct the angle-time map (ATM). Secondly, a hand gesture detection method is proposed to segment the continuous hand gestures using a decision threshold. Thirdly, the central time-frequency trajectory of each hand gesture spectrogram is clustered using the k-means algorithm, and then the Fusion Dynamic Time Warping (FDTW) algorithm is presented to recognize the hand gestures. Finally, experiments show that the accuracy of the proposed hand gesture detection method can reach 96.17\%. The hand gesture average recognition accuracy of the proposed FDTW algorithm is 95.83\%, while its time complexity is reduced by more than 50\%.},
	journal = {IEEE Access},
	author={Wang, Yong and Ren, Aihu and Zhou, Mu and Wang, Wen and Yang, Xiaobo},
	year = {2020},
	keywords = {2-Dimensional Fast Fourier Transform, angle-time map, central time-frequency trajectory, Classification algorithms, Clustering algorithms, continuous hand gesture detection, continuous hand gesture recognition detection, continuous wave radar, CW radar, fast Fourier transforms, FDTW, feature extraction, FM radar, FMCW radar, Fusion Dynamic Time Warping algorithm, gesture recognition, Gesture recognition, hand gesture average recognition accuracy, hand gesture detection method, hand gesture raw data, hand gesture spectrogram, Heuristic algorithms, image matching, Multiple Signal Classification algorithm, Radar detection, radar imaging, recognition method, Time-frequency analysis, time-frequency trajectory},
	pages = {167264--167275},
}


@ARTICLE{Wang:2022,
    author={Wang, Jinqiang and Cao, Dianguo and Li, Yang and Wang, Jiashuai and Wu, Yuqiang},
    journal={Frontiers in Neurorobotics}, 
    title={{Multi-user motion recognition using sEMG via discriminative canonical correlation analysis and adaptive dimensionality reduction}}, 
    year={2022},
    volume={16},
    number={997134},
    doi={10.3389/fnbot.2022.997134},
    url={https://ieeexplore.ieee.org/document/9385096},
    url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9650084/},
}


@article{Warren:2016,
    title = {{gprMax: Open source software to simulate electromagnetic wave propagation for Ground Penetrating Radar}},
    journal = {Computer Physics Communications},
    volume = {209},
    pages = {163-170},
    year = {2016},
    issn = {0010-4655},
    doi = {https://doi.org/10.1016/j.cpc.2016.08.020},
    url = {https://www.sciencedirect.com/science/article/pii/S0010465516302533},
    author = {Warren, Craig and Giannopoulos, Antonios and Giannakis, Iraklis},
    keywords = {Computational electromagnetism, Ground Penetrating Radar, Finite-Difference Time-Domain, Open source, Python},
    abstract = {gprMax is open source software that simulates electromagnetic wave propagation, using the Finite-Difference Time-Domain (FDTD) method, for the numerical modelling of Ground Penetrating Radar (GPR). gprMax was originally developed in 1996 when numerical modelling using the FDTD method and, in general, the numerical modelling of GPR were in their infancy. Current computing resources offer the opportunity to build detailed and complex FDTD models of GPR to an extent that was not previously possible. To enable these types of simulations to be more easily realised, and also to facilitate the addition of more advanced features, gprMax has been redeveloped and significantly modernised. The original C-based code has been completely rewritten using a combination of Python and Cython programming languages. Standard and robust file formats have been chosen for geometry and field output files. New advanced modelling features have been added including: an unsplit implementation of higher order Perfectly Matched Layers (PMLs) using a recursive integration approach; diagonally anisotropic materials; dispersive media using multi-pole Debye, Drude or Lorenz expressions; soil modelling using a semi-empirical formulation for dielectric properties and fractals for geometric characteristics; rough surface generation; and the ability to embed complex transducers and targets.
    Program summary
    Program title: gprMax Catalogue identifier: AFBG_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFBG_v1_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GNU GPL v3 No. of lines in distributed program, including test data, etc.: 627180 No. of bytes in distributed program, including test data, etc.: 26762280 Distribution format: tar.gz Programming language: Python. Computer: Any computer with a Python interpreter and a C compiler. Operating system: Microsoft Windows, Mac OS X, and Linux. RAM: Problem dependent Classification: 10. External routines: Cython[1], h5py[2], matplotlib[3], NumPy[4], mpi4py[5] Nature of problem: Classical electrodynamics Solution method: Finite-Difference Time-Domain (FDTD) Running time: Problem dependent References:[1]Cython, http://www.cython.org[2]h5py, http://www.h5py.org[3]matplotlib, http://www.matplotlib.org[4]NumPy, http://www.numpy.org[5]mpi4py, http://mpi4py.scipy.org},
}


@article{Weichert:2013,
    author = {Weichert, Frank and Bachmann, Daniel and Rudak, Bartholomäus and Fisseler, Denis},
    year = {2013},
    month = {05},
    pages = {6380-6393},
    title = {{Analysis of the Accuracy and Robustness of the Leap Motion Controller}},
    volume = {13},
    journal = {Sensors},
    venue={Basel, Switzerland},
    doi = {10.3390/s130506380},
    url={https://www.mdpi.com/1424-8220/13/5/6380},
}


@inproceedings{Westeyn:2003,
    author = {Westeyn, Tracy and Brashear, Helene and Atrash, Amin and Starner, Thad},
    title = {{Georgia Tech Gesture Toolkit: Supporting Experiments in Gesture Recognition}},
    year = {2003},
    isbn = {1581136218},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/958432.958452},
    doi = {10.1145/958432.958452},
    booktitle = {Proceedings of the 5th International Conference on Multimodal Interfaces},
    pages = {85–92},
    numpages = {8},
    keywords = {hidden Markov models, american sign language, wearable computers, context recognition, gesture recognition, interfaces, toolkit},
    venue = {Vancouver, British Columbia, Canada},
    series = {ICMI ’03},
}


@INPROCEEDINGS{White:2007,
    author={White, Sean and Lister, Levi and Feiner, Steven},
    booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, 
    title={{Visual Hints for Tangible Gestures in Augmented Reality}}, 
    year={2007},
    volume={},
    number={},
    pages={47-50},
    keywords={Augmented reality;Displays;User interfaces;Chromium;Multimedia systems;Virtual reality;Documentation;Morphology;Radio access networks;History;Tangible augmented reality;visual hints;gestures;electronic field guide},
    doi={10.1109/ISMAR.2007.4538824},
}



@book{Wigdor:2011,
    title =     {{Brave NUI world: designing natural user interfaces for touch and gesture}},
    author =    {Wigdor, Daniel and Wixon, Dennis},
    publisher = {Morgan Kaufmann Publishers},
    address = {Burlington, Massachusetts, United States},
    isbn =      {978-0-12-382231-4},
    year =      {2011},
    series =    {The Morgan Kaufmann Series in Computer Architecture and Design},
    edition =   {1},
    doi =    {https://doi.org/10.1016/C2009-0-64091-5},
    url =       {https://www.sciencedirect.com/book/9780123822314/brave-nui-world},
}


@inproceedings{Wittorf:2016,
    author = {Wittorf, Markus L. and Jakobsen, Mikkel R.},
    title = {{Eliciting Mid-Air Gestures for Wall-Display Interaction}},
    year = {2016},
    isbn = {9781450347631},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2971485.2971503},
    doi = {10.1145/2971485.2971503},
    abstract = {Freehand mid-air gestures are a promising input method for interacting with wall displays. However, work on mid-air gestures for wall-display interaction has mainly explored what is technically possible, which might not result in gestures that users would prefer. This paper presents a guessability study where 20 participants performed gestures for 25 actions on a three-meter wide display. Based on the resulting 1124 gestures, we describe user-defined mid-air gestures for wall-display interaction and characterize the types of gesture users prefer for this context. The resulting gestures were largely influenced by surface interaction; they tended to be larger and more physically-based than gestures elicited in previous studies using smaller displays.},
    booktitle = {Proceedings of the 9th Nordic Conference on Human-Computer Interaction},
    articleno = {3},
    numpages = {4},
    keywords = {Gesture elicitation, Mid-air gestures, Wall displays},
    venue = {Gothenburg, Sweden},
    series = {NordiCHI '16},
}


@inproceedings{Wobbrock:2005,
    author = {Wobbrock, Jacob O. and Aung, Htet Htet and Rothrock, Brandon and Myers, Brad A.},
    title = {{Maximizing the Guessability of Symbolic Input}},
    year = {2005},
    isbn = {1595930027},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1056808.1057043},
    doi = {10.1145/1056808.1057043},
    abstract = {Guessability is essential for symbolic input, in which users enter gestures or keywords to indicate characters or commands, or rely on labels or icons to access features. We present a unified approach to both maximizing and evaluating the guessability of symbolic input. This approach can be used by anyone wishing to design a symbol set with high guessability, or to evaluate the guessability of an existing symbol set. We also present formulae for quantifying guessability and agreement among guesses. An example is offered in which the guessability of the EdgeWrite unistroke alphabet was improved by users from 51.0\% to 80.1\% without designer intervention. The original and improved alphabets were then tested for their immediate usability with the procedure used by MacKenzie and Zhang (1997). Users entered the original alphabet with 78.8\% and 90.2\% accuracy after 1 and 5 minutes of learning, respectively. The improved alphabet bettered this to 81.6\% and 94.2\%. These improved results were competitive with prior results for Graffiti, which were 81.8\% and 95.8\% for the same measures.},
    booktitle = {CHI '05 Extended Abstracts on Human Factors in Computing Systems},
    pages = {1869–1872},
    numpages = {4},
    keywords = {symbols, labels, keywords, command-line, gestures, edgewrite, immediate usability, icons, unistrokes, guessability, referents, proposals, text entry, commands},
    venue = {Portland, OR, USA},
    series = {CHI EA '05},
}


@inproceedings{Wobbrock:2007,
    author = {Wobbrock, Jacob O. and Wilson, Andrew D. and Li, Yang},
    title = {{Gestures Without Libraries, Toolkits or Training: A \$1 Recognizer for User Interface Prototypes}},
    booktitle = {Proceedings of the 20th Annual ACM Symposium on User Interface Software and Technology},
    series = {UIST '07},
    year = {2007},
    isbn = {978-1-59593-679-0},
    venue = {Newport, Rhode Island, USA},
    pages = {159--168},
    numpages = {10},
    url = {http://doi.acm.org/10.1145/1294211.1294238},
    doi = {10.1145/1294211.1294238},
    acmid = {1294238},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {dynamic time warping, gesture recognition, marks, rapid prototyping, recognition rates, rubine, statistical classifiers, strokes, symbols, unistrokes, user interfaces},
} 


@inproceedings{Wobbrock:2009,
	address = {New York, {NY}, {USA}},
	title = {{User-defined Gestures for Surface Computing}},
	isbn = {978-1-60558-246-7},
	url = {http://doi.acm.org/10.1145/1518701.1518866},
	doi = {10.1145/1518701.1518866},
	series = {{CHI} '09},
	venue = {Boston, MA, USA},
	abstract = {Many surface computing prototypes have employed gestures created by system designers. Although such gestures are appropriate for early investigations, they are not necessarily reflective of user behavior. We present an approach to designing tabletop gestures that relies on eliciting gestures from non-technical users by first portraying the effect of a gesture, and then asking users to perform its cause. In all, 1080 gestures from 20 participants were logged, analyzed, and paired with think-aloud data for 27 commands performed with 1 and 2 hands. Our findings indicate that users rarely care about the number of fingers they employ, that one hand is preferred to two, that desktop idioms strongly influence users' mental models, and that some commands elicit little gestural agreement, suggesting the need for on-screen widgets. We also present a complete user-defined gesture set, quantitative agreement scores, implications for surface technology, and a taxonomy of surface gestures. Our results will help designers create better gesture sets informed by user behavior.},
	pages = {1083--1092},
	publisher = {{ACM}},
	booktitle = {Proceedings of the ACM Conference on Human Factors in Computing Systems},
	author = {Wobbrock, Jacob O. and Morris, Meredith Ringel and Wilson, Andrew D.},
	urldate = {2018-02-23},
	date = {2009},
	keywords = {{ACM}, gesture recognition, gestures, guessability, referents, signs, surface, tabletop, think-aloud},
}


@misc{Wolff:2022, 
    title={{Waves and Frequency Ranges}}, 
    url={https://www.radartutorial.eu/07.waves/Waves and Frequency Ranges.en.html}, 
    journal={RadarTutorial.eu}, 
    author={Wolff, Christian}, 
    year={2022}, 
    month=jan,
} 


@inproceedings{Worth:2003,
    author    = {Worth, Carl D.},
    title     = {{xstroke: Full-screen Gesture Recognition for X}},
    booktitle = {Proceedings of the {FREENIX} Track: 2003 {USENIX} Annual Technical
               Conference, June 9-14, 2003, San Antonio, Texas, {USA}},
    pages     = {187--196},
    year      = {2003},
    url       = {http://www.usenix.org/events/usenix03/tech/freenix03/worth.html},
    timestamp = {Wed, 03 Sep 2003 14:02:56 +0200},
    biburl    = {https://dblp.org/rec/bib/conf/usenix/Worth03},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@article{Wu:2019,
    title = {{A new drone-borne GPR for soil moisture mapping}},
    journal = {Remote Sensing of Environment},
    volume = {235},
    pages = {111456},
    year = {2019},
    issn = {0034-4257},
    doi = {https://doi.org/10.1016/j.rse.2019.111456},
    url = {https://www.sciencedirect.com/science/article/pii/S0034425719304754},
    author = {Wu, Kaijun and Arambulo Rodriguez, Gabriela and Zajc, Marjana and Jacquemin, Elodie and Clément, Michiels and De Coster, Albéric and Lambot, Sébastien},
    keywords = {Ground-penetrating radar, GPR, Drone, Full-wave inversion, Soil moisture mapping},
    abstract = {In this study, we set up a new drone-borne ground-penetrating radar (GPR) for soil moisture mapping. The whole radar system weighs 1.5 kg and consists of a handheld vector network analyzer (VNA) working as frequency domain radar, a lightweight hybrid horn-dipole antenna covering a wide frequency range (250–2800 MHz), a GPS for positioning, a microcomputer with the controlling application, and a smartphone for remote control. Soil moisture is derived from the radar data using full-wave inverse modeling based on the radar equation of Lambot et al. and multilayered media Green's functions. The inversion is performed in the time domain and focuses on the surface reflection. The antenna-drone system is characterized by global reflection and transmission functions which are determined through a calibration procedure. We performed drone-GPR measurements over three different agricultural fields in the loess belt region of Belgium. In this study, we used the 500–700 MHz range to avoid soil surface roughness effects and to focus on the top 10–20 cm of the soil. These fields present a range of landform conditions leading to specific soil moisture distributions. The soil moisture maps were constructed from the local measurements using kriging. The obtained soil moisture maps are in good agreement with the topographical conditions of the fields and aerial orthophotography observations. These results demonstrated the potential and benefits of drone-GPR for fast, high-resolution mapping of soil moisture at the field scale, and to support, e.g., precision agriculture and environmental monitoring.},
}


@inproceedings{Wu:2020,
	address = {New York, NY, USA},
	series = {{CHI} '20},
	title = {Fabriccio: {Touchless} {Gestural} {Input} on {Interactive} {Fabrics}},
	isbn = {978-1-4503-6708-0},
	shorttitle = {Fabriccio},
	url = {https://doi.org/10.1145/3313831.3376681},
	doi = {10.1145/3313831.3376681},
	abstract = {We present Fabriccio, a touchless gesture sensing technique developed for interactive fabrics using Doppler motion sensing. Our prototype was developed using a pair of loop antennas (one for transmitting and the other for receiving), made of conductive thread that was sewn onto a fabric substrate. The antenna type, configuration, transmission lines, and operating frequency were carefully chosen to balance the complexity of the fabrication process and the sensitivity of our system for touchless hand gestures, performed at a 10 cm distance. Through a ten-participant study, we evaluated the performance of our proposed sensing technique across 11 touchless gestures as well as 1 touch gesture. The study result yielded a 92.8\% cross-validation accuracy and 85.2\% leave-one-session-out accuracy. We conclude by presenting several applications to demonstrate the unique interactions enabled by our technique on soft objects.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Wu, Te-Yen and Qi, Shutong and Chen, Junchi and Shang, MuJie and Gong, Jun and Seyed, Teddy and Yang, Xing-Dong},
	month = apr,
	year = {2020},
	keywords = {doppler effect, interactive fabrics, touchless gesture},
	pages = {1--14},
}


@article{Wu:2022,  
    author={Wu, Kaijun and Lambot, Sébastien}, 
    journal={IEEE Transactions on Geoscience and Remote Sensing},   
    title={{Effect of Radar Incident Angle on Full-Wave Inversion for the Retrieval of Medium Surface Permittivity for Drone-Borne Applications}},   
    year={2022},  
    volume={60},  
    number={},  
    pages={1-10},  
    doi={10.1109/TGRS.2022.3157370},
    url={https://ieeexplore.ieee.org/document/9729749},
}


%===========================================================
% References starting by X
%===========================================================
@article{Xia:2020,
	title = {Multidimensional {Feature} {Representation} and {Learning} for {Robust} {Hand}-{Gesture} {Recognition} on {Commercial} {Millimeter}-{Wave} {Radar}},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2020.3010880},
	abstract = {This article presents a robust hand-gesture recognition method via multidimensional feature representation and learning specifically designed for commercial frequency-modulated continuous wave (FMCW) multi-input multi-output (MIMO) millimeter-wave radar. First, the optimal configuration of the radar system parameters for the hand-gesture recognition scenario is investigated and a standard procedure to determine the system configuration is given. Then a moving scattering center model is proposed to represent the 3-D point cloud in the range-Doppler (RD)-angular multidimensional feature space. A scattering point detection and tracking algorithm is presented based on a set of motion constraints in terms of position, velocity, and acceleration. It is derived from the space-time continuity of a nonrigid target. Finally, a lightweight multichannel convolutional neural network (CNN) is designed to learn and classify multidimensional gesture features including radial RD and tangential azimuth-elevation. Extensive experiments are carried out with the developed system and a large data set is obtained to train and test the classifier. The results show that the proposed gesture recognition method can effectively distinguish gestures that are easily confused in the RD domain and achieve robust performances under various conditions.},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author={Xia, Zhaoyang and Luomei, Yixiang and Zhou, Chenglong and Xu, Feng},
	year = {2020},
	keywords = {Detection and tracking, Feature extraction, gesture recognition, Gesture recognition, millimeter-wave (mmw) radar, moving scattering center model, multichannel convolutional neural network (CNN), multidimensional feature., Radar imaging, Robustness, Scattering, Signal resolution},
	pages = {4749--4764},
    volume = {59},
    number = {6},
}


@ARTICLE{Xu:2021,
    author={Xu, Hao and Xiong, Anbin},
    journal={IEEE Sensors Journal}, 
    title={{Advances and Disturbances in sEMG-Based Intentions and Movements Recognition: A Review}}, 
    year={2021},
    volume={21},
    number={12},
    pages={13019-13028},
    doi={10.1109/JSEN.2021.3068521},
    url={https://ieeexplore.ieee.org/document/9385096}
}


@inproceedings{Xu:2022,
    author = {Xu, Xuhai and Gong, Jun and Brum, Carolina and Liang, Lilian and Suh, Bongsoo and Gupta, Shivam Kumar and Agarwal, Yash and Lindsey, Laurence and Kang, Runchang and Shahsavari, Behrooz and Nguyen, Tu and Nieto, Heriberto and Hudson, Scott E and Maalouf, Charlie and Mousavi, Jax Seyed and Laput, Gierad},
    title = {{Enabling Hand Gesture Customization on Wrist-Worn Devices}},
    year = {2022},
    isbn = {9781450391573},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3491102.3501904},
    doi = {10.1145/3491102.3501904},
    abstract = {We present a framework for gesture customization requiring minimal examples from users, all without degrading the performance of existing gesture sets. To achieve this, we first deployed a large-scale study (N=500+) to collect data and train an accelerometer-gyroscope recognition model with a cross-user accuracy of 95.7\% and a false-positive rate of 0.6 per hour when tested on everyday non-gesture data. Next, we design a few-shot learning framework which derives a lightweight model from our pre-trained model, enabling knowledge transfer without performance degradation. We validate our approach through a user study (N=20) examining on-device customization from 12 new gestures, resulting in an average accuracy of 55.3\%, 83.1\%, and 87.2\% on using one, three, or five shots when adding a new gesture, while maintaining the same recognition accuracy and false-positive rate from the pre-existing gesture set. We further evaluate the usability of our real-time implementation with a user experience study (N=20). Our results highlight the effectiveness, learnability, and usability of our customization framework. Our approach paves the way for a future where users are no longer bound to pre-existing gestures, freeing them to creatively introduce new gestures tailored to their preferences and abilities.},
    booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
    articleno = {496},
    numpages = {19},
    keywords = {transfer learning, few-shot learning, Gesture customization},
    location = {New Orleans, LA, USA},
    series = {CHI '22},
}


%===========================================================
% References starting by Y
%===========================================================
@inproceedings{Yang:2017,
    author    = {Yang, Lin and Ye, Long and Zhong, Wei and Zhang, Yingying and Zhang, Qin},
    booktitle = {LNCS on Transactions on Edutainment XIII - Volume 10092},
    title     = {{A Real-Time Interactive System Based on Hand Gesture Recognition in Virtual Fitting}},
    year      = {2017},
    address   = {Berlin, Heidelberg},
    pages     = {86–96},
    publisher = {Springer-Verlag},
    doi       = {10.1007/978-3-662-54395-5_8},
    isbn      = {9783662543948},
    keywords  = {Leap motion, Hand gesture interaction, Gesture recognition, Virtual reality},
    numpages  = {11},
    url       = {https://doi.org/10.1007/978-3-662-54395-5_8},
}


@article{Yang:2018,
	title = {{Sparsity aware dynamic gesture recognition using radar sensors with angular diversity}},
	volume = {12},
	issn = {1751-8792},
	doi = {10.1049/iet-rsn.2018.5206},
	abstract = {In this study, two radar sensors with angular diversity are used to recognise dynamic hand gestures by analysing the sparse micro-Doppler signatures. The radar echoes are firstly mapped into the time–frequency domain through the Gaussian-windowed Fourier dictionary at each radar sensor. Then the sparse time–frequency features are extracted via the orthogonal matching pursuit algorithm. Finally, the sparse time–frequency features at two radar sensors are fused and input into the modified-Hausdorff-distance-based nearest neighbour classifier to achieve the dynamic hand gesture recognition. The experimental results based on the measured data under three different experimental scenes demonstrate that (i) the recognition accuracy can be improved by fusing the features extracted at two radar sensors when each radar sensor works well on its own; (ii) the recognition accuracy produced by feature fusion keeps satisfactory even if one of the radar sensors has poor performance, which means that the feature fusion can improve the robustness of the recognition system; and (iii) it would be more helpful if the line-of-sights of the two radar sensors are set to be orthogonal to each other.},
	number = {10},
	journal = {IET Radar, Sonar Navigation},
	author = {Yang, Li and Li, Gang},
	year = {2018},
	keywords = {angular diversity, Doppler radar, dynamic hand gesture recognition, feature extraction, Gaussian-windowed Fourier dictionary, gesture recognition, modified-Hausdorff-distance-based nearest neighbour classifier, orthogonal matching pursuit algorithm, radar receivers, radar sensor, radar signal processing, sensor fusion, signal classification, sparse microDoppler signatures, sparse time–frequency feature extraction, sparsity aware dynamic gesture recognition, time-frequency analysis, time–frequency domain},
	pages = {1114--1120},
}


@inproceedings{Yao:2017,
    author    = {Yao, Yuan and Chiu, Po-Tsung and Fu, Wai-Tat},
    booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces Companion},
    title     = {{A Gestural Interface for Practicing Children's Spatial Skills}},
    year      = {2017},
    address   = {New York, NY, USA},
    pages     = {43–47},
    publisher = {Association for Computing Machinery},
    series    = {IUI '17 Companion},
    doi       = {10.1145/3030024.3038265},
    isbn      = {9781450348935},
    keywords  = {video game, children, 3D interaction, spatial reasoning, gestural interface},
    venue  = {Limassol, Cyprus},
    numpages  = {5},
    url       = {https://doi.org/10.1145/3030024.3038265},
}


@article{Yasen:2019,
    title = {{A systematic review on hand gesture recognition techniques, challenges and applications}},
    author = {Yasen, Mais and Jusoh, Shaidah},
    year = 2019,
    month = sep,
    keywords = {Hand gesture, Hand gesture applications, Hand gesture recognition challenges, Recognition techniques, Recognition},
    volume = 5,
    pages = {e218},
    journal = {PeerJ Computer Science},
    issn = {2376-5992},
    url = {https://doi.org/10.7717/peerj-cs.218},
    doi = {10.7717/peerj-cs.218},
}


@inproceedings{Yeo:2016,
    author = {Yeo, Hui-Shyong and Flamich, Gergely and Schrempf, Patrick and Harris-Birtill, David and Quigley, Aaron},
    title = {{RadarCat: Radar Categorization for Input \& Interaction}},
    year = {2016},
    isbn = {9781450341899},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2984511.2984515},
    doi = {10.1145/2984511.2984515},
    abstract = {In RadarCat we present a small, versatile radar-based system for material and object classification which enables new forms of everyday proximate interaction with digital devices. We demonstrate that we can train and classify different types of materials and objects which we can then recognize in real time. Based on established research designs, we report on the results of three studies, first with 26 materials (including complex composite objects), next with 16 transparent materials (with different thickness and varying dyes) and finally 10 body parts from 6 participants. Both leave one-out and 10-fold cross-validation demonstrate that our approach of classification of radar signals using random forest classifier is robust and accurate. We further demonstrate four working examples including a physical object dictionary, painting and photo editing application, body shortcuts and automatic refill based on RadarCat. We conclude with a discussion of our results, limitations and outline future directions.},
    booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
    pages = {833–841},
    numpages = {9},
    keywords = {material classification, radar, object recognition, ubiquitous computing, context-aware interaction, machine learning},
    venue = {Tokyo, Japan},
    series = {UIST '16},
}


@article{Yeo:2017,
    author = {Yeo, Hui-Shyong and Quigley, Aaron},
    title = {{Radar Sensing in Human-Computer Interaction}},
    year = {2017},
    issue_date = {January + February 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {25},
    number = {1},
    issn = {1072-5520},
    url = {https://doi.org/10.1145/3159651},
    doi = {10.1145/3159651},
    abstract = {Envisioning, designing, and implementing the user interface require a comprehensive understanding of interaction technologies. In this forum we scout trends and discuss new technologies with the potential to influence interaction design. ---Albrecht Schmidt, Editor},
    journal = {Interactions},
    month = dec,
    pages = {70–73},
    numpages = {4},
}


@article{Yeo:2019,
    author = {Yeo, Hui-Shyong and Minami, Ryosuke and Rodriguez, Kirill and Shaker, George and Quigley, Aaron},
    title = {{Exploring Tangible Interactions with Radar Sensing}},
    year = {2018},
    issue_date = {December 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {2},
    number = {4},
    url = {https://doi.org/10.1145/3287078},
    doi = {10.1145/3287078},
    abstract = {Research has explored miniature radar as a promising sensing technique for the recognition of gestures, objects, users' presence and activity. However, within Human-Computer Interaction (HCI), its use remains underexplored, in particular in Tangible User Interface (TUI). In this paper, we explore two research questions with radar as a platform for sensing tangible interaction with the counting, ordering, identification of objects and tracking the orientation, movement and distance of these objects. We detail the design space and practical use-cases for such interaction which allows us to identify a series of design patterns, beyond static interaction, which are continuous and dynamic. With a focus on planar objects, we report on a series of studies which demonstrate the suitability of this approach. This exploration is grounded in both a characterization of the radar sensing and our rigorous experiments which show that such sensing is accurate with minimal training. With these techniques, we envision both realistic and future applications and scenarios. The motivation for what we refer to as Solinteraction, is to demonstrate the potential for radar-based interaction with objects in HCI and TUI.},
    journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
    month = dec,
    articleno = {200},
    numpages = {25},
    keywords = {Soli, Radar Sensing, Context-Aware Interaction, Machine Learning, Ubiquitous Computing, Token+Constraint, Tangible Interaction, Tangible User Interface},
}


@inproceedings{Yoo:2015,
    author = {Yoo, Soojeong and Parker, Callum and Kay, Judy and Tomitsch, Martin},
    title = {{To Dwell or Not to Dwell: An Evaluation of Mid-Air Gestures for Large Information Displays}},
    year = {2015},
    isbn = {9781450336734},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2838739.2838819},
    doi = {10.1145/2838739.2838819},
    abstract = {This paper investigates user preferences for mid-air gestures to interact with large public information displays. We designed and implemented a public display application that allows people to navigate between Twitter feeds and to find details about particular tweets. The application supports selection and navigation through (1) point-and-dwell and (2) push and grab-and-pull. A within-subject evaluation with 10 participants found that although point-and-dwell was perceived to be more accurate, push was preferred for selecting items and grab-and-pull was preferred for navigation. Based on our findings we derive recommendations for designing gesture-based information displays.},
    booktitle = {Proceedings of the Annual Meeting of the Australian Special Interest Group for Computer Human Interaction},
    pages = {187–191},
    numpages = {5},
    keywords = {push, point-and-dwell, natural user interfaces, mid-air gestures, grab-and-pull, Public displays},
    venue = {Parkville, VIC, Australia},
    series = {OzCHI '15},
}


@inproceedings{Yu:2018,
    author    = {Yu, Ge and Liang, Ji and Guo, Lili},
    booktitle = {Proceedings of the 2018 International Conference on Artificial Intelligence and Virtual Reality},
    title     = {{Multi-Modal Interaction for Space Telescience of Fluid Experiments}},
    year      = {2018},
    address   = {New York, NY, USA},
    pages     = {35–41},
    publisher = {Association for Computing Machinery},
    series    = {AIVR 2018},
    doi       = {10.1145/3293663.3293672},
    isbn      = {9781450366410},
    keywords  = {Single-Channel Speech Separation, Space telescience, Gesture recognition, Space fluid experiment},
    venue  = {Nagoya, Japan},
    numpages  = {7},
    url       = {https://doi.org/10.1145/3293663.3293672},
}


@inproceedings{Yu:2020b,
	title = {{mmWave} {Radar}-based {Hand} {Gesture} {Recognition} using {Range}-{Angle} {Image}},
	doi = {10.1109/VTC2020-Spring48590.2020.9128573},
	abstract = {The radar sensing on fine human-motion/hand-gesture provides further human-computer interaction (HCI) experience. Most of the studies about gesture recognition with mmWave frequency modulated continuous wave (FMCW) radar adopts the range and the velocity estimated from the raw data, such as the time-frequency spectrogram, micro-doppler spectrogram, or range-Doppler image (RDI). Besides, the angle estimated using multiple receive antennas also contains rich information of gesture, especially in the discrimination among the horizontal movement of the gesture. Thus, we propose to use the range-angle image (RAI) as the input and train a model consisting of the convolutional neural network and long short term memory that is capable of recognizing hand gestures. We validate the proposed scheme based on the collection of hand gestures by several subjects in different classrooms using 77 - 81GHz mmWave radar of Texas Instrument. Based on the configuration of one transmit antenna and four receive antennas, we show that the hand gesture recognition using RAI outperforms that using RDI. Also, we adopt the fusion strategy to consider both RDI and RAI to further improve accuracy.},
	booktitle = {2020 {IEEE} 91st {Vehicular} {Technology} {Conference} ({VTC2020}-{Spring})},
	author={Yu, Jih-Tsun and Yen, Li and Tseng, Po-Hsuan},
	month = may,
	year = {2020},
	note = {ISSN: 2577-2465},
	keywords = {antenna arrays, Chirp, continuous wave radar, convolutional neural nets, CW radar, Doppler radar, FM radar, FMCW Radar, frequency 77.0 GHz to 81.0 GHz, gesture recognition, Gesture recognition, Hand-Gesture Recognition, Human-Computer Interaction, human-computer interaction experience, image fusion, image motion analysis, Machine Learning, microdoppler spectrogram, millimetre wave radar, mmWave frequency, mmWave radar, mmWave radar-based hand gesture recognition, multiple receive antennas, Radar antennas, radar imaging, Radar imaging, radar sensing, range-angle image, range-Doppler image, RDI, receiving antennas, Receiving antennas, recurrent neural nets, Time-frequency analysis, time-frequency spectrogram, transmitting antennas},
	pages = {1--5},
}


@article{Yu:2020a,
	title = {A {Frame} {Detection} {Method} for {Real}-{Time} {Hand} {Gesture} {Recognition} {Systems} {Using} {CW}-{Radar}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/8/2321},
	doi = {10.3390/s20082321},
	abstract = {In this paper, a method to detect frames was described that can be used as hand gesture data when configuring a real-time hand gesture recognition system using continuous wave (CW) radar. Detecting valid frames raises accuracy which recognizes gestures. Therefore, it is essential to detect valid frames in the real-time hand gesture recognition system using CW radar. The conventional research on hand gesture recognition systems has not been conducted on detecting valid frames. We took the R-wave on electrocardiogram (ECG) detection as the conventional method. The detection probability of the conventional method was 85.04\%. It has a low accuracy to use the hand gesture recognition system. The proposal consists of 2-stages to improve accuracy. We measured the performance of the detection method of hand gestures provided by the detection probability and the recognition probability. By comparing the performance of each detection method, we proposed an optimal detection method. The proposal detects valid frames with an accuracy of 96.88\%, 11.84\% higher than the accuracy of the conventional method. Also, the recognition probability of the proposal method was 94.21\%, which was 3.71\% lower than the ideal method.},
	number = {8},
	journal = {Sensors},
	author = {Yu, Myoungseok and Kim, Narae and Jung, Yunho and Lee, Seongjoo},
	year = {2020},
    pages = {1--17},
}


%===========================================================
% References starting by Z
%===========================================================
@article{Zeng:2018,
    author     = {Zeng, Wei and Wang, Cong and Wang, Qinghui},
    journal    = {Multimedia Tools Appl.},
    title      = {{Hand Gesture Recognition Using Leap Motion via Deterministic Learning}},
    year       = {2018},
    issn       = {1380-7501},
    month      = nov,
    number     = {21},
    pages      = {28185–28206},
    volume     = {77},
    address    = {USA},
    issue_date = {November 2018},
    keywords   = {Deterministic learning, Hand gesture recognition, Leap Motion, Hand motion dynamics, RBF neural networks},
    numpages   = {22},
    publisher  = {Kluwer Academic Publishers},
}


@article{Zeng:2020a,
	title = {Arm {Motion} {Classification} {Using} {Time}-{Series} {Analysis} of the {Spectrogram} {Frequency} {Envelopes}},
	volume = {12},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/3/454},
	doi = {10.3390/rs12030454},
	abstract = {Hand and arm gesture recognition using radio frequency (RF) sensing modality proves valuable in man-machine interfaces and smart environments. In this paper, we use the time-series analysis method to accurately measure the similarity of the micro-Doppler (MD) signatures between the training and test data, thus providing improved gesture classification. We characterize the MD signatures by the maximum instantaneous Doppler frequencies depicted in the spectrograms. In particular, we apply two machine learning (ML) techniques, namely, the dynamic time warping (DTW) method and the long short-term memory (LSTM) network. Both methods take into account the values as well as the temporal evolution and characteristics of the time-series data. It is shown that the DTW method achieves high gesture classification rates and is robust to time misalignment.},
	number = {3},
	journal = {Remote Sensing},
	author = {Zeng, Zhengxin and Amin, Moeness G. and Shan, Tao},
	year = {2020},
    pages = {1--20},
}


@article{Zeng:2020b,
	title = {Automatic {Arm} {Motion} {Recognition} {Based} on {Radar} {Micro}-{Doppler} {Signature} {Envelopes}},
	volume = {20},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2020.3004581},
	abstract = {In considering human-machine interface (HMI) for smart environment, a simple but effective method is proposed for automatic arm motion recognition with a Doppler radar sensor. Arms, in lieu of hands, have stronger radar cross-section and can be recognized from relatively longer distances. An energy-based thresholding algorithm is applied to the spectrograms to extract the micro-Doppler (MD) signature envelopes. The positive and negative frequency envelopes are concatenated to form a feature vector. The nearest neighbor (NN) classifier with Manhattan distance (L1) is then used to recognize the arm motions. It is shown that this simple method yields classification accuracy above 97 percent for six classes of arm motions. Despite its simplicity, the proposed method is superior to those of handcrafted feature-based classifications and low-dimension representation techniques based on principal component analysis (PCA), and is comparable to convolutional neural network (CNN).},
	number = {22},
	journal = {IEEE Sensors Journal},
	author={Zeng, Zhengxin and Amin, Moeness G. and Shan, Tao},
	month = nov,
	year = {2020},
	keywords = {Arm motion recognition, arm motions, automatic arm motion recognition, Doppler effect, Doppler radar, Doppler radar sensor, energy-based thresholding algorithm, feature extraction, Feature extraction, handcrafted feature-based classifications, human-machine interface, image classification, micro-Doppler signature, negative frequency envelopes, neural nets, positive frequency envelopes, principal component analysis, radar cross-sections, radar microDoppler signature envelopes, relatively longer distances, Sensors, simple but effective method, simple method yields, Spectrogram, spectrograms, stronger radar cross-section, Time-frequency analysis},
	pages = {13523--13532},
}


@article{Zhai:2012,
    author    = {Zhai, Shumin and Kristensson, Per Ola and Appert, Caroline and Anderson, Tue Haste and Cao, Xiang},
    title     = {{Foundational Issues in Touch-Surface Stroke Gesture Design - An Integrative  Review}},
    journal   = {Found. Trends Hum. Comput. Interact.},
    volume    = {5},
    number    = {2},
    pages     = {97--205},
    year      = {2012},
    url       = {https://doi.org/10.1561/1100000012},
    doi       = {10.1561/1100000012},
    timestamp = {Fri, 13 Mar 2020 14:40:07 +0100},
    biburl    = {https://dblp.org/rec/journals/fthci/ZhaiKAAC12.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org},
}


@inproceedings{Zhang:2016,
	title = {{Dynamic hand gesture classification based on radar micro-{Doppler} signatures}},
	doi = {10.1109/RADAR.2016.8059518},
	abstract = {Dynamic hand gesture recognition is of great importance for human-computer interaction. In this paper, we present a method to discriminate the four kinds of dynamic hand gestures, snapping fingers, flipping fingers, hand rotation and calling, using a radar micro-Doppler sensor. Two micro-Doppler features are extracted from the time-frequency spectrum and the support vector machine is used to classify these four kinds of gestures. The experimental results on measured data demonstrate that the proposed method can produce a classification accuracy higher than 88.56\%.},
	booktitle = {2016 {CIE} {International} {Conference} on {Radar} ({RADAR})},
	author={Zhang, Shimeng and Li, Gang and Ritchie, Matthew and Fioranelli, Francesco and Griffiths, Hugh},
	month = oct,
	year = {2016},
	keywords = {classification accuracy, Doppler radar, dynamic hand gesture classification, dynamic hand gesture recognition, feature extraction, Feature extraction, flipping fingers, gesture recognition, Gesture recognition, hand gesture classification, hand rotation, human computer interaction, human-computer interaction, image classification, micro-Doppler signatures, microDoppler feature extraction, Radar, radar computing, radar imaging, radar microDoppler sensor, radar microDoppler signatures, snapping fingers, support vector machine, support vector machines, Support vector machines, Thumb, time-frequency analysis, Time-frequency analysis, time-frequency spectrum},
	pages = {1--4},
}


@inproceedings{Zhang:2017,
	title = {{Deformable deep convolutional generative adversarial network in microwave based hand gesture recognition system}},
	doi = {10.1109/WCSP.2017.8170976},
	abstract = {Traditional vision-based hand gesture recognition systems is limited under dark circumstances. In this paper, we build a hand gesture recognition system based on microwave transceiver and deep learning algorithm. A Doppler radar sensor with dual receiving channels at 5.8GHz is used to acquire a big database of hand gestures signals. The received hand gesture signals are then processed with time-frequency analysis. Based on these big database of hand gesture, we propose a new classification architecture called deformable deep convolutional generative adversarial network. Experimental results shows the new architecture can upgrade the recognition rate by 10\% and the deformable kernel can reduce the testing time cost by 30\%.},
	booktitle = {2017 9th {International} {Conference} on {Wireless} {Communications} and {Signal} {Processing} ({WCSP})},
	author={Zhang, Jiajun and Shi, Zhiguo},
	month = oct,
	year = {2017},
	note = {ISSN: 2472-7628},
	keywords = {big database, classification architecture, Convolution, Databases, deep learning algorithm, deformable deep convolutional generative adversarial network, Doppler radar, Doppler radar sensor, dual receiving channels, feedforward neural nets, frequency 5.8 GHz, gesture recognition, Gesture recognition, hand gesture signals, image classification, learning (artificial intelligence), microwave based hand gesture recognition system, Microwave imaging, Microwave theory and techniques, microwave transceiver, radio transceivers, recognition rate, Testing, time-frequency analysis, Time-frequency analysis},
	pages = {1--6},
}


@inproceedings{Zhang:2018d,
	address = {Brussels, BEL},
	series = {{MOBIMEDIA}'18},
	title = {Application of {FMCW} {Radar} for {Dynamic} {Continuous} {Hand} {Gesture} {Recognition}},
	isbn = {978-1-63190-164-5},
	abstract = {Recently, hand gesture recognition systems have become increasingly interesting to researchers in the field of human-computer interfaces. Real-world systems for human dynamic hand gesture recognition is challenging as: 1) the system must be robustness to various conditions; 2) there is a rich diversity in how people perform hand gestures, making hand gesture recognition difficult; 3) the system must detect and recognize hand gestures continuously using unsegmented input streams in order to avoid noticeable lag between performing a gesture and its classification. In this paper, to address these challenges, we present a novel system for dynamic continuous hand gesture recognition based on Frequency Modulated Continuous Wave (FMCW) radar sensor. The radar system does not depend on lighting, noise or atmospheric conditions. We employ a recurrent three-dimensional convolutional neural network to perform classification of dynamic hand gestures. To enhance the processing performance, Connectionist Temporal Classification (CTC) algorithm is used to train the network to predict class labels from inprogress gestures in unsegmented input streams. The experimental results show that this system is able to achieve high recognition rates of 96\%, which is higher than state-of-the-art hand gesture recognition systems. In addition, the conclusion in this work can be used for real-time hand gesture recognition system design.},
	urldate = {2020-12-21},
	booktitle = {Proceedings of the 11th {EAI} {International} {Conference} on {Mobile} {Multimedia} {Communications}},
	publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
	author = {Zhang, Zhenyuan and Tian, Zengshan and Mu, Zhou and Liu, Yi},
	month = sep,
	year = {2018},
	keywords = {connectionist temporal classification, fmcw radar, hand gesture recognition, recurrent three-dimensional convolutional neural network},
	pages = {298--303},
}


@article{Zhang:2018a,
	title = {Latern: {Dynamic} {Continuous} {Hand} {Gesture} {Recognition} {Using} {FMCW} {Radar} {Sensor}},
	volume = {18},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2018.2808688},
	abstract = {Recently, hand gesture recognition systems have become increasingly interesting to researchers in the field of human-computer interfaces. Real-world systems for human dynamic hand gesture recognition is challenging as: 1) the system must be robust to various conditions; 2) there is a rich diversity in how people perform hand gestures, making hand gesture recognition difficult; and 3) the system must detect and recognize hand gestures continuously using unsegmented input streams in order to avoid noticeable lag between performing a gesture and its classification. In this paper, to address these challenges, we present Latern, a novel system for dynamic continuous hand gesture recognition based on a frequency-modulated continuous wave radar sensor. The radar system does not depend on lighting, noise, or atmospheric conditions. We employ a recurrent 3-D convolutional neural network to perform the classification of dynamic hand gestures. To enhance the processing performance, a connectionist temporal classification algorithm is used to train the network to predict class labels from inprogress gestures in unsegmented input streams. The experimental results show that Latern is able to achieve high recognition rates of 96\%, which is higher than state-of-the-art hand gesture recognition systems. In addition, the conclusion in this paper can be used for a real-time hand gesture recognition system design.},
	number = {8},
	journal = {IEEE Sensors Journal},
	author={Zhang, Zhenyuan and Tian, Zengshan and Zhou, Mu},
	month = apr,
	year = {2018},
	keywords = {connectionist temporal classification, CW radar, Doppler radar, dynamic continuous hand gesture recognition, Feature extraction, FM radar, FMCW radar, FMCW radar sensor, frequency-modulated continuous wave radar sensor, gesture recognition, Gesture recognition, Hand gesture recognition, Heuristic algorithms, Hidden Markov models, human dynamic hand gesture recognition, human-computer interfaces, inprogress gestures, Latern, real-time hand gesture recognition system design, recurrent 3D convolutional neural network, recurrent neural nets, recurrent three-dimensional convolutional neural network, Sensors, unsegmented input streams},
	pages = {3278--3289},
}


@inproceedings{Zhang:2018b,
	title = {Riddle: {Real}-{Time} {Interacting} with {Hand} {Description} via {Millimeter}-{Wave} {Sensor}},
	doi = {10.1109/ICC.2018.8422765},
	abstract = {In this paper, we present a Real-time Interacting with Hand Description system via Millimeter-wave Sensor (Riddle) for human-computer interaction. Firstly, we describe a new approach to developing a radar-based system. When hand motions are captured by millimeter- wave radar sensor, the unique range information can be observed in the spectrogram. Compared to traditional hand gesture recognition systems based on optical sensors, the radar-based system avoids the influence of ambient light conditions. Secondly, we employ deep neural networks combined with connectionist temporal classification algorithm to recognize diverse hand gestures in real-time. Besides, we visualize the feature maps extracted from different layers to understand the deep neural networks. The deep neural networks are powerful to extract hand gesture features as well as class boundaries through a training process. Finally, we demonstrate that Riddle is capable of detecting six hand gestures and achieving high recognition accuracy of 96\%.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Communications} ({ICC})},
	author={Zhang, Zhenyuan and Tian, Zengshan and Zhou, Mu and Nie, Wei and Li, Ze},
	month = may,
	year = {2018},
	note = {ISSN: 1938-1883},
	keywords = {connectionist temporal classification algorithm, deep neural networks, feature extraction, Feature extraction, Frequency modulation, gesture recognition, Gesture recognition, hand gesture features, hand gesture recognition systems, hand motions, human computer interaction, human-computer interaction, image classification, image motion analysis, Millimeter wave radar, millimeter-wave radar sensor, millimetre wave radar, neural nets, Neural networks, real-time interacting with hand description system, Real-time systems, Riddle},
	pages = {1--6},
}


@inproceedings{Zhang:2018c,
	title = {Dynamic {Hand} {Gesture} {Recognition} {Using} {FMCW} {Radar} {Sensor} for {Driving} {Assistance}},
	doi = {10.1109/WCSP.2018.8555642},
	abstract = {Dynamic hand gesture recognition is very important for human-computer interaction. In vehicles, hand gesture recognition can be used as the driver's auxiliary system to achieve remote control of the instrument. To a certain extent, this system can avoid physical buttons and touch screens causing interference to the driver. In this paper, we describe a driver assisted dynamic gesture recognition system to classify nine hand gestures based on micro-Doppler signatures obtained by 77GHz FMCW radar using a convolutional neural network (CNN). We further explore the changes in the accuracy of same gestures in a variety of experimental scenarios to help optimize the robustness of the system.},
	booktitle = {2018 10th {International} {Conference} on {Wireless} {Communications} and {Signal} {Processing} ({WCSP})},
	author={zhang, Xuhao and Wu, Qisong and Zhao, Dixian},
	month = oct,
	year = {2018},
	note = {ISSN: 2472-7628},
	keywords = {Chirp, CNN, convolution, convolutional neural network, CW radar, Doppler radar, driver assistance system, driver assisted dynamic hand gesture recognition system, driver information systems, drivers auxiliary system, feedforward neural nets, FM radar, FMCW radar sensor, gesture recognition, Gesture recognition, hand gesture recognition, human computer interaction, human-computer interaction, micro-Doppler signatures, physical buttons, Radar, Radar antennas, remote control, Spectrogram, telecontrol, Thumb, touch screens, Training},
	pages = {1--6},
}


@inproceedings{Zhang:2019a,
	title = {Implementation of {C4}.5 decision tree in {Human} {Gesture} {Recognition} based on {Doppler} radars},
	abstract = {In order to improve the accuracy of gesture recognition, we investigate the feasibility of using a three-dimensional Doppler-radar array at 24GHz to recognize human gestures with a model consisted of ten classical gestures. On the basis of C4.5 algorithm, phase difference and spectral energy, extracted by correlation processing and power integral, are used as the features to construct decision tree and separate ten gestures. The experiment result shows that this system could achieve a high accuracy of classification reached 99.25\%.},
	booktitle = {2019 {International} {Symposium} on {Antennas} and {Propagation} ({ISAP})},
	author={Zhang, Guiyuan and Zhang, Kang and Yun, Yihan and Lu, Gang and Lan, Shengchang},
	month = oct,
	year = {2019},
	keywords = {24GHz radar, C4.5 algorithm, C4.5 decision tree algorithm, correlation processing, decision tree, decision trees, Doppler radar, doppler radar array, frequency 24.0 GHz, gesture recognition, human gesture recognition, phase difference, power integral, spectral energy, three-dimensional Doppler-radar array},
	pages = {1--3},
}


@article{Zhang:2019b,
	title = {u-{DeepHand}: {FMCW} {Radar}-{Based} {Unsupervised} {Hand} {Gesture} {Feature} {Learning} {Using} {Deep} {Convolutional} {Auto}-{Encoder} {Network}},
	volume = {19},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2019.2910810},
	abstract = {Recently, although radar sensors have been widely applied for hand gesture recognition (HGR) tasks, conventional radar-based HGR systems still have two major challenges. First, these systems rely on supervised learning approaches to learn gesture features, which normally require a large-scale labeled dataset to address the overfitting problem. However, the acquisition of such dataset is time-consuming. Second, the radar signature of hand movement is often influenced by micromotion caused by other body parts, which leads to distorted motion features, resulting in poor identification accuracy. To overcome these problems, we propose an unsupervised hand gesture feature learning method using the deep convolutional auto-encoder network to analyze hand gesture signal collected by a frequency modulated continuous wave (FMCW) radar sensor. First, via a convolutional encoder sub-network, input radar range profiles are transformed into lower dimensional representations. Then, the representations are expanded to reconstruct the corresponding input profiles by a deconvolutional decoder sub-network. In addition, to investigate the mechanisms of the proposed network and evaluate its performance, we conduct an in-depth study of the feature maps learned from various hand gesture experimental data and evaluate the corresponding classification performance. The results demonstrate that the proposed convolutional auto-encoder network is able to achieve high recognition accuracy with low training sample cost, which outperforms the state-of-the-art hand gesture recognition systems based on transfer learning VGGNet and fully connected-based auto-encoder network.},
	number = {16},
	journal = {IEEE Sensors Journal},
	author={Zhang, Zhenyuan and Tian, Zengshan and Zhang, Ying and Zhou, Mu and Wang, Bang},
	month = aug,
	year = {2019},
	keywords = {classification performance, Convolution, convolutional auto-encoder network, convolutional encoder sub-network, convolutional neural nets, CW radar, deconvolutional decoder sub-network, deep convolutional auto-encoder network, distorted motion features, Doppler radar, feature extraction, Feature extraction, feature maps, FM radar, FMCW radar, FMCW radar-based unsupervised hand gesture feature learning, frequency modulated continuous wave radar sensor, gesture recognition, hand gesture experimental data, Hand gesture recognition, hand gesture recognition tasks, hand gesture signal, hand movement, image classification, input radar range profiles, performance evaluation, radar imaging, radar sensors, radar signature, radar-based HGR systems, Sensors, Task analysis, Training, transfer learning VGGNet, unsupervised learning},
	pages = {6811--6821},
}


@incollection{Zhang:2019c,
	address = {Singapore},
	title = {Doppler-{Radar} {Based} {Hand} {Gesture} {Recognition} {System} {Using} {Convolutional} {Neural} {Networks}},
	volume = {463},
	isbn = {978-981-10-6570-5 978-981-10-6571-2},
	url = {http://link.springer.com/10.1007/978-981-10-6571-2_132},
	language = {en},
	urldate = {2020-12-21},
	booktitle = {Communications, {Signal} {Processing}, and {Systems}},
	publisher = {Springer Singapore},
	author = {Zhang, Jiajun and Tao, Jinkun and Shi, Zhiguo},
	editor = {Liang, Qilian and Mu, Jiasong and Jia, Min and Wang, Wei and Feng, Xuhong and Zhang, Baoju},
	year = {2019},
	doi = {10.1007/978-981-10-6571-2_132},
	note = {Series Title: Lecture Notes in Electrical Engineering},
	pages = {1096--1113},
}


@inproceedings{Zhang:2019d,
	title = {{SmartFinger}: {A} {Finger}-{Sensing} {System} for {Mobile} {Interaction} via {MIMO} {FMCW} {Radar}},
	doi = {10.1109/GCWkshps45667.2019.9024578},
	abstract = {In this paper, we present a finger-grained gesture recognition system that can be deployed on commodity Multiple Input Multiple Output Frequency Modulated Continuous Wave (MIMO-FMCW) radar platform as software, without any hardware modification. Firstly, we utilize the two-dimension fast Fourier transform algorithm (2D-FFT) to jointly estimate range-Doppler information. Secondly, by combining with binary phase modulation MIMO (BPM-MIMO) technique, a discrete Fourier transformation (DFT) based Multiple Signal Classification (MUSIC) algorithm is proposed to jointly measure range and angle of arrival (AOA) information without prior knowledge about the number of targets. Thirdly, a recurrent 3D convolutional neural network (R3DCNN) is employed to extract spatial-temporal fusion- features existing in range-Doppler and range-AOA map sequences. Next, we implement and evaluate this system utilizing commercial-off-the-shelf FMCW radar platform. The experimental results show that this system is able to achieve a high recognition rate of 93\%.},
	booktitle = {2019 {IEEE} {Globecom} {Workshops} ({GC} {Wkshps})},
	author={Zhang, Zhenyuan and Tian, Zengshan and Zhou, Mu},
	month = dec,
	year = {2019},
	keywords = {angle of arrival information, Antenna arrays, binary phase modulation MIMO technique, BPM-MIMO, Chirp, commercial-off-the-shelf FMCW radar platform, commodity Multiple Input Multiple Output Frequency Modulated Continuous Wave radar platform, convolutional neural nets, direction-of-arrival estimation, discrete Fourier transformation, Doppler radar, fast Fourier transforms, feature extraction, Feature extraction, finger-grained gesture recognition system, finger-sensing system, gesture recognition, Gesture recognition, MIMO communication, MIMO FMCW radar, MIMO radar, mobile computing, mobile interaction, Multiple Signal Classification algorithm, Radar, Radar antennas, range-AOA map sequences, range-Doppler information, recurrent 3D convolutional neural network, recurrent neural nets, signal classification, spatial-temporal fusion-feature extraction, two-dimension fast Fourier transform algorithm},
	pages = {1--5},
}


@inproceedings{Zhang:2020a,
	title = {{RaCon}: {A} gesture recognition approach via {Doppler} radar for intelligent human-robot interaction},
	doi = {10.1109/PerComWorkshops48775.2020.9156109},
	abstract = {As an important entrance for human-robot interaction, the hand gesture recognition based on wireless sensor has received great attention in recent years. By recognizing fine-grained arm movements, remotely deployed collaborative robot could work more accurately to satisfy human demands. Existing approaches mostly use wearable sensors or wireless devices to recognize human movement, which is with strict position requirements. In this paper, we propose a robust gesture recognition method based on double Doppler radars. Specifically, we use two Doppler radars to collect two sources of Doppler signal of a gesture. Then 6 types of gestures with different angles between people and the radar were classified by employing an improved dynamic time warping (DTW) algorithm. Furthermore, we demonstrate the practicability of the proposed method by developing a cooperative robot control system and the average recognition accuracy is 96\%.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} ({PerCom} {Workshops})},
	author={Zhang, Kaijie and Yu, Zhiwen and Zhang, Dong and Wang, Zhu and Guo, Bin},
	month = mar,
	year = {2020},
	keywords = {collaborative robot, Doppler radar, Doppler signal, double Doppler radars, Feature extraction, fine-grained arm movements, gesture recognition, Gesture recognition, gesture recognition approach, hand gesture recognition, human demands, human movement, human-robot interaction, Human-robot interaction, important entrance, robot control system, robust gesture recognition method, Signal synthesis analysis, strict position requirements, Time-frequency analysis, wearable sensors, Wireless communication, wireless devices, Wireless sensing, wireless sensor, Wireless sensor networks},
	pages = {1--6},
}


@inproceedings{Zhang:2020b,
	title = {Temporal-{Range}-{Doppler} {Features} {Interpretation} and {Recognition} of {Hand} {Gestures} {Using} {mmW} {FMCW} {Radar} {Sensors}},
	doi = {10.23919/EuCAP48036.2020.9135694},
	abstract = {This paper introduced a comparative study of using deep neural networks in non-contact hand gesture recognition based on millimeter wave FMCW radar. Range-doppler maps are processed with a zero-filling strategy to boost the range and velocity information of gesture motions. Two optimal types of deep neural networks, 3D-CNN and CNN-LSTM are respectively constructed to reveal the temporal gesture motion signatures encoded in multiple adjacent radar chirps. With the proposed networks, the recognition accuracy of six popular hand gestures reach to 95\%. Meanwhile, this letter further explores the performance of the proposed networks in the impacts of training data size on the recognition accuracy. The proposed methods can be applied in the recognition of minor finger motions, providing some preliminary experimental results compared with other baseline methods.},
	booktitle = {2020 14th {European} {Conference} on {Antennas} and {Propagation} ({EuCAP})},
	author={Zhang, Guiyuan and Lan, Shengchang and Zhang, Kang and Ye, Linting},
	month = mar,
	year = {2020},
	keywords = {3D-CNN, CNN-LSTM, convolutional neural nets, CW radar, deep neural networks, Doppler radar, finger motions, FM radar, FMCW radar, gesture motions, gesture recognition, hand gesture recognition, hand gestures, millimeter wave, millimeter wave FMCW radar, mmW FMCW radar sensors, multiple adjacent radar chirps, noncontact hand gesture recognition, radar detection, range-doppler maps, recognition accuracy, recurrent neural nets, temporal gesture motion signatures, temporal-range-doppler features interpretation, velocity information, zero-filling strategy},
	pages = {1--4},
}


@inproceedings{Zhang:2021,
    author={Zhang, Bo and Zhang, Lei and Wu, Mojun and Wang, Yan},
    booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
    title={{Dynamic Gesture Recognition Based on RF Sensor and AE-LSTM Neural Network}}, 
    year={2021},
    volume={},
    number={},
    pages={1-5},
    doi={10.1109/ISCAS51556.2021.9401065}
}


@inproceedings{Zhao:2019,
	title = {Interference {Suppression} {Based} {Gesture} {Recognition} {Method} with {FMCW} {Radar}},
	doi = {10.1109/WCSP.2019.8928108},
	abstract = {In this paper, we propose a gesture recognition method based on interference suppression with Frequency Modulation Continuous Wave (FMCW)radar. We analyze the sampling point and the time needed for snapshot of radar signal, and estimate the range and doppler parameter of gesture. Afterward, we design a complete background and target interference suppression method for the characteristics of radar signal interference, then use Time Sequential Inflated 3 Dimensions(TS- I3D)network for gesture feature extraction and classification. The experimental results show that the suppression method has effectively solved the problems caused by non-gesture interference, and the average accuracy rate of gesture recognition is 96.67 \%.},
	booktitle = {2019 11th {International} {Conference} on {Wireless} {Communications} and {Signal} {Processing} ({WCSP})},
	author={Zhao, Zedong and Wang, Yong and Zhou, Mu and Yang, Xiaolong and Xie, Liangbo},
	month = oct,
	year = {2019},
	note = {ISSN: 2472-7628},
	keywords = {CW radar, Delays, Doppler parameter estimation, Doppler radar, feature extraction, Feature extraction, FM radar, FMCW radar, frequency modulation continuous wave radar, gesture feature classification, gesture feature extraction, gesture recognition, Gesture recognition, image classification, image denoising, image sampling, Interference suppression, interference suppression based gesture recognition method, nongesture interference, parameter estimation, radar imaging, radar interference, radar signal interference, range parameter estimation, sampling point, target interference suppression method, time sequential inflated 3 dimension network, TS- I3D network, TS-I3D network},
	pages = {1--6},
}


@article{Zhou:2018a,
	title = {Dynamic {Gesture} {Recognition} with a {Terahertz} {Radar} {Based} on {Range} {Profile} {Sequences} and {Doppler} {Signatures}},
	volume = {18},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/18/1/10},
	doi = {10.3390/s18010010},
	abstract = {The frequency of terahertz radar ranges from 0.1 THz to 10 THz, which is higher than that of microwaves. Multi-modal signals, including high-resolution range profile (HRRP) and Doppler signatures, can be acquired by the terahertz radar system. These two kinds of information are commonly used in automatic target recognition; however, dynamic gesture recognition is rarely discussed in the terahertz regime. In this paper, a dynamic gesture recognition system using a terahertz radar is proposed, based on multi-modal signals. The HRRP sequences and Doppler signatures were first achieved from the radar echoes. Considering the electromagnetic scattering characteristics, a feature extraction model is designed using location parameter estimation of scattering centers. Dynamic Time Warping (DTW) extended to multi-modal signals is used to accomplish the classifications. Ten types of gesture signals, collected from a terahertz radar, are applied to validate the analysis and the recognition system. The results of the experiment indicate that the recognition rate reaches more than 91\%. This research verifies the potential applications of dynamic gesture recognition using a terahertz radar.},
	number = {1},
	journal = {Sensors},
	author = {Zhou, Zhi and Cao, Zongjie and Pi, Yiming},
	year = {2018},
    pages = {1--15},
}


@inproceedings{Zhou:2018b,
    author    = {Zhou, Andrew Jie and Yang, Grace Hui},
    booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
    title     = {{Minority Report by Lemur: Supporting Search Engine with Virtual Reality}},
    year      = {2018},
    address   = {New York, NY, USA},
    pages     = {1329–1332},
    publisher = {Association for Computing Machinery},
    series    = {SIGIR '18},
    doi       = {10.1145/3209978.3210179},
    isbn      = {9781450356572},
    keywords  = {virtual reality, search engine, user interface},
    venue  = {Ann Arbor, MI, USA},
    numpages  = {4},
    url       = {https://doi.org/10.1145/3209978.3210179},
}


@inproceedings{Zhou:2019,  
    author={Zhou, Feifei and Li, Xiangyu and Wang, Zhihua},  
    booktitle={2019 IEEE SENSORS},   
    title={{Efficiently User-Independent Ultrasonic-Based Gesture Recognition Algorithm}},   
    year={2019},  
    volume={},  
    number={},  
    pages={1-4},  
    doi={10.1109/SENSORS43011.2019.8956774},
}


@inproceedings{Zhu:2018,
    author={Zhu, Shangyue and Xu, Junhong and Guo, Hanqing and Liu, Qiwei and Wu, Shaoen and Wang, Honggang},
    booktitle={2018 IEEE International Conference on Communications (ICC)}, 
    title={{Indoor Human Activity Recognition Based on Ambient Radar with Signal Processing and Machine Learning}}, 
    year={2018},
    volume={},
    number={},
    pages={1-6},
    doi={10.1109/ICC.2018.8422107}
}


@inproceedings{Zigelbaum:2010,
    author = {Zigelbaum, Jamie and Browning, Alan and Leithinger, Daniel and Bau, Olivier and Ishii, Hiroshi},
    title = {{G-Stalt: A Chirocentric, Spatiotemporal, and Telekinetic Gestural Interface}},
    year = {2010},
    isbn = {9781605588414},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1709886.1709939},
    doi = {10.1145/1709886.1709939},
    abstract = {In this paper we present g-stalt, a gestural interface for interacting with video. g-stalt is built upon the g-speak spatial operating environment (SOE) from Oblong Industries. The version of g-stalt presented here is realized as a three-dimensional graphical space filled with over 60 cartoons. These cartoons can be viewed and rearranged along with their metadata using a specialized gesture set. g-stalt is designed to be chirocentric, spatiotemporal, and telekinetic.},
    booktitle = {Proceedings of the Fourth International Conference on Tangible, Embedded, and Embodied Interaction},
    pages = {261–264},
    numpages = {4},
    keywords = {spatiotemporal, g-speak, gesture, pinch, chirocentric, video, telekinetic, gestural interface, 3d},
    venue = {Cambridge, Massachusetts, USA},
    series = {TEI '10},
}


@inproceedings{Zocco:2015,
    author    = {Zocco, Alessandro and Zocco, Matteo D. and Greco, Antonella and Livatino, Salvatore and Paolis, Lucio Tommaso},
    booktitle = {Proceedings of the Second International Conference on Augmented and Virtual Reality - Volume 9254},
    title     = {{Touchless Interaction for Command and Control in Military Operations}},
    year      = {2015},
    address   = {Berlin, Heidelberg},
    pages     = {432–445},
    publisher = {Springer-Verlag},
    doi       = {10.1007/978-3-319-22888-4_32},
    isbn      = {9783319228877},
    keywords  = {Network Centric Warfare, Touchless Interaction, Augmented Reality, Leap Motion, Human computer interaction, Command and control system},
    numpages  = {14},
    url       = {https://doi.org/10.1007/978-3-319-22888-4_32},
}


@article{Zou:2017,
  author     = {Zou, Yi-Bo and Chen, Yi-Min and Gao, Ming-Ke and Liu, Quan and Jiang, Si-Yu and Lu, Jia-Hui and Huang, Chen and Li, Ze-Yu and Zhang, Dian-Hua},
  journal    = {J. Med. Syst.},
  title      = {{Coronary Heart Disease Preoperative Gesture Interactive Diagnostic System Based on Augmented Reality}},
  year       = {2017},
  issn       = {0148-5598},
  month      = aug,
  number     = {8},
  pages      = {1–18},
  volume     = {41},
  address    = {USA},
  doi        = {10.1007/s10916-017-0768-6},
  issue_date = {August 2017},
  keywords   = {coronary heart disease, gesture interaction, HMM, leap motion controller, augmented reality, preoperative diagnosis, K-means},
  numpages   = {18},
  publisher  = {Plenum Press},
  url        = {https://doi.org/10.1007/s10916-017-0768-6},
}
